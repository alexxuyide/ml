[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I am Yide (Alex) Xu, a Geography and Mathematics double major from Middlebury College. On this webpage I post most of the classwork for CSCI 0451 Machine Learning and some machine learning works."
  },
  {
    "objectID": "posts/final-project-post/index.html",
    "href": "posts/final-project-post/index.html",
    "title": "Project Blog Post",
    "section": "",
    "text": "Our project takes on the challenge of predicting population density in regions lacking data. Leveraging landcover image data and tract geometry, our approach involves computing zonal statistics and employing machine learning models. With this problem in mind, we employ satellite data from Connecticut due to its completeness and its potential to also be applied to other Northeastern States within the US. We create Linear Regression models and Spatial Autoregression models with our zonal statistics and tract data. We gauge their efficacy based on their mean-squared error and \\(R^2\\) value. Through this, we find that Linear Regression with No Penalty works best out of our Linear Regression models and our Endogenous Spatial Autoregression model works better than the Exogenous model. Furthermore, we conclude that Spatial Autoregression is more effective at predicting population density than Linear Regression. In regions without adequate census data, the Exogenous model would improve estimations of population density by taking into account the landcover of a given region and its neighbors. Our code and datasets are available through our Github\n\n\n\nIn countries such as the US, there is a large and accurate amount of census data. However there are many ountries in areas where the resources for gathering census data is lesser. This is potentially due to geographic inaccessibility, political conflict, administrative failiure, and as mentioned previously, a lack of resources. Thus, we want a way to predict human populations around the world with the data of the land itself, satellite imagery. With this imaging, the geography is divided into classes which we can then use as variables for our model. Research into this topic has stagnated to a degree, however Tian et al. (2005) produced a hallmark paper which tested the effectivity of modeling population with land cover data. It found that a similar model could have “feasible” and can have “high accuracy”. They utilized Linear Regression, and also manually broke down China into even 1km by 1km cells. Because of availablity of census data, we instead used census tracts, but we continued with the idea of utilizing Linear Regression. With some exploratory graphs of Connecticut, we discovered there might be a Spatial Pattern within our data. In order to take this into account during modeling, we started researching into machine learning algorithms with a spatial component. We came across a paper by Liu, Kounadi, and Zurita-Milla (2022), which concluded that models with a spatial component, such as spatial lag, garner better results than those without. They used spatial lag, and eigvenvectors spatial filtering to predict things beyond our datasets such as soil types. Thus, we sought to create Linear Regression Models and Spatial Autoregressive models, and compare the them to see which is more effective in predicting population density based on land cover.\n\n\n\nNASA in a webinar session called “Humanitarian Applications Using NASA Earth Observations” presented how satellite remote-sensing data could be useful in monitoring humanitarian conditions at refugee settlements. Human settlements could be detected through remote sensing images and therefore could be used to predict the population in a region. This talk alerted us that we still lack necessary population data in many parts of the world, but also demonstrated how remote sensing could be a powerful tool in tackling this problem and solving lack of population data in different countries. Thus, we decide to investigate the connection between remote sensing land cover data and population density in a context with better data coverage.\nThis type of model would be most beneficial by governments and government organizations. These users would most likely be hospital contractors, policy makers, emergency services providers such as ambulances and firefighers, and sociologists. Population census data is crucial for policy makers as it assists in city management so that the equitable distribution of resources can be better calculated.\nThe implications extend beyond helping users. Real people would be affected by this technology. Those who are workers in fields such as emergency service work, or school teachers who might have been over-worked previously may be relieved by the building of new hospitals and schools to compensate for population changes. However, the negative effects are also extremely real.\nImagining that this model expanded beyond the barriers of Connecticut and is being used in countries with much lower census data such as Brazil, there might be a calculation for a forestry company to continue harvesting wood from the Amazon, but they do not want to affect populations. Our algorithm calculates there are very few people in the area, as there is very dense land cover in the Amazon. This company starts to cut down trees and discovers that they are in an area of Indigenous peoples. A minority group that is already negatively affected continues to be disenfranchised. The issue of undercalculating the population density in an area can also affect the amount of resources a policymaker might provide to a region with a much greater population and lacking resources. This would also continue to negatively impact an already negatively impacted area.\nUltimately, the world would be a more equitable and sustainable place if this type of technology could assist countries lacking population data. The positive aspects of providing data where there is none provides the potential for great resource partioning, and better understanding of a countries population.\n\n\n\n\n\nWith this project being the entire state of Connecticut, we utilized landcover data, population, shape files for graphing, and synthesized data which combined our various data sets into manageable datasets suitable for modeling.\nThe bread and butter of our data stems from a 1-meter resolution landcover imagery covering the entire state of Connecticut. Derived from NAIP, the data has already been processed such that every pixel represents a certain class of landcover.\nAt over 800 MB, the dataset is too large to share via GitHub, and is downloadable by clicking on the first option at this link. This landcover dataset was one of the most complete datsets we could find, which is why we wanted to use it for our modelling.\nOur other data sources are the geometries and population data on the Census tract level for the state of Connecticut. We downloaded tract geometries directly into our Jupyter Notebook final_project.ipynb using the Pygris package, and we downloaded the population data from Social Explorer, storing it at data/population.csv.\n\n\n\nFirst, we clean and prepare our data for the model. We start by combining our Tract Geometry of CT with the Population Data of CT to form a new dataset. We utilize both the CT Landcover Data and the Tracts Data in a calculation of Zonal Statistics. This means we calculate the proportion of pixels within each tract that are of a given landcover class. This then is saved as a combined dataset which we then continue to clean by imputing values, performing more advanced Zonal Statistics, and dropping any NA Columns. From there, we are left with data ready to be used in a model.\nThe flowchart below more elegantly outlines this process\n\n\n\n\n\nflowchart LR\n  A(Population Data) --&gt; B(Tracts Data)\n  C(Tracts Geometry Data) --&gt; B(Tracts Data)\n  B --&gt; D{Zonal Statistics}\n  E(CT Landcover Data) --&gt; D{Zonal Statistics}\n  D{Zonal Statistics} --&gt; F(Combined Data)\n  F(Combined Data) --&gt; |Impute Data| G[Ready for Model]\n  F --&gt; |Additional Landcover Statistics| G[Ready for Model]\n  F --&gt; |Drop Uncommon Landcover| G[Cleaned Data]\n\n\n\n\n\n\nWe then implement three types of Linear Regression:\n\nLinear Regression with No Penalty Term\nLinear Regression with \\(\\ell_1\\) Regularization (Lasso Regression)\nLinear Regression with \\(\\ell_1\\) Regularization (Ridge Regression)\n\nBy utilizing the \\(R^2\\) and Mean Squared Error, we quantified the success of each of our models against one another as well as comparing them to sci-kit learn’s own implementations of each of these Linear Regression Models.\nFollowing Linear Regression, we then wanted to implement two types of Spatial AutoRegression:\n\nEndogenous Spatial Autoregression\nExogenous Spatial Autoregression\n\nAs our data can be plotted on a map of Connecticut, we felt it would be amiss to not explore Spatial Autogression. Through this style of model, we can take into account the spatial aspect of each tract when we are predicting. We chose both Endogenous and Exogenous Models. Endogenous Models take into account the neighboring tract population densities of a given tract. Exogenous Models take into account the zonal statistics of a given tract’s neighbors.\nWe merge our data with shape file and calculate the spatial lag of a each tract’s neighbors. The spatial lag is this case is the average population density of a given tracts of land. We also calculate the average landcover types of a given’s tracts neighbors.\nIn total, we create 8 models which we compare in order to determine the best way to predict population density with landcover data\n\n\n\n\n\nflowchart \nA[Cleaned Data] --&gt; B{No Penalty LR}\nA --&gt; C{Lasso LR}\nB --&gt; K{ours}\nB --&gt; L{sci-kit learn}\nC --&gt; G{ours}\nC --&gt; H{sci-kit learn}\nA --&gt; D{Ridge LR}\nD --&gt; I{ours}\nD --&gt; J{sci-kit learn}\nA --&gt; |Spatial Lag Pop Density| E{Endogenous}\nA --&gt; |Spatial Lag Landcover| F{Exogenous}"
  },
  {
    "objectID": "posts/final-project-post/index.html#abstract",
    "href": "posts/final-project-post/index.html#abstract",
    "title": "Project Blog Post",
    "section": "",
    "text": "Our project takes on the challenge of predicting population density in regions lacking data. Leveraging landcover image data and tract geometry, our approach involves computing zonal statistics and employing machine learning models. With this problem in mind, we employ satellite data from Connecticut due to its completeness and its potential to also be applied to other Northeastern States within the US. We create Linear Regression models and Spatial Autoregression models with our zonal statistics and tract data. We gauge their efficacy based on their mean-squared error and \\(R^2\\) value. Through this, we find that Linear Regression with No Penalty works best out of our Linear Regression models and our Endogenous Spatial Autoregression model works better than the Exogenous model. Furthermore, we conclude that Spatial Autoregression is more effective at predicting population density than Linear Regression. In regions without adequate census data, the Exogenous model would improve estimations of population density by taking into account the landcover of a given region and its neighbors. Our code and datasets are available through our Github"
  },
  {
    "objectID": "posts/final-project-post/index.html#introduction",
    "href": "posts/final-project-post/index.html#introduction",
    "title": "Project Blog Post",
    "section": "",
    "text": "In countries such as the US, there is a large and accurate amount of census data. However there are many ountries in areas where the resources for gathering census data is lesser. This is potentially due to geographic inaccessibility, political conflict, administrative failiure, and as mentioned previously, a lack of resources. Thus, we want a way to predict human populations around the world with the data of the land itself, satellite imagery. With this imaging, the geography is divided into classes which we can then use as variables for our model. Research into this topic has stagnated to a degree, however Tian et al. (2005) produced a hallmark paper which tested the effectivity of modeling population with land cover data. It found that a similar model could have “feasible” and can have “high accuracy”. They utilized Linear Regression, and also manually broke down China into even 1km by 1km cells. Because of availablity of census data, we instead used census tracts, but we continued with the idea of utilizing Linear Regression. With some exploratory graphs of Connecticut, we discovered there might be a Spatial Pattern within our data. In order to take this into account during modeling, we started researching into machine learning algorithms with a spatial component. We came across a paper by Liu, Kounadi, and Zurita-Milla (2022), which concluded that models with a spatial component, such as spatial lag, garner better results than those without. They used spatial lag, and eigvenvectors spatial filtering to predict things beyond our datasets such as soil types. Thus, we sought to create Linear Regression Models and Spatial Autoregressive models, and compare the them to see which is more effective in predicting population density based on land cover."
  },
  {
    "objectID": "posts/final-project-post/index.html#values-statement",
    "href": "posts/final-project-post/index.html#values-statement",
    "title": "Project Blog Post",
    "section": "",
    "text": "NASA in a webinar session called “Humanitarian Applications Using NASA Earth Observations” presented how satellite remote-sensing data could be useful in monitoring humanitarian conditions at refugee settlements. Human settlements could be detected through remote sensing images and therefore could be used to predict the population in a region. This talk alerted us that we still lack necessary population data in many parts of the world, but also demonstrated how remote sensing could be a powerful tool in tackling this problem and solving lack of population data in different countries. Thus, we decide to investigate the connection between remote sensing land cover data and population density in a context with better data coverage.\nThis type of model would be most beneficial by governments and government organizations. These users would most likely be hospital contractors, policy makers, emergency services providers such as ambulances and firefighers, and sociologists. Population census data is crucial for policy makers as it assists in city management so that the equitable distribution of resources can be better calculated.\nThe implications extend beyond helping users. Real people would be affected by this technology. Those who are workers in fields such as emergency service work, or school teachers who might have been over-worked previously may be relieved by the building of new hospitals and schools to compensate for population changes. However, the negative effects are also extremely real.\nImagining that this model expanded beyond the barriers of Connecticut and is being used in countries with much lower census data such as Brazil, there might be a calculation for a forestry company to continue harvesting wood from the Amazon, but they do not want to affect populations. Our algorithm calculates there are very few people in the area, as there is very dense land cover in the Amazon. This company starts to cut down trees and discovers that they are in an area of Indigenous peoples. A minority group that is already negatively affected continues to be disenfranchised. The issue of undercalculating the population density in an area can also affect the amount of resources a policymaker might provide to a region with a much greater population and lacking resources. This would also continue to negatively impact an already negatively impacted area.\nUltimately, the world would be a more equitable and sustainable place if this type of technology could assist countries lacking population data. The positive aspects of providing data where there is none provides the potential for great resource partioning, and better understanding of a countries population."
  },
  {
    "objectID": "posts/final-project-post/index.html#materials-and-methods",
    "href": "posts/final-project-post/index.html#materials-and-methods",
    "title": "Project Blog Post",
    "section": "",
    "text": "With this project being the entire state of Connecticut, we utilized landcover data, population, shape files for graphing, and synthesized data which combined our various data sets into manageable datasets suitable for modeling.\nThe bread and butter of our data stems from a 1-meter resolution landcover imagery covering the entire state of Connecticut. Derived from NAIP, the data has already been processed such that every pixel represents a certain class of landcover.\nAt over 800 MB, the dataset is too large to share via GitHub, and is downloadable by clicking on the first option at this link. This landcover dataset was one of the most complete datsets we could find, which is why we wanted to use it for our modelling.\nOur other data sources are the geometries and population data on the Census tract level for the state of Connecticut. We downloaded tract geometries directly into our Jupyter Notebook final_project.ipynb using the Pygris package, and we downloaded the population data from Social Explorer, storing it at data/population.csv.\n\n\n\nFirst, we clean and prepare our data for the model. We start by combining our Tract Geometry of CT with the Population Data of CT to form a new dataset. We utilize both the CT Landcover Data and the Tracts Data in a calculation of Zonal Statistics. This means we calculate the proportion of pixels within each tract that are of a given landcover class. This then is saved as a combined dataset which we then continue to clean by imputing values, performing more advanced Zonal Statistics, and dropping any NA Columns. From there, we are left with data ready to be used in a model.\nThe flowchart below more elegantly outlines this process\n\n\n\n\n\nflowchart LR\n  A(Population Data) --&gt; B(Tracts Data)\n  C(Tracts Geometry Data) --&gt; B(Tracts Data)\n  B --&gt; D{Zonal Statistics}\n  E(CT Landcover Data) --&gt; D{Zonal Statistics}\n  D{Zonal Statistics} --&gt; F(Combined Data)\n  F(Combined Data) --&gt; |Impute Data| G[Ready for Model]\n  F --&gt; |Additional Landcover Statistics| G[Ready for Model]\n  F --&gt; |Drop Uncommon Landcover| G[Cleaned Data]\n\n\n\n\n\n\nWe then implement three types of Linear Regression:\n\nLinear Regression with No Penalty Term\nLinear Regression with \\(\\ell_1\\) Regularization (Lasso Regression)\nLinear Regression with \\(\\ell_1\\) Regularization (Ridge Regression)\n\nBy utilizing the \\(R^2\\) and Mean Squared Error, we quantified the success of each of our models against one another as well as comparing them to sci-kit learn’s own implementations of each of these Linear Regression Models.\nFollowing Linear Regression, we then wanted to implement two types of Spatial AutoRegression:\n\nEndogenous Spatial Autoregression\nExogenous Spatial Autoregression\n\nAs our data can be plotted on a map of Connecticut, we felt it would be amiss to not explore Spatial Autogression. Through this style of model, we can take into account the spatial aspect of each tract when we are predicting. We chose both Endogenous and Exogenous Models. Endogenous Models take into account the neighboring tract population densities of a given tract. Exogenous Models take into account the zonal statistics of a given tract’s neighbors.\nWe merge our data with shape file and calculate the spatial lag of a each tract’s neighbors. The spatial lag is this case is the average population density of a given tracts of land. We also calculate the average landcover types of a given’s tracts neighbors.\nIn total, we create 8 models which we compare in order to determine the best way to predict population density with landcover data\n\n\n\n\n\nflowchart \nA[Cleaned Data] --&gt; B{No Penalty LR}\nA --&gt; C{Lasso LR}\nB --&gt; K{ours}\nB --&gt; L{sci-kit learn}\nC --&gt; G{ours}\nC --&gt; H{sci-kit learn}\nA --&gt; D{Ridge LR}\nD --&gt; I{ours}\nD --&gt; J{sci-kit learn}\nA --&gt; |Spatial Lag Pop Density| E{Endogenous}\nA --&gt; |Spatial Lag Landcover| F{Exogenous}"
  },
  {
    "objectID": "posts/final-project-post/index.html#acquire-tract-geometries",
    "href": "posts/final-project-post/index.html#acquire-tract-geometries",
    "title": "Project Blog Post",
    "section": "Acquire Tract Geometries",
    "text": "Acquire Tract Geometries\nAs a test of concept, lets utilize the pygris library to access the CT tracts information and then let’s do a simple plot to ensure it’s correct.\n\n# Download geometry\nct_tracts = tracts(state = \"CT\", cb = True, cache = True, year = 2016)\n\n# Display geometry\nfig, ax = plt.subplots()\nct_tracts.plot(ax = ax)\nplt.title(\"Tracts Cartographic Boundaries\");\n\nUsing FIPS code '09' for input 'CT'"
  },
  {
    "objectID": "posts/final-project-post/index.html#calculate-population-density",
    "href": "posts/final-project-post/index.html#calculate-population-density",
    "title": "Project Blog Post",
    "section": "Calculate Population Density",
    "text": "Calculate Population Density\nBefore we begin our journey into zonal statistics and eventually creating a predictive model, we first want to understand what the population density looks like in Connecticut. We have some general hypotheses that the areas around New Haven and Hartford are going to have higher amounts of population, and we also expect to see some small pockets of communities around Connecticut.\n\n# Import tracts population data\npop = pd.read_csv(\"../data/population.csv\")\n\n# Convert data type so join key matches\nct_tracts[\"Geo_TRACT\"] = ct_tracts[\"TRACTCE\"].astype(int)\n\n# Join attributes to geometry\ntracts = ct_tracts.merge(pop, how = \"inner\", on='Geo_TRACT')\n\n# Project tracts\ntracts = tracts.to_crs(\"EPSG:3857\")\n\n# Calculate area in KM^2\ntracts[\"Area\"] = tracts.area/1000**2\n\n# Calculate population density\ntracts[\"PopDensity\"] = tracts[\"SE_A00001_001\"]/tracts[\"Area\"]\n\n# Create map\ntracts.plot(\"PopDensity\", legend = True);"
  },
  {
    "objectID": "posts/final-project-post/index.html#first-steps",
    "href": "posts/final-project-post/index.html#first-steps",
    "title": "Project Blog Post",
    "section": "First steps",
    "text": "First steps\nHere we open our path to our file, and more importantly, we set up our data to be used in zonal statistics. .read turns our data into a Numpy Array. Following this we are going to .transform our data, which means we are going to take the pixel locations of our coordinates (row col) and map them to our spatial coordinates (x, y). These coordinate values are relative to the CRS (Coordinate Reference System) which we defined earlier as “EPSG:2234”\n\n%%script echo skipping\n#the data can be accessed from https://coastalimagery.blob.core.windows.net/ccap-landcover/CCAP_bulk_download/High_Resolution_Land_Cover/Phase_2_Expanded_Categories/Legacy_Land_Cover_pre_2024/CONUS/ct_2016_ccap_hires_landcover_20200915.zip\nraster_path = '../data/ct_2016_ccap_hires_landcover_20200915.tif'\nlandcover = rasterio.open(raster_path)\narr = landcover.read(1)\naffine = landcover.transform\n\nskipping"
  },
  {
    "objectID": "posts/final-project-post/index.html#performing-zonal-statistics",
    "href": "posts/final-project-post/index.html#performing-zonal-statistics",
    "title": "Project Blog Post",
    "section": "Performing Zonal statistics",
    "text": "Performing Zonal statistics\nIt’s as simple as importing rasterstats. We have handled the important data manipulation, and now it’s basically plug and play! One function to note is .to_crs which takes in given coordinate reference system and transforms all the points in our dataframe to match that system.\nThe rasterstats library is very good at getting information from rasters, and we can in fact gain more information by using categorical = True. This allows to see the amount of each type of pixel at a given tract.\n\n%%script echo skipping\ndf_new = zonal_stats(zone, arr, affine=affine, categorical = True)\n\nskipping\n\n\nTaking a look at our dataframe, we can confirm that each column is a type of pixel and each row is a tract\n\n%%script echo skipping\ndf_categorical = pd.DataFrame(df_new)\ndf_categorical\n\nskipping"
  },
  {
    "objectID": "posts/final-project-post/index.html#visualizing-zonal-stats",
    "href": "posts/final-project-post/index.html#visualizing-zonal-stats",
    "title": "Project Blog Post",
    "section": "Visualizing Zonal Stats",
    "text": "Visualizing Zonal Stats\nNow that we have information on the amount of each pixel at a given tract, we can find the most common pixel per tract by using the function .idxmax() which will through each row and find the column with the largest value.\n\n%%script echo skipping\ndf_categorical['max_type'] = df_categorical.idxmax(axis=1)\ncombined_df = pd.concat([tracts, df_categorical], axis=1)\ncombined_df['max_type'] = combined_df['max_type'].astype(str)\n\nskipping\n\n\n\n%%script echo skipping\ncombined_df.plot(\"max_type\", legend = True);\n\nskipping\n\n\n\nSaving this data\nThese statistics took quite a while to run, and it may be beneficial to save this data as a csv to continue running statistics in the future\n\n%%script echo skipping\n\ncombined_df.to_csv('../data/combined_data.csv', index=False)\n\nskipping"
  },
  {
    "objectID": "posts/final-project-post/index.html#data-preparation-1",
    "href": "posts/final-project-post/index.html#data-preparation-1",
    "title": "Project Blog Post",
    "section": "Data Preparation",
    "text": "Data Preparation\nFirst, we import our data.\n\n# Import and display data\ndata = pd.read_csv(\"../data/combined_data.csv\")\ndata.head()\n\n\n\n\n\n\n\n\n\nSTATEFP\nCOUNTYFP\nTRACTCE\nAFFGEOID\nGEOID\nNAME\nLSAD\nALAND\nAWATER\ngeometry\n...\n18\n19\n20\n21\n22\n7\n6\n0\n23\nmax_type\n\n\n\n\n0\n9\n1\n11000\n1400000US09001011000\n9001011000\n110.0\nCT\n4473567\n3841130\nPOLYGON ((-8191739.173321358 5013468.769836016...\n...\n136572.0\n423692.0\n142589.0\n1378858.0\nNaN\nNaN\nNaN\nNaN\nNaN\n2\n\n\n1\n9\n1\n20800\n1400000US09001020800\n9001020800\n208.0\nCT\n2315472\n0\nPOLYGON ((-8187432.3302968815 5025136.84023609...\n...\nNaN\nNaN\n27939.0\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n11\n\n\n2\n9\n1\n21400\n1400000US09001021400\n9001021400\n214.0\nCT\n1640443\n0\nPOLYGON ((-8189589.702028457 5021116.993618919...\n...\nNaN\nNaN\n13728.0\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n2\n\n\n3\n9\n1\n22200\n1400000US09001022200\n9001022200\n222.0\nCT\n1442382\n117063\nPOLYGON ((-8186995.178656538 5019223.193891366...\n...\nNaN\n20584.0\n80161.0\n99956.0\nNaN\nNaN\nNaN\nNaN\nNaN\n2\n\n\n4\n9\n1\n43100\n1400000US09001043100\n9001043100\n431.0\nCT\n6652660\n58522\nPOLYGON ((-8178763.436270848 5029936.759394648...\n...\nNaN\nNaN\n9940.0\n68655.0\n486.0\nNaN\nNaN\nNaN\nNaN\n11\n\n\n\n\n5 rows × 87 columns\n\n\n\n\nLooks like there is some missing data in tracts that contain no pixels of a certain class. Let’s impute 0 for all NaN values.\n\n# Impute 0 for missing data\nprint(\"Before imputation, there were\", pd.isnull(data.iloc[:,68:-1]).sum().sum(), \"NaN values.\")\ndata[pd.isnull(data.iloc[:,68:-1])] = 0\nprint(\"After imputation, there are\", pd.isnull(data.iloc[:,68:-1]).sum().sum(), \"NaN values.\")\ndata.head()\n\nBefore imputation, there were 5774 NaN values.\nAfter imputation, there are 0 NaN values.\n\n\n\n\n\n\n\n\n\n\nSTATEFP\nCOUNTYFP\nTRACTCE\nAFFGEOID\nGEOID\nNAME\nLSAD\nALAND\nAWATER\ngeometry\n...\n18\n19\n20\n21\n22\n7\n6\n0\n23\nmax_type\n\n\n\n\n0\n9\n1\n11000\n1400000US09001011000\n9001011000\n110.0\nCT\n4473567\n3841130\nPOLYGON ((-8191739.173321358 5013468.769836016...\n...\n136572.0\n423692.0\n142589.0\n1378858.0\n0.0\n0.0\n0.0\n0.0\n0.0\n2\n\n\n1\n9\n1\n20800\n1400000US09001020800\n9001020800\n208.0\nCT\n2315472\n0\nPOLYGON ((-8187432.3302968815 5025136.84023609...\n...\n0.0\n0.0\n27939.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n11\n\n\n2\n9\n1\n21400\n1400000US09001021400\n9001021400\n214.0\nCT\n1640443\n0\nPOLYGON ((-8189589.702028457 5021116.993618919...\n...\n0.0\n0.0\n13728.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n2\n\n\n3\n9\n1\n22200\n1400000US09001022200\n9001022200\n222.0\nCT\n1442382\n117063\nPOLYGON ((-8186995.178656538 5019223.193891366...\n...\n0.0\n20584.0\n80161.0\n99956.0\n0.0\n0.0\n0.0\n0.0\n0.0\n2\n\n\n4\n9\n1\n43100\n1400000US09001043100\n9001043100\n431.0\nCT\n6652660\n58522\nPOLYGON ((-8178763.436270848 5029936.759394648...\n...\n0.0\n0.0\n9940.0\n68655.0\n486.0\n0.0\n0.0\n0.0\n0.0\n11\n\n\n\n\n5 rows × 87 columns\n\n\n\n\nNow that we have complete data, we can calculate the proportion of pixels belonging to each class.\n\n# Calculate total number of pixels in each tract\ndata[\"sum\"] = data.iloc[:,68:-1].sum(axis = 1)\n\n# Calculate proportion of pixels belonging to each class\ndata.iloc[:,68:-2] = data.iloc[:,68:-2].div(data['sum'], axis=0)\n\n# View data\ndata.head()\n\n\n\n\n\n\n\n\n\nSTATEFP\nCOUNTYFP\nTRACTCE\nAFFGEOID\nGEOID\nNAME\nLSAD\nALAND\nAWATER\ngeometry\n...\n19\n20\n21\n22\n7\n6\n0\n23\nmax_type\nsum\n\n\n\n\n0\n9\n1\n11000\n1400000US09001011000\n9001011000\n110.0\nCT\n4473567\n3841130\nPOLYGON ((-8191739.173321358 5013468.769836016...\n...\n0.069327\n0.023331\n0.225616\n0.000000\n0.0\n0.0\n0.0\n0.0\n2\n6111530.0\n\n\n1\n9\n1\n20800\n1400000US09001020800\n9001020800\n208.0\nCT\n2315472\n0\nPOLYGON ((-8187432.3302968815 5025136.84023609...\n...\n0.000000\n0.012054\n0.000000\n0.000000\n0.0\n0.0\n0.0\n0.0\n11\n2317904.0\n\n\n2\n9\n1\n21400\n1400000US09001021400\n9001021400\n214.0\nCT\n1640443\n0\nPOLYGON ((-8189589.702028457 5021116.993618919...\n...\n0.000000\n0.008350\n0.000000\n0.000000\n0.0\n0.0\n0.0\n0.0\n2\n1644135.0\n\n\n3\n9\n1\n22200\n1400000US09001022200\n9001022200\n222.0\nCT\n1442382\n117063\nPOLYGON ((-8186995.178656538 5019223.193891366...\n...\n0.013289\n0.051753\n0.064533\n0.000000\n0.0\n0.0\n0.0\n0.0\n2\n1548918.0\n\n\n4\n9\n1\n43100\n1400000US09001043100\n9001043100\n431.0\nCT\n6652660\n58522\nPOLYGON ((-8178763.436270848 5029936.759394648...\n...\n0.000000\n0.001484\n0.010249\n0.000073\n0.0\n0.0\n0.0\n0.0\n11\n6698858.0\n\n\n\n\n5 rows × 88 columns\n\n\n\n\n\n# Separate predictors and outcome\nX = data.iloc[:,68:-2]\ny = data[\"PopDensity\"]\n\nWe had an issue where our results were not quite matching those of scikit-learn and we discovered that this was due to a way we set up our dataset. Since we have calculated the proportion of pixels in each tract belonging to each landcover class, the landcovers sum to 1 in every row. Since we create an additional column of ones in order to calculate a y-intercept for linear regression with gradient descent, this means that our y-intercept column is equal to the sum of our other columns. In other words, the constant column is linearly dependent on our other predictor columns. To address this issue, we drop some columns that seem unimportant. Specifically, these columns are mostly zero, meaning that they are not very common in Connecticut anyway.\n\n# Drop some landcovers to address issue of linear combination \nX = X[['2', '5', '11', '12', '8', '13', '14', '15', '20', '21']]"
  },
  {
    "objectID": "posts/final-project-post/index.html#linear-regression-with-no-penalty-term",
    "href": "posts/final-project-post/index.html#linear-regression-with-no-penalty-term",
    "title": "Project Blog Post",
    "section": "Linear Regression with No Penalty Term",
    "text": "Linear Regression with No Penalty Term\n\nSci-kit Learn\n\nTrain Model\nFirst, we fit a linear regression model with scikit-learn. We do this simply to verify against our own implementation of linear regression.\n\n# Fit model\n# Doing this just for the purpose of seeing what it looks like\n# We can use the results from this package to verify that our implementation is working properly\n\n#Train and test split creation\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nLR_s = LinearRegression() \n\nm = LR_s.fit(X_train, y_train)\n\nLinear regression seeks to minimize the mean squared error, so we report the mean square error from scikit-learn’s model here.\n\n# MSE\nmean_squared_error(y_train, LR_s.predict(X_train))\n\n227396.12768129486\n\n\nLet’s check the \\(R^2\\) value of our model. Recall that \\(R^2\\) is also known as the coefficient of determination, and it represents the proportion of variation in one’s outcome variable that is explained by one’s model.\n\n# R^2 value\nm.score(X_train, y_train)\n\n0.7723901708932351\n\n\nWith an \\(R^2\\) value of roughly \\(0.772\\), our ordinary least squares regression model accounts for about \\(77.2\\)% of the variation of the population densities in Connecticut’s tracts.\nLet’s inspect the y-intercept and coefficients to verify that our coefficients seem logical.\n\n# Y-intercept\nprint(\"Intercept:\", m.intercept_)\n\n# Min and max population density\nprint(\"Population Density Min:\", y_train.min())\nprint(\"Population Density Max:\", y_train.max())\n\nIntercept: 983.2395073145441\nPopulation Density Min: 0.0\nPopulation Density Max: 6084.305602883675\n\n\nSince our predictions are proportions of pixels in a tract of a given landcover, it is impossible for all of our predictors to be zero. Basically this means that no tract will be in the situation where all variables are equal to zero, leaving the y-intercept as its population density. However, in theory, in the absence of any landcover pixels, the population density would be \\(983\\) people per square kilometer. With y_train ranging from 0 to 6084, this seems somewhat reasonable.\n\n# Variable coefficients\nm.coef_\n\narray([  3409.40801231,  -2942.65854175,   -917.38563842,  -4525.6598175 ,\n          668.32452458,  -2125.96537456,  -1746.52921947,  -1576.35637606,\n       -13652.09857612,  -1417.12360532])\n\n\n\n# Columns\nX.columns\n\nIndex(['2', '5', '11', '12', '8', '13', '14', '15', '20', '21'], dtype='object')\n\n\nMost of these coefficients are negative, indicating that as the proportion of pixels representing a given landcover type increases, the population density of the tract decreases. The only positive values are the coefficient of 2, which represents developed impervious landcover, and the coefficient of 8, which represents grassland/herbaceous landcover. We definitely anticipated a positive coefficient for 2, as impervious developed surfaces like buildings and roads are a major marker of human presence. The documentation indicates that while this landcover cannot be used for tilling, it can be used for grazing, so perhaps the positive coefficient is indicative of population density associated with farming. Also, Connecticut is generally forested in rural areas, so grassy areas are likely in suburbia or near urban areas. The magnitude of 2 is much larger than 8, however, indicating that developed impervious landcover is the most important factor increasing population density.\nThe negative coefficients correspond to developed open space, mixed forest, shrub, palustrine forested wetland, palustrine scrub/shrub wetland, palustrine emergent wetland, barren land, and open water. With the exception of developed open space, these landcovers are generally not associated with population density. And developed open space does not necessitate people living in that location – people could live in one tract and commute to a tract with developed open space for recreational purposes, for example. Thus it makes sense that increased values of these variables contribute to less population density.\n\n\nTest Model\nNow that we have evaluated the basic interpretation of our model on our training data, let us check the performance of our model on our testing data. First, we calculate our predictions.\n\n# Create predictions (on test data)\npreds = LR_s.predict(X_test)\n\nLet us inspect the mean square error of our model on the testing data.\n\n# MSE\nmean_squared_error(y_test, preds)\n\n373799.85511504946\n\n\nAt \\(373,800\\), the mean squared error of our model on the testing data is much larger than the mean squared error on the training data, which was \\(227,396\\). This makes sense as our model was fit specifically to the tendencies of the training data.\nTo evaluate the explanatory power of our model, let’s also calculate the \\(R^2\\) value on our testing data.\n\n# Test R^2 value\nr2_score(y_test, preds)\n\n0.7086666350845903\n\n\nAs one might anticipate, the \\(R^2\\) value of the testing data is lower than the training data. However, at \\(0.709\\), the \\(R^2\\) of the testing data is only \\(0.064\\) lower than the \\(R^2\\) of the training data. In other words, our model explains \\(6.4\\)% less of the variation of the population density in our testing data. This is not a negligible amount, but we are still relatively satisfied with a model that explains over \\(70\\)% of the variation in population density.\n\n\n\nOur Implementation\nWe implemented ordinary linear regression with gradient descent in linear_regression.py. Let us train the model using our implementation and verify that our results roughly match those of scikit-learn.\n\nTrain Model\nFirst, we need to convert our training and testing data to the torch.tensor format to match the expected input of our model. We also add a column of ones at the end of the X training and testing data for the purposes of training our y-intercept.\n\n# convert to torch tensors\n# add column of ones for y-intercept\nX_train_torch = torch.cat((torch.tensor(X_train.values), torch.ones((X_train.shape[0], 1))), 1)\ny_train_torch = torch.tensor(y_train.values)\nX_test_torch = torch.cat((torch.tensor(X_test.values), torch.ones((X_test.shape[0], 1))), 1)\ny_test_torch = torch.tensor(y_test.values)\n\nNow that we have our data in the appropriate format, we can train our model.\n\n# fit linear regression model\nLR = LinearRegress()\nopt = GradientDescentOptimizer(LR)\n\n# initialize vector to record loss values\nloss_vec = []\n\n# fit model\nfor i in range(500000): \n    # update model\n    opt.step(X_train_torch, y_train_torch, alpha = 0.01)\n\n    # calculate and record loss\n    loss = LR.loss(X_train_torch, y_train_torch) \n    loss_vec.append(loss)\n\nLet’s inspect the evolution of our loss function (mean squared error) to verify that our model has converged to a solution.\n\n# plot the changes in loss \nplt.plot(loss_vec, color = \"slategrey\")\nlabs = plt.gca().set(xlabel = \"Iteration\", ylabel = \"Loss\")\nplt.title(\"Loss Function Throughout Model Training\");\n\n# MSE\nprint(\"Mean squared error after training:\", LR.mse(X_train_torch, y_train_torch).item())\n\nMean squared error after training: 227396.1319467965\n\n\n\n\n\n\n\n\n\nGreat! After \\(500,000\\) iterations, our mean squared error is \\(227,396.132\\), which is essentially equivalent to the mean squared error of \\(227,396.128\\) found by scikit-learn.\nLet’s inspect the y-intercept and coefficients to verify that they are similar to scikit-learn’s solution.\n\n# Y-intercept\nLR.w[-1]\n\ntensor(983.0291, dtype=torch.float64)\n\n\nThis y-intercept is also similar to the figure of \\(983.2395\\) reported by scikit-learn.\n\n# Variable coefficients\nprint(\"Coefficients:\", LR.w[:-1])\n\n# Differences in signs\nprint(\"Differences in sign:\", (torch.tensor(m.coef_)*LR.w[:-1]&lt; 0).sum().item())\n\n# Maximum difference in coefficient\nprint(\"Maximum coefficient difference:\", torch.abs((torch.tensor(m.coef_)-LR.w[:-1])).max().item())\n\nCoefficients: tensor([  3409.6334,  -2942.3605,   -917.2350,  -4527.3370,    669.5791,\n         -2126.6514,  -1721.1996,  -1578.8776, -13651.8922,  -1416.8399],\n       dtype=torch.float64)\nDifferences in sign: 0\nMaximum coefficient difference: 25.329603726808955\n\n\nOur coefficients are very similar to those from scikit-learn’s solution! All coefficients have the same sign and the maximum difference between a coefficient in our two models is \\(25\\). Considering the magnitude of the coefficients, this difference is relatively small. Thus the interpretation of our model matches the interpretation of scikit-learn’s model, making us confident that we have implemented linear regression correctly.\n\n# Compute R^2 score\nLR.r2(X_train_torch, y_train_torch)\n\ntensor(0.7724, dtype=torch.float64)\n\n\nOur \\(R^2\\) value is the same as scikit-learn’s.\n\n\nTest Model\nNow we inspect our model’s performance on the testing data.\n\n# MSE\nLR.mse(X_test_torch, y_test_torch)\n\ntensor(373801.8165, dtype=torch.float64)\n\n\nAt \\(373,802\\), our implementation’s testing MSE is very similar to scikit-learn’s \\(373,800\\), indicating similar performance. Once again, this is substantially larger than the training MSE, indicating that our model did not generalize perfectly.\n\n# R^2 value\nLR.r2(X_test_torch, y_test_torch)\n\ntensor(0.7087, dtype=torch.float64)\n\n\nScikit-learn’s testing \\(R^2\\) value was also \\(0.7087\\)! Overall, it appears that we have succesfully implemented linear regression in a manner that achieves similar results to scikit-learn."
  },
  {
    "objectID": "posts/final-project-post/index.html#linear-regression-with-ell_1-regularization",
    "href": "posts/final-project-post/index.html#linear-regression-with-ell_1-regularization",
    "title": "Project Blog Post",
    "section": "Linear Regression with \\(\\ell_1\\) Regularization",
    "text": "Linear Regression with \\(\\ell_1\\) Regularization\n\nSci-kit Learn\n\nTrain Model\nFirst, we fit the model with scikit-learn Lasso and inspect the resulting model. As before, we do this simply to verify against our own implementation.\n\n# Fit model\nLR_s_l1 = Lasso(alpha = 1)\n\nm = LR_s_l1.fit(X_train, y_train)\n\n# Report results\nprint(\"MSE:\", mean_squared_error(y_train, LR_s_l1.predict(X_train)),\n      \"\\nR^2:\", m.score(X_train, y_train),\n      \"\\nY-intercept:\", m.intercept_,\n      \"\\nCoefficients:\\n\", m.coef_)\n\nMSE: 236944.40360579122 \nR^2: 0.7628329217280919 \nY-intercept: -191.72480712350455 \nCoefficients:\n [ 4358.88007237 -1696.29039686   224.06934601    -0.\n     0.            -0.            -0.            -0.\n -6028.66936275  -279.44578393]\n\n\n\n# Columns\nX.columns\n\nIndex(['2', '5', '11', '12', '8', '13', '14', '15', '20', '21'], dtype='object')\n\n\nThe training MSE is slightly larger and the training \\(R^2\\) is slightly smaller than linear regression with no regularizer, which makes sense as we have applied a penalty to help prevent overfitting. The y-intercept is closer to \\(0\\), and many of the coefficients are equal to exactly \\(0\\), making them more interpretable: some coefficients simply do not matter! In this model, landcover 2 (developed impervious) again has a positive coefficient, and with a large magnitude, it remains the main driver in high population density. There is one other variable, 11 (mixed forest), which has a positive coefficient. Interestingly, it was negative in the other model, leading to confusion in its interpretation. But with a somewhat small magnitude, this variable overall has a minor impact on population density, only changing the population density by 224 people per square kilometer as its value increases from 0 to 1. With the \\(\\ell_1\\) regularizer, the landcovers of shrub, grassland/herbaceous, palustrine forested wetland, palustrine scrub/shrub wetland, and palustrine emergent wetland are now equal to zero. These coefficients must not have been that important to the model, as our regularizer made them have zero impact on population density. Variables with negative coefficients are developed open space, barren land, and open water, probably for the same reasons that they were negative earlier.\n\n\nTest Model\nNext, we discover whether the \\(\\ell_1\\) regularizer actually made the model generalize better to the testing data.\n\n# Create predictions (on test data)\npreds = LR_s_l1.predict(X_test)\n\n# Report results\nprint(\"MSE:\", mean_squared_error(y_test, preds),\n      \"\\nR^2:\", r2_score(y_test, preds))\n\nMSE: 390639.95954704983 \nR^2: 0.6955417389066836\n\n\nOur new MSE of \\(390,639\\) is actually larger than the MSE of \\(373,800\\) with no regularizer, indicating that the \\(\\ell_1\\) regularizer did not help our model generalize to the testing data. Furthermore, the \\(R^2\\) value was larger in the previous model, meaning that the model with no regularizer explained more variation in the outcome variable.\n\n\n\nOur Implementation\nLet’s fit linear regression with the \\(\\ell_1\\) norm with our own implementation and verify that our results match those of scikit-learn. Note that scikit-learn uses an algorithm known as coordinate descent to find their solution, but we learned about gradient descent in this class. Coordinate descent is better suited for lasso regression because it allows some coefficients to equal exactly zero. Gradient descent with the \\(\\ell_1\\) norm makes some coefficients much smaller, but does not cause any of them to equal exactly zero. To mimick their results, in our implementation we set our coefficients equal to zero if they are below a selected threshold. We allow our model \\(5000\\) iterations to begin learning the coefficients before applying this threshold.\n\nTrain Model\n\n# fit linear regression model\nLR_l1 = LinearRegress(penalty = \"l1\", lam = 1) # 1 in scikit-learn\nopt_l1 = GradientDescentOptimizer(LR_l1)\n\n# initialize vector to record loss values\nloss_vec_l1 = []\n\n# fit model\nfor i in range(50000):\n    # update model\n    opt_l1.step(X_train_torch, y_train_torch, alpha = 0.001)\n\n    # set coefs equal to zero after model has had enough learning time\n    if i &gt; 5000:\n        LR_l1.w[torch.abs(LR_l1.w) &lt; 500] = 0\n\n    # calculate and record loss\n    loss = LR_l1.loss(X_train_torch, y_train_torch) \n    loss_vec_l1.append(loss)\n\n# plot the changes in loss \nplt.plot(loss_vec_l1, color = \"slategrey\")\nlabs = plt.gca().set(xlabel = \"Iteration\", ylabel = \"Loss\")\nplt.title(\"Loss Function Throughout Model Training\");\n\n\n\n\n\n\n\n\nIt appears that our model converged to a solution with a similar loss function value! Note that the small upwards blip in the loss function occured at iteration \\(5000\\) when we began allowing our model to set some coefficients equal to zero. Let us inspect our results and compare them to scikit-learn’s output.\n\n# Report results\nprint(\"MSE:\", LR_l1.mse(X_train_torch, y_train_torch).item(),\n      \"\\nR^2:\", LR_l1.r2(X_train_torch, y_train_torch),\n      \"\\nY-intercept:\", LR_l1.w[-1],\n      \"\\nCoefficients:\\n\", LR_l1.w[:-1],\n      \"\\nDifferences in sign:\", (torch.tensor(m.coef_)*LR_l1.w[:-1]&lt; 0).sum().item(),\n      \"\\nMaximum coefficient difference:\", torch.abs((torch.tensor(m.coef_)-LR_l1.w[:-1])).max().item())\n\nMSE: 233736.21710488532 \nR^2: tensor(0.7660, dtype=torch.float64) \nY-intercept: tensor(0., dtype=torch.float64) \nCoefficients:\n tensor([ 4257.6646, -1977.7857,     0.0000,     0.0000,     0.0000,     0.0000,\n            0.0000,     0.0000, -7781.5773,  -551.3119], dtype=torch.float64) \nDifferences in sign: 0 \nMaximum coefficient difference: 1752.9079262978257\n\n\nOur model’s MSE of \\(233,736\\) is slightly smaller than scikit-learn’s MSE of \\(236,944\\) and our model’s \\(R^2\\) of \\(0.7660\\) is slightly larger than scikit-learn’s \\(R^2\\) of \\(0.7628\\), indicating that our linear regression model with the \\(\\ell_1\\) norm performed marginally better than theirs. This difference could have occured due to differences in the optimizer and the number of training iterations. Additionally, these MSE and \\(R^2\\) metrics are both slightly worse than what our implementation achieved with no regularizer, which makes sense as we are attempting to prevent overfitting.\nOne should note that our workaround for setting coefficients equal to zero is not ideal for several reasons. First, we hard-coded a certain threshold for choosing coefficients to set equal to zero, as well as a certain number of iterations at which to begin checking for these low-magnitude coefficients. Most users probably do not want to decide on such a threshold. Second, our method did not exactly replicate the output from scikit-learn. Adjusting our parameters to exactly reproduce the coefficients set to zero proved difficult, and the best we were able to do involved setting the y-intercept and landcover 11 equal to zero, while they were nonzero in scikit-learn’s solution. Landcover 11 represents mixed forest and was the one coefficient with a somewhat counterintuitive value in scikit-learn’s model, so in terms of interpretation, our new model still makes sense. All coefficients have the same sign as scikit-learn’s model with similar magnitudes, making us confident that our model is successfully describing the situation, despite the minor discrepancies.\n\n\nTest Model\n\n# Report results\nprint(\"MSE:\", LR_l1.mse(X_test_torch, y_test_torch),\n      \"\\nR^2:\", LR_l1.r2(X_test_torch, y_test_torch))\n\nMSE: tensor(384765.6761, dtype=torch.float64) \nR^2: tensor(0.7001, dtype=torch.float64)\n\n\nThese values are pretty similar to the ones we have seen already. At \\(384,766\\), our implementation’s MSE is less than scikit-learn’s \\(390,640\\), and at \\(0.7001\\), our implementation’s \\(R^2\\) is slightly more than scikit-learn’s \\(0.6955\\). This means that our model generalized slightly better to the testing data, in addition to performing better on the training data. Again, this can likely be explained by differences in the optimization method and the number of training iterations.\nFurthermore, this MSE is slightly larger than the \\(373,802\\) figure returned by our implementation of linear regression with no penalty term, and this \\(R^2\\) is slighly smaller than the \\(0.7087\\) figure, indicating that linear regression with the \\(\\ell_1\\) penalty did not generalize better to the testing data."
  },
  {
    "objectID": "posts/final-project-post/index.html#linear-regression-with-ell_2-regularization",
    "href": "posts/final-project-post/index.html#linear-regression-with-ell_2-regularization",
    "title": "Project Blog Post",
    "section": "Linear Regression with \\(\\ell_2\\) Regularization",
    "text": "Linear Regression with \\(\\ell_2\\) Regularization\n\nSci-kit Learn\n\nTrain Model\nFirst, we fit the model with scikit-learn Ridge and inspect the resulting model. As before, we do this to assess the validity of our own implementation.\n\n# Fit model\nLR_s_l2 = Ridge(alpha = .1)\n\nm = LR_s_l2.fit(X_train, y_train)\n\n# Report results\nprint(\"MSE:\", mean_squared_error(y_train, LR_s_l2.predict(X_train)),\n      \"\\nR^2:\", m.score(X_train, y_train),\n      \"\\nY-intercept:\", m.intercept_,\n      \"\\nCoefficients:\\n\", m.coef_)\n\nMSE: 235049.69085940512 \nR^2: 0.7647294150800621 \nY-intercept: 69.8570955883946 \nCoefficients:\n [ 4146.87047622 -2058.81157194     6.57006522 -1039.66258053\n   107.03266863  -815.93549227  -127.78253829  -231.19197573\n -6438.07336424  -692.08973348]\n\n\n\n# Columns\nX.columns\n\nIndex(['2', '5', '11', '12', '8', '13', '14', '15', '20', '21'], dtype='object')\n\n\nThe training MSE is larger and the training \\(R^2\\) is smaller than scikit-learn’s linear regression with no regularizer. We anticipated this would be true in comparison to the no regularizer model as the penalty term helps prevent overfitting. It appears that with our chosen parameters, lasso regression performed better than ridge regression in terms of both MSE and \\(R^2\\), but this will change depending on the selected value for parameters.\nThe y-intercept and all coefficients except for landcover 2 are smaller than they were under linear regression without regularization, indicating that the regularization method has been successful in decreasing the magnitude of our coefficients. None of the coefficients are equal to exactly zero, but that is to be expected when working with the \\(\\ell_2\\) penalty.\nThe sign of every coefficient in this model is the same as in the original linear regression model except for landcover 11 (mixed forest), which is now positive and was also positive under lasso regression. However, the magnitude of this coefficient is really small; at 6.57, a location’s population density only changes by 6.57 people per square kilometer as the proportion of pixels represented by mixed forest increases from 0 to 1.\n\n\nTest Model\n\n# Create predictions (on test data)\npreds = LR_s_l2.predict(X_test)\n\n# Report results\nprint(\"MSE:\", mean_squared_error(y_test, preds),\n      \"\\nR^2:\", r2_score(y_test, preds))\n\nMSE: 387635.2059385676 \nR^2: 0.6978835936921313\n\n\nOn the testing data, our MSE of \\(387,635\\) is similar to the result of \\(390,640\\) with the \\(\\ell_1\\) regularizer but larger than the MSE of \\(373,800\\) with no regularizer, indicating that the \\(\\ell_2\\) regularizer also did not help our model generalize to the testing data better than unregularized linear regression. The \\(R^2\\) value was also larger in linear regression, meaning that the model without regularization explained more variation in the outcome variable.\n\n\n\nOur Implementation\n\nTrain Model\nLet’s fit linear regression with the \\(\\ell_1\\) norm with our own implementation and verify that our results are reasonably similar to those of scikit-learn.\n\n# fit linear regression model\nLR_l2 = LinearRegress(penalty = \"l2\", lam = .1/X_train_torch.shape[0]) \nopt_l2 = GradientDescentOptimizer(LR_l2)\n\n# initialize vector to record loss values\nloss_vec_l2 = []\n\n# fit model\nfor i in range(1000000): \n    # update model\n    opt_l2.step(X_train_torch, y_train_torch, alpha = 0.00001)\n\n    # calculate and record loss\n    loss = LR_l2.loss(X_train_torch, y_train_torch) \n    loss_vec_l2.append(loss)\n\n# plot the changes in loss \nplt.plot(loss_vec_l2, color = \"slategrey\")\nlabs = plt.gca().set(xlabel = \"Iteration\", ylabel = \"Loss\")\nplt.title(\"Loss Function Throughout Model Training\");\n\n\n\n\n\n\n\n\n\nm.coef_\n\narray([ 4146.87047622, -2058.81157194,     6.57006522, -1039.66258053,\n         107.03266863,  -815.93549227,  -127.78253829,  -231.19197573,\n       -6438.07336424,  -692.08973348])\n\n\n\n# Report results\nprint(\"MSE:\", LR_l2.mse(X_train_torch, y_train_torch).item(),\n      \"\\nR^2:\", LR_l2.r2(X_train_torch, y_train_torch),\n      \"\\nY-intercept:\", LR_l2.w[-1],\n      \"\\nCoefficients:\\n\", LR_l2.w[:-1],\n      \"\\nDifferences in sign:\", (torch.tensor(m.coef_)*LR_l2.w[:-1]&lt; 0).sum().item(),\n      \"\\nMaximum coefficient difference:\", torch.abs((torch.tensor(m.coef_)-LR_l2.w[:-1])).max().item())\n\nMSE: 246031.0025521462 \nR^2: tensor(0.7537, dtype=torch.float64) \nY-intercept: tensor(-258.6768, dtype=torch.float64) \nCoefficients:\n tensor([ 4417.6162, -1821.4942,   373.5831,  -372.9745,  -251.1601,  -436.8005,\n          -43.2542,  -109.9865, -2181.9550,  -541.2076], dtype=torch.float64) \nDifferences in sign: 1 \nMaximum coefficient difference: 4256.118320893194\n\n\n\nX.columns\n\nIndex(['2', '5', '11', '12', '8', '13', '14', '15', '20', '21'], dtype='object')\n\n\nFirst of all, our implementation does not generate identical output to scikit-learn’s implementation. In order to make our implementation converge to a solution, we needed to make \\(\\lambda\\) far smaller than in their implementation. This may have occurred because we are using the optimization technique of gradient descent, but scikit-learn has implemented a number of more complex techniques and automatically detects which one to use depending on the dataset it receives as input. It is also possible that they implemented their loss function as the sum of squared error rather than the mean squared error. If this is the case, then dividing our \\(\\lambda\\) by the number of observations should theoretically produce identical results. In the code above, we opt for this implementation; however, it should be noted that we have not confirmed whether scikit-learn actually uses the sum of squares in their loss function. Even with this modification, our model has converged to a different solution than theirs, for reasons we have not uncovered.\nAlthough our results are different, they are not drastically different. Our MSE is \\(246,030\\) rather than \\(235,050\\) and our \\(R^2\\) is \\(0.7537\\) rather than \\(0.7647\\), differences that are not ideal but also not terrible. All coefficients have the same sign as their solution except for 8 (grassland/herbaceous). In all prior models, the coefficient of landcover 8 has been positive or zero, but in this model, it is negative! This confuses the interpretation of landcover 8, but with only one discrepancy it does not necessarily ring alarm bells. Perhaps if we had the time to confirm scikit-learn’s loss function and implement the same optimization method we would achive more similar results.\n\n\nTest Model\n\n# Report results\nprint(\"MSE:\", LR_l2.mse(X_test_torch, y_test_torch),\n      \"\\nR^2:\", LR_l2.r2(X_test_torch, y_test_torch))\n\nMSE: tensor(399693.2117, dtype=torch.float64) \nR^2: tensor(0.6885, dtype=torch.float64)\n\n\nAt \\(399,692\\), our implementation’s MSE is more than scikit-learn’s \\(387,635\\) as well as all prior results. And at \\(0.6885\\), our implementation’s \\(R^2\\) is less than scikit-learn’s \\(0.6979\\) and all other results. We could achieve better results by modifying our parameter values, but we were unable to identically reproduce the output of scikit-learn. Overall, our results indicate that for this problem, regularization does not lead to improved performance on the testing data, although it may facilitate interpretation of coefficients."
  },
  {
    "objectID": "posts/final-project-post/index.html#discussion-of-linear-regression",
    "href": "posts/final-project-post/index.html#discussion-of-linear-regression",
    "title": "Project Blog Post",
    "section": "Discussion of Linear Regression",
    "text": "Discussion of Linear Regression\nIn linear regression, a major assumption is that all observations are independent of each other. However, when working with spatial data, nearby observations are often similar, such that observations are not independent if they are in close proximity to each other. In order to determine whether our model suffers from such spatial dependence, we will fit a linear regression model on the entire dataset and produce a map of our model’s residuals. We opt for linear regression without regularization due to its higher performance in the work above.\n\n# convert to torch tensors\n# add column of ones for y-intercept\nX_torch = torch.cat((torch.tensor(X.values), torch.ones((X.shape[0], 1))), 1)\ny_torch = torch.tensor(y.values)\n\n# fit linear regression model\nLR_full = LinearRegress()\nopt_full = GradientDescentOptimizer(LR_full)\n\n# fit model\nfor i in range(500000): \n    # update model\n    opt_full.step(X_torch, y_torch, alpha = 0.01)\n\n    # calculate and record loss\n    loss = LR_full.loss(X_torch, y_torch) \n\n\n# calculate residuals\nresid = (y_torch - LR_full.pred(X_torch))\n\n# add residual column to tracts\ntracts[\"resid\"] = resid\n\n# specify that color ramp should be centered at 0\ndivnorm = TwoSlopeNorm(vmin=-3000, vcenter=0., vmax = 3000)\n\n# create map\nresid_map = tracts.plot(\"resid\", legend = True, cmap = \"seismic\", norm = divnorm, figsize = (8,8))\nplt.title(\"Residual Map\");\n\n\n\n\n\n\n\n\nIn an ideal scenario with spatially independent observations, the values of residuals would be distributed randomly throughout the map. However, with clear clusters of red and blue, our model visually appears to be making similar errors in nearby places. In other words, our residuals suffer from spatial autocorrelation. This may occur because the population density in one tract influences the population density in another tract; similarly, the landcover in one tract may influence the population density in a neighboring tract. Fortunately, there exists an entire field of spatial statistics dedicated to addressing issues of spatial autocorrelation. In the following section, we will employ one technique, known as spatial lag regression, in order to account for spatial dependence and hopefully improve our results. Before continuing to our section on spatial autoregression, we first perform cross-validation on linear regression and report the average root mean squared error (RMSE) in order to compare our results to our autoregressive results. We will opt for scikit-learn’s linear regression class since it is faster and achieves identical results to ours.\n\n# define model\nLR = LinearRegression() \n\n# define scoring function\n# this is just required in order to use scikit-learn's cross_val_score function\n# basically they multiply the MSE by -1, so we need to account for that afterwards\nmse_score = make_scorer(mean_squared_error, greater_is_better = False)\n\n# cross validation\ncv_scores_LR = cross_val_score(estimator = LR, X = X, y = y, scoring = mse_score, cv = 4)\n\n# compute average RMSE\nnp.sqrt(-1*cv_scores_LR).mean()\n\n503.0545511056982\n\n\nWith regular linear regression, we have achieved an average cross-validation RMSE of \\(503\\) people per square kilometer. Let’s see if accounting for space can improve our results!"
  },
  {
    "objectID": "posts/final-project-post/index.html#data-processing-and-exploration",
    "href": "posts/final-project-post/index.html#data-processing-and-exploration",
    "title": "Project Blog Post",
    "section": "Data Processing and Exploration",
    "text": "Data Processing and Exploration\nIn this spatial autoregression model, we adopt queen criterion to construct spatial continuity weight matrix. The queen criterion defines neighbors as spatial units sharing a common edge or a common vertex. This means that in our model, we will add the features and characteristics of the neighboring tracts as part of the prediction variables.\nTo find the weight matrix, we need to introduce geometry to our dataset. Here, I am merging the csv file to a shapefile and convert the merged data to a GeoDataFrame format. Later, I calculate the queen spatial continuity matrix using the libpysal pacakge. Using the spatial weight continuity matrix, we can then calculate the spatial lag data of population density, which is the mean population density of the neighboring tracts.\n\n# import shapefile\n# need separate shapefile because the one form pygris didn't cooperate with the weights matrix functions\ndata = pd.read_csv(\"../data/combined_data.csv\")\ngdf = gpd.read_file('../data/tl_2016_09_tract.shp')\n\n# create merge columns\ngdf['TRACTCE'] = gdf['TRACTCE'].astype(int)\ndata['TRACTCE'] = data['TRACTCE'].astype(int)\n\n# merge csv with shapfile using TRACTCE\nmerged_gdf = gdf.merge(data, on='TRACTCE', how='left')\n\n# make merged_gdf into geo dataframe\nmerged_gdf = gpd.GeoDataFrame(merged_gdf)\n\n# drop out all rows that have no population density\nmerged_gdf = merged_gdf.dropna(subset=['PopDensity'], axis=0)\n\n# clean tracts that have truncated data on population density\nmerged_gdf = merged_gdf[merged_gdf['PopDensity'] != 0]\nmerged_gdf = merged_gdf[merged_gdf['TRACTCE'] != 194202]\n\n# define the geometry_x column to be the geometry feature \nmerged_gdf.set_geometry(\"geometry_x\", inplace=True)\n\n# calculate Queen's neighbor weights for each tracts\nw = lp.weights.Queen.from_dataframe(merged_gdf)\nw.transform = 'R'\n\n# compute spatial lag of population density\nmerged_gdf['spatial_lag_PopDens'] = lp.weights.lag_spatial(w, merged_gdf['PopDensity'])\n\n# calculate the mean pop density of each tract's neighbors\n#merged_gdf['avg_neighbor_density'] = merged_gdf.groupby('TRACTCE')['spatial_lag'].transform('mean')\nmerged_gdf['PopDensity'] = merged_gdf['PopDensity'].astype(float)\n\n# download merged_gdf to csv file\nmerged_gdf.to_csv('../data/merged_gdf.csv', index=False)\n\n/tmp/ipykernel_18572/2255761594.py:27: FutureWarning: `use_index` defaults to False but will default to True in future. Set True/False directly to control this behavior and silence this warning\n  w = lp.weights.Queen.from_dataframe(merged_gdf)\n\n\nNext, we want to perform a spatial autocorrelation evaluation using Global Moran’s I index. This evaluation assesses the spatial distribution characteristic of the entire region. We plot the scatter plot between the population denisty and mean population denisty of tract’s neighbors. The Global Moran’s I index, if we do not delve into its mathematical details, is the slope of the best fit line between these two numbers. In our case, we calculated the Moran’s I index to be 0.6. Together with the distribution of the scatter plot, we believe that population density of the neighboring tracts are dependent. We also want to inspect the spatial association at a local scale. The color of each tract is based on its own population density and the population density of its surrounding tracts.\nMoran’s Scatterplot has four categories: High-High, High-Low, Low-High, Low-Low. High/low before the dash means whether the tract has a populuation density that is higher/lower than the mean overall population density. High/low after the dash means whether the tract’s neighbors population denisty is above/below the average population density. After categorization, we map the tracts to inspect the distribution of the tracts’ categories. We find that High-High tracts are usually in urban areas, Low-High tracts are usually suburbs, High-Low tracts are typically towns in the rural area, and Low-Low are rural tracts. Therefore, we believe that by taking into account the characteristics of the target tract’s neighboring tract, we are able to predict population density better than ordinary least square regression.\n\n# read data\nmerged_csv_moran = pd.read_csv(\"../data/merged_gdf.csv\", usecols=['PopDensity', 'spatial_lag_PopDens', \"Geo_NAME\"]).dropna()\n\n# Extract x and y columns from the DataFrame\nx = merged_csv_moran['PopDensity'].values.reshape(-1, 1)  # Reshape to make it a 2D array for scikit-learn\ny = merged_csv_moran['spatial_lag_PopDens'].values\n\n# Calculate the average for 'spatial_lag_PopDens' and 'PopDensity'\np = merged_csv_moran['spatial_lag_PopDens'].mean()\nq = merged_csv_moran['PopDensity'].mean()\n\n# Categorize the rows based on conditions\nmerged_csv_moran['category'] = 0  # Initialize category column\nmerged_csv_moran.loc[(merged_csv_moran['spatial_lag_PopDens'] &gt;= p) & (merged_csv_moran['PopDensity'] &gt;= q), 'category'] = 'High-High'\nmerged_csv_moran.loc[(merged_csv_moran['spatial_lag_PopDens'] &gt;= p) & (merged_csv_moran['PopDensity'] &lt; q), 'category'] = 'Low-High'\nmerged_csv_moran.loc[(merged_csv_moran['spatial_lag_PopDens'] &lt; p) & (merged_csv_moran['PopDensity'] &gt;= q), 'category'] = 'High-Low'\nmerged_csv_moran.loc[(merged_csv_moran['spatial_lag_PopDens'] &lt; p) & (merged_csv_moran['PopDensity'] &lt; q), 'category'] = 'Low-Low'\n\n# Calculate the average for 'spatial_lag_PopDens' and 'PopDensity'\np = merged_csv_moran['spatial_lag_PopDens'].mean()\nq = merged_csv_moran['PopDensity'].mean()\n\n# Define custom colors for categories\ncolors = {'High-High': '#F47E3E', 'Low-Low': '#0FA3B1', 'Low-High': '#D9E5D6', 'High-Low': '#DCC156'}\n\n# Create a scatter plot of x vs y\nscatter = plt.scatter(x, y, color=merged_csv_moran['category'].map(colors))\n\n# Fit a linear regression model\nmodel = LinearRegression()\nmodel.fit(x, y)\n\n# Get the slope and intercept of the fitted line\nslope = model.coef_[0]\nintercept = model.intercept_\n\n# Plot the fitted line\nplt.plot(x, model.predict(x), color='red', label=f'Linear Regression (y = {slope:.2f}x + {intercept:.2f})')\n\n# Add labels and title\nplt.xlabel('Population Density')\nplt.ylabel('Spatial Lag Density')\nplt.title(\"Moran's I = 0.60\")\n\n# Create legend entries manually\nlegend_patches = [\n    Patch(color=color, label=label) for label, color in colors.items()\n]\n\n# Add the legend with custom entries and regression equation\nplt.legend(handles=legend_patches + [scatter, plt.Line2D([0], [0], color='red', label=f'(y = {slope:.2f}x + {intercept:.2f})')])\n\n# Draw horizontal and vertical dashed line at y = p\nplt.axhline(y=p, color='gray', linestyle='--')\nplt.axvline(x=q, color='gray', linestyle='--')\n\n# Show plot\nplt.show()\n\n/tmp/ipykernel_18572/4162410205.py:14: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value 'High-High' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n  merged_csv_moran.loc[(merged_csv_moran['spatial_lag_PopDens'] &gt;= p) & (merged_csv_moran['PopDensity'] &gt;= q), 'category'] = 'High-High'\n/tmp/ipykernel_18572/4162410205.py:51: MatplotlibDeprecationWarning: An artist whose label starts with an underscore was passed to legend(); such artists will no longer be ignored in the future.  To suppress this warning, explicitly filter out such artists, e.g. with `[art for art in artists if not art.get_label().startswith('_')]`.\n  plt.legend(handles=legend_patches + [scatter, plt.Line2D([0], [0], color='red', label=f'(y = {slope:.2f}x + {intercept:.2f})')])\n\n\n\n\n\n\n\n\n\n\n# Calculate the average for 'spatial_lag_PopDens' and 'PopDensity'\np = merged_gdf['spatial_lag_PopDens'].mean()\nq = merged_gdf['PopDensity'].mean()\n\n# Categorize the rows based on conditions\nmerged_gdf['category'] = 0  # Initialize category column\nmerged_gdf.loc[(merged_gdf['spatial_lag_PopDens'] &gt;= p) & (merged_gdf['PopDensity'] &gt;= q), 'category'] = 'High-High'\nmerged_gdf.loc[(merged_gdf['spatial_lag_PopDens'] &gt;= p) & (merged_gdf['PopDensity'] &lt; q), 'category'] = 'Low-High'\nmerged_gdf.loc[(merged_gdf['spatial_lag_PopDens'] &lt; p) & (merged_gdf['PopDensity'] &gt;= q), 'category'] = 'High-Low'\nmerged_gdf.loc[(merged_gdf['spatial_lag_PopDens'] &lt; p) & (merged_gdf['PopDensity'] &lt; q), 'category'] = 'Low-Low'\n\n# Define custom colors for categories\ncolors = {'High-High': '#F47E3E', 'Low-Low': '#0FA3B1', 'Low-High': '#D9E5D6', 'High-Low': '#DCC156'}\n\n# Plot the map using custom colors\nfig, ax = plt.subplots(figsize=(10, 10))\nmerged_gdf.plot(column='category', ax=ax, color=merged_gdf['category'].map(colors), legend=True)\nplt.title('Map of Moran Scatterplot Quadrants')\nplt.show()\n\n/tmp/ipykernel_18572/3545356962.py:7: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value 'High-High' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n  merged_gdf.loc[(merged_gdf['spatial_lag_PopDens'] &gt;= p) & (merged_gdf['PopDensity'] &gt;= q), 'category'] = 'High-High'\n/tmp/ipykernel_18572/3545356962.py:17: UserWarning: Only specify one of 'column' or 'color'. Using 'color'.\n  merged_gdf.plot(column='category', ax=ax, color=merged_gdf['category'].map(colors), legend=True)\n\n\n\n\n\n\n\n\n\nInstead of using all possible land cover types, we are going to use land cover types that are more common among all tracts in CT for density prediction. The land cover types we selected are the same as the ones in linear regression section.\n\n# All landcover types\nall_landcover = ['2', '5', '8', '11', '12', '13', '14', '15', '17', '18', '19', '20', '21', '22', '7', '6', '0', '23']\nall_landcover_pct = ['2pct', '5pct', '8pct', '11pct', '12pct', '13pct', '14pct', '15pct', '17pct', '18pct', '19pct', '20pct', '21pct', '22pct', '7pct', '6pct', '0pct', '23pct']\n\n# Select landcover types\nlandcover_types = ['2', '5', '11', '12', '8', '13', '14', '15', '20', '21'] #, '22', '7', '8', '13', '14', '15', '20', '21'\nlandcover_pct = ['2pct', '5pct', '11pct', '12pct', '8pct', '13pct', '14pct', '15pct', '20pct', '21pct'] # , '22pct', '7pct', '8pct', '13pct', '14pct', '15pct', '20pct', '21pct'\n\n# Merge them into our data\nmerged_gdf['sum'] = merged_gdf[all_landcover].sum(axis=1)\nmerged_gdf[all_landcover_pct] = merged_gdf[all_landcover].div(merged_gdf['sum'], axis=0).multiply(100).astype(float)\n\n# Download merged_gdf to csv file optionally \n#merged_gdf.to_csv('merged_gdf_saved.csv', index=False)"
  },
  {
    "objectID": "posts/final-project-post/index.html#spatial-lag-regression",
    "href": "posts/final-project-post/index.html#spatial-lag-regression",
    "title": "Project Blog Post",
    "section": "Spatial Lag Regression",
    "text": "Spatial Lag Regression\n\nEndogenous vs. Exogenous: What’s the Difference?\nThere are two types of spatially lagged regression models. The first one is spatially lagged endogenous regression model. The endogenous model includes the spatial lagged value of the target variable as one of the explanatory variables for regression. In our case, the population density of a tract’s neighbor is part of the variables we use to predict the population density of the tract.\nThe second type of spatially lagged regression model is spatially lagged exogenous regression model. Instead of taking into account the population density, our target variable, of the neighboring tracts, the exogenous model considers the explanatory variables of the tract’s surroundings. In our case, the spatially lagged exogenous model adds neighbors’ land type information to the model. We will calculate the spatial lagged value of each land cover type for all tracts and include them as part of the predictor variables.\nWe first fit both models to the entirety of CT and map their residuals on each tract. First, we fit the endogenous model.\n\n# Endogenous model: consider spatial lag population denisty\npredictor = landcover_pct + ['spatial_lag_PopDens']\n\n# Get explanatory variables and target variable\nX_merged_gdf = merged_gdf[predictor].values\ny_merged_gdf = merged_gdf['PopDensity'].values.reshape(-1, 1)\n\n# Create, fit, and predict with Linear Regression\nmodel = LinearRegression()\nmodel.fit(X_merged_gdf, y_merged_gdf)\ny_pred = model.predict(X_merged_gdf)\n\n# Calculate residuals \nresiduals = y_merged_gdf - y_pred\nmerged_gdf['residuals'] = residuals\n\n# Remove Spatial lag so that our Exogenous model does not take this into account\nmerged_gdf.drop(columns=['spatial_lag_PopDens'], inplace=True)\n\nNext, we fit the exogenous model.\n\n# Exogenous model: consider\nexo_predictor = landcover_pct + ['lag_2pct', 'lag_5pct', 'lag_11pct', 'lag_12pct', 'lag_8pct', 'lag_13pct', 'lag_14pct', 'lag_15pct', 'lag_20pct', 'lag_21pct'] \n\nfor i in range(len(landcover_pct)):\n        merged_gdf['lag_' + landcover_pct[i]] = lp.weights.lag_spatial(w, merged_gdf[landcover_pct[i]])\n\n# Get explanatory variables and target variable\nX_merged_gdf_exo = merged_gdf[exo_predictor].values\ny_merged_gdf_exo = merged_gdf['PopDensity'].values.reshape(-1, 1)\n\n#Create, fit, and predict with Linear Regression\nmodel_exo = LinearRegression()\nmodel_exo.fit(X_merged_gdf_exo, y_merged_gdf_exo)\ny_pred_exo = model_exo.predict(X_merged_gdf_exo)\n\n#Calculate Residuals and make new column\nresiduals_exo = y_merged_gdf_exo - y_pred_exo\nmerged_gdf['residuals_exo'] = residuals_exo\n\nNow, we visualize the map of residuals for both models.\n\n# Define the colors for the custom colormap\ncolors = [(0, 'brown'), (0.5, 'white'), (1, 'green')]  # Position 0 is brown, position 0.5 is white, position 1 is green\n\n# Create the colormap\ncmap = LinearSegmentedColormap.from_list('custom_cmap', colors)\n\n# Determine the range of residuals to be used for normalization\nresiduals_max = max(abs(merged_gdf['residuals_exo'].max()), abs(merged_gdf['residuals'].max()))\nvmax = residuals_max * 0.75  # Adjust the factor as needed\n\n# Create a normalization object\nnorm = Normalize(vmin=-vmax, vmax=vmax)\n\n# First graph\nfig, axes = plt.subplots(1, 2, figsize=(20, 10))  # Create a figure with 1 row and 2 columns\n\n# Graph 1 - Exogenous variables\nmerged_gdf.plot(column='residuals_exo', cmap=cmap, legend=True, ax=axes[0], vmax=vmax, norm=norm)\naxes[0].set_title('Spatial Distribution of Residuals (Exogenous)')\naxes[0].set_xlabel('Longitude')\naxes[0].set_ylabel('Latitude')\n\n# Graph 2 - Spatial lag of PopDensity\nmerged_gdf.plot(column='residuals', cmap=cmap, legend=True, ax=axes[1], vmax=vmax, norm=norm)\naxes[1].set_title('Spatial Distribution of Residuals (Endogenous)')\naxes[1].set_xlabel('Longitude')\naxes[1].set_ylabel('Latitude')\n\nplt.show()\n\n\n\n\n\n\n\n\n\nnum_bins = 50\nhist_range = (0, 2000)\n\n# Create subplots with two columns\nfig, axs = plt.subplots(1, 2, figsize=(12, 6))\n\n# Plot the first histogram\naxs[0].hist(merged_gdf['residuals_exo'], bins=num_bins, range=hist_range, color='green')\naxs[0].set_xlabel('Absolute Residual')\naxs[0].set_ylabel('Number of Rows')\naxs[0].set_title('Distribution of Absolute Residuals (exogenous)')\n\n# Plot the second histogram\naxs[1].hist(merged_gdf['residuals'], bins=num_bins, range=hist_range, color='green')\naxs[1].set_xlabel('Absolute Residual')\naxs[1].set_ylabel('Number of Rows')\naxs[1].set_title('Distribution of Absolute Residuals (endogenous)')\n\n# Adjust layout\nplt.tight_layout()\n\n# Show plots\nplt.show()\n\n\n\n\n\n\n\n\nThe exogenous spatial lag residual map is on the left and the endogenous spatial lag residual map is on the right. Qualitatively assessing the these two maps, we see both models tend to underestimate the population density in urban areas. It is reasonable as land cover data is only two dimensional and does not account for the vertical height of the buildings. We also see slightly differences in prediction of rural areas between Exogenous and Endogenous. Endogenous is more accurate in rural areas as the landcover is not a sole factor of prediction, and it instead takes into account the population density of the tracts around it. Exogenous is slightly more inaccurate in these regions for the lack of this parameter. Both models tend to have a better performance at predicting density in less populated areas (Low-Low tracts).\nContinuing to explore, we created two residual histograms. We noted that they are similar to our map of CT and do not present any new pattern.\nTo explore a bit deeper into our dataset, we will look at a snapshot of our map around Hartford and its neighboring cities.\n\n# Create a normalization object\nnorm = Normalize(vmin=-2000, vmax=2000)\n\n# First graph\nfig, axes = plt.subplots(1, 2, figsize=(20, 10))  # Create a figure with 1 row and 2 columns\n\n# Graph 2 - Exogenous variables\nmerged_gdf.plot(column='residuals_exo', cmap=cmap, legend=True, ax=axes[0], vmax=vmax, norm=norm)\naxes[0].set_title('Spatial Distribution of Residuals Near Hartford (Exogenous)')\naxes[0].set_xlabel('Longitude')\naxes[0].set_ylabel('Latitude')\n\n# Graph 1 - Spatial lag of PopDensity\nmerged_gdf.plot(column='residuals', cmap=cmap, legend=True, ax=axes[1], vmax=vmax, norm=norm)\naxes[1].set_title('Spatial Distribution of Residuals Near Hartford (Endogenous)')\naxes[1].set_xlabel('Longitude')\naxes[1].set_ylabel('Latitude')\n\naxes[0].set_ylim([41.6, 41.8])\naxes[0].set_xlim([-72.9, -72.5])\naxes[1].set_ylim([41.6, 41.8])\naxes[1].set_xlim([-72.9, -72.5])\n\nplt.show()\n\n\n\n\n\n\n\n\nOne area we were particularly curious about was Hartford as it is a large urban hub in the central of Connecticut. We noticed that we were grossly underestimating densely populated areas, which makes sense as they are relatively large outliers from the rest of the relatively lower population and spread out suburban areas of Connecticut. However, we were better at calculating more densely populated areas with the Endogenous model. We hypothesize this is due to the fact that Endogenous Models inherently take into account the population densities of neighboring tracts. Thus, there is a greater likelihood that the model will “self-correct” by knowing the population of its neighbors."
  },
  {
    "objectID": "posts/final-project-post/index.html#training-and-testing-cross-validation",
    "href": "posts/final-project-post/index.html#training-and-testing-cross-validation",
    "title": "Project Blog Post",
    "section": "Training and Testing Cross Validation",
    "text": "Training and Testing Cross Validation\nFor training and testing, we need to separate the data into two. Due to the spatial dependence of tracts, we cannot randomly select tracts from the dataset and assign them to either training or testing data because neighboring tracts will not be in the same dataset. Therefore, to minimize the rupture of spatial relations, we decide to separate training and testing data by neighboring counties to ensure that all tracts in training and testiing data are countinuous. Later, we perform for loops on each set of training and testing data and calculate their mean RMSE for each training and testing set for both endogenous and exogenous model.\n\nmerged_csv = pd.read_csv(\"../data/merged_gdf.csv\")\n\n# Extract the county name from the the Geo_NAME column. \nmerged_gdf['County'] = merged_gdf['Geo_NAME'].str.split(',').str[1].str.strip().str.replace(' ', '')\nmerged_gdf = merged_gdf.dropna(subset=['County'])\n\n\n# Spatially lagged endogenous regressor\nodd_counties = ['NewLondonCounty', 'NewHavenCounty', 'LitchfieldCounty', 'TollandCounty']\neven_counties = ['MiddlesexCounty', 'FairfieldCounty','HartfordCounty', 'WindhamCounty']\n\nrmse = []\n\nfor i in range(4):\n    # Splitting training and testing counties\n    train_1 = merged_gdf[(merged_gdf['County'] != odd_counties[i]) & (merged_gdf['County'] != even_counties[i])]\n    test_1 = merged_gdf[(merged_gdf['County'] == odd_counties[i]) | (merged_gdf['County'] == even_counties[i])]\n\n    # Queen weight matrix for each train and test\n    train_1_w = lp.weights.Queen.from_dataframe(train_1)\n    test_1_w = lp.weights.Queen.from_dataframe(test_1)\n    \n    # Regularize the weights\n    train_1_w.transform = 'R'\n    test_1_w.transform = 'R'\n    \n    # Calculate the spatial lag pop density\n    train_1['spatial_lag_PopDens'] = lp.weights.lag_spatial(train_1_w, train_1['PopDensity'])\n    test_1['spatial_lag_PopDens'] = lp.weights.lag_spatial(test_1_w, test_1['PopDensity'])\n    \n    y_train = np.array(train_1['PopDensity']).reshape((-1,1))\n    x_train = np.array(train_1[predictor])\n\n    y_test = np.array(test_1['PopDensity'])\n    x_test = np.array(test_1[predictor])\n\n    # Fit linear regression model using scikit-learn \n    model = LinearRegression()\n    model.fit(x_train, y_train)\n\n    # Predict on test data\n    y_pred_test = model.predict(x_test)\n\n    # Calculate RMSE\n    test_rmse = np.sqrt(mean_squared_error(y_test, y_pred_test))\n\n    rmse.append(test_rmse)\n\n\nnp.mean(rmse)\n\n382.27553702535505\n\n\nThe average root mean square error of the spatially lagged endogenous regression model is 382.28. The endogenous model is more advantageous when we have a relatively higher coverage of census data and we need to predict the population density of small region surrounded by regions with good census.\nNext, we do training and testing cross validation for the exogenous spatial lagged model.\n\n# Spatially lagged exogenous regressors\n\nrmse_exo = []\n\n# Set loops for each set of different counties\nfor i in range(4):\n\n    train_1 = merged_gdf[(merged_gdf['County'] != odd_counties[i]) & (merged_gdf['County'] != even_counties[i])]\n    test_1 = merged_gdf[(merged_gdf['County'] == odd_counties[i]) | (merged_gdf['County'] == even_counties[i])]\n\n    train_1_w = lp.weights.Queen.from_dataframe(train_1)\n    test_1_w = lp.weights.Queen.from_dataframe(test_1)\n\n    train_1_w.transform = 'R'\n    test_1_w.transform = 'R'\n\n    # Calculate spatial lag \n    for j in range(len(landcover_pct)):\n        train_1['lag_' + landcover_pct[j]] = lp.weights.lag_spatial(train_1_w, train_1[landcover_pct[j]])\n        test_1['lag_' + landcover_pct[j]] = lp.weights.lag_spatial(test_1_w, test_1[landcover_pct[j]])\n    \n    # Extract training and test data \n    y_train = np.array(train_1['PopDensity']).reshape((-1,1))\n    x_train = np.array(train_1[exo_predictor])\n\n    y_test = np.array(test_1['PopDensity'])\n    x_test = np.array(test_1[exo_predictor])\n\n    # Fit linear regression model using scikit-learn \n    model = LinearRegression()\n    model.fit(x_train, y_train)\n\n    # Predict on test data\n    y_pred_test = model.predict(x_test)\n\n    # Calculate RMSE\n    test_rmse = np.sqrt(mean_squared_error(y_test, y_pred_test))\n\n    rmse_exo.append(test_rmse)\n\n\nnp.mean(rmse_exo)\n\n391.66561692553\n\n\nThe average RMSE of the spatially lagged endogenous regression model cross validation is 391.67, which is slightly larger than the RMSE of the endogenous model. The exogenous model is more applicable to scenarios when we have good satellite data but sparse census data.\nComparing our Spatial Autoregression to our Linear regression, it is clear that our Spatial Regression yields better results."
  },
  {
    "objectID": "posts/final-project-post/index.html#concluding-discussion",
    "href": "posts/final-project-post/index.html#concluding-discussion",
    "title": "Project Blog Post",
    "section": "Concluding Discussion",
    "text": "Concluding Discussion\nThrough this project, we were able to implement three different forms of Linear Regression, as well as create Spatial Autoregression models, and we determined the efficacy of each of these models both mathematically and graphically. Our results were relatively similar to Tian et al. (2005) in that they underpredicted the population density in densely populated urban areas more frequently than other plots of land, and over-predicted population density in rural areas. Overall, we accomplished a few key things with project. Through our models, we were able to predict population density with only landcover with relatively strong accuracy. We successfully compared different machine learning models and concluded that Spatial Autoregression was more accurate than Linear Regression. With more time, we would have liked to implement Poisson Regression and performed analysis at the block group level instead of tract level. With more computational power, we would have liked to calculate a larger dataset, representing a larger spatial region. Overall, we are proud of our work!"
  },
  {
    "objectID": "posts/final-project-post/index.html#group-contributions-statement",
    "href": "posts/final-project-post/index.html#group-contributions-statement",
    "title": "Project Blog Post",
    "section": "Group Contributions Statement",
    "text": "Group Contributions Statement\nLiam helped with data acquisition and preparation, wrote our implementation of linear regression with gradient descent in linear_regression.py, and compared the output of our class with that of scikit-learn. Alex acquired landcover data from Conus, and shapefile data of Connecticut. He then implemented zonal statistics with Manny. Alex explained the differences in Spatial Autoregression methods, trained the models, and utilized cross validation. Manny created visualizations of our models to represent graphically the residuals of each model. He proof-read all of our code, making corrections, rewriting descriptions, and ensuring cohesive and consistent writing styles. He also contributed to code in the Spatial Auto Regression section."
  },
  {
    "objectID": "posts/final-project-post/index.html#personal-reflection",
    "href": "posts/final-project-post/index.html#personal-reflection",
    "title": "Project Blog Post",
    "section": "Personal Reflection",
    "text": "Personal Reflection\nAlex: throughout the process of researching and implementing my project, I learned the background logic and the implementation of spatial autoregressive model. I was initially trying to use packages that do spatial lag regression, but found the packages not helpful enough for train-test cross validation and model fitting. This motivates me to learn the background knowledge and mathematics of the model. The process of communicating the spatial autoregressive model to my teamates and during the presentation also improved my understanding of the model. I feel very satisfied with what we have accomplished, because the performance of the spatial lagged model is better than the OLS, as expected. We did not have enough time for the Poisson regression, which deviates from our initial goal. I would also love to add penalty term to the regression of the spatial model, but unfortunately we did not have enough time for this. What I learned from this project, especially the spatial lagged model, will contribute to future research on topics with spatial component."
  },
  {
    "objectID": "posts/deepMusic-post/index.html",
    "href": "posts/deepMusic-post/index.html",
    "title": "Deep Music Genre Classification",
    "section": "",
    "text": "In this blog post, we will construct and implement deep learning neural network to classify song genres. The dataset we will be using is collected by researchers from Spotify and is available through Kaggle. The blog will build and train three different neural networks: one only based on lyrics, one only based on engineered features, and one on both lyrics and features. We will do training and testing and compare the performance of these three models.\n\n\n\n\n\nFirst, we acquire data from the url. Here are some first rows of the data.\n\nimport pandas as pd\n\nurl = \"https://raw.githubusercontent.com/PhilChodrow/PIC16B/master/datasets/tcc_ceds_music.csv\"\ndf = pd.read_csv(url)\n\n\ndf.head()\n\n\n\n  \n    \n\n\n\n\n\n\nUnnamed: 0\nartist_name\ntrack_name\nrelease_date\ngenre\nlyrics\nlen\ndating\nviolence\nworld/life\n...\nsadness\nfeelings\ndanceability\nloudness\nacousticness\ninstrumentalness\nvalence\nenergy\ntopic\nage\n\n\n\n\n0\n0\nmukesh\nmohabbat bhi jhoothi\n1950\npop\nhold time feel break feel untrue convince spea...\n95\n0.000598\n0.063746\n0.000598\n...\n0.380299\n0.117175\n0.357739\n0.454119\n0.997992\n0.901822\n0.339448\n0.137110\nsadness\n1.0\n\n\n1\n4\nfrankie laine\ni believe\n1950\npop\nbelieve drop rain fall grow believe darkest ni...\n51\n0.035537\n0.096777\n0.443435\n...\n0.001284\n0.001284\n0.331745\n0.647540\n0.954819\n0.000002\n0.325021\n0.263240\nworld/life\n1.0\n\n\n2\n6\njohnnie ray\ncry\n1950\npop\nsweetheart send letter goodbye secret feel bet...\n24\n0.002770\n0.002770\n0.002770\n...\n0.002770\n0.225422\n0.456298\n0.585288\n0.840361\n0.000000\n0.351814\n0.139112\nmusic\n1.0\n\n\n3\n10\npérez prado\npatricia\n1950\npop\nkiss lips want stroll charm mambo chacha merin...\n54\n0.048249\n0.001548\n0.001548\n...\n0.225889\n0.001548\n0.686992\n0.744404\n0.083935\n0.199393\n0.775350\n0.743736\nromantic\n1.0\n\n\n4\n12\ngiorgos papadopoulos\napopse eida oneiro\n1950\npop\ntill darling till matter know till dream live ...\n48\n0.001350\n0.001350\n0.417772\n...\n0.068800\n0.001350\n0.291671\n0.646489\n0.975904\n0.000246\n0.597073\n0.394375\nromantic\n1.0\n\n\n\n\n5 rows × 31 columns\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\n\nHere are the engineered features of the data. Each song receives a score below each category.\n\nengineered_features = ['dating', 'violence', 'world/life', 'night/time','shake the audience','family/gospel', 'romantic', 'communication','obscene', 'music', 'movement/places', 'light/visual perceptions','family/spiritual', 'like/girls', 'sadness', 'feelings', 'danceability','loudness', 'acousticness', 'instrumentalness', 'valence', 'energy']\n\nHere are the genres of music in the dataset. We first want to encode the genre. Here I use LabelEncoder from scikit learn.\n\ndf[\"genre\"].unique()\n\narray(['pop', 'country', 'blues', 'jazz', 'reggae', 'rock', 'hip hop'],\n      dtype=object)\n\n\n\nfrom sklearn.preprocessing import LabelEncoder\n\n# Creating a instance of label Encoder.\nle = LabelEncoder()\n\n# Using .fit_transform function to fit label encoder and return encoded label\nlabel = le.fit_transform(df['genre'])\n\ndf.drop(\"genre\", axis=1, inplace=True)\n\n# Appending the array to our dataFrame with column name 'Purchased'\ndf[\"genre\"] = label\n\n\ndf['genre'].unique()\n\narray([4, 1, 0, 3, 5, 6, 2])\n\n\nHere is the percentage of each genre in the entire dataset. The music genre that occupies the largest proportion is Pop music (around 24.8%). It sets the base classification rate to be 24.8%.\n\ndf.groupby(\"genre\").size() / len(df)\n\ngenre\n0    0.162273\n1    0.191915\n2    0.031862\n3    0.135521\n4    0.248202\n5    0.088045\n6    0.142182\ndtype: float64\n\n\n\n\n\nNext, we want to define a data loader class, which we will use to load only batches of the data at a time. The indexer methods in TextDataFromDF class will return columns of lyrics, genre, and engineered features.\n\nfrom torch.utils.data import Dataset, DataLoader\n\nclass TextDataFromDF(Dataset):\n    def __init__(self, df):\n        self.df = df\n\n    def __getitem__(self, index):\n        # 4 is lyrics, 30 is genre, 6-28 are features\n        return self.df.iloc[index, 4], self.df.iloc[index, 30], self.df.iloc[index, 6:28].values\n\n    def __len__(self):\n        return len(self.df)\n\n\nfrom sklearn.model_selection import train_test_split\n\ndf_train, df_val = train_test_split(df,shuffle = True, test_size = 0.2)\ntrain_data = TextDataFromDF(df_train)\nval_data   = TextDataFromDF(df_val)\n\n\ntrain_data[194]\n\n('schmit couldn reason felt kind blue watch hello sweet friend come sing lullaby amend mama lookin acan start anew search finally come realize tellin lie livin afraid read line different today shin know couldn change mind twist point view kind choose reason lose lookin silly game',\n 4,\n array([0.0013157896412805, 0.0013157895964813, 0.322708678411067,\n        0.0013157896186701, 0.0013157894770102, 0.0279231073879107,\n        0.00131578966616, 0.2961412546260131, 0.1707160272570204,\n        0.0959860879895431, 0.001315789547777, 0.0423188121300265,\n        0.0284165576046225, 0.0013157895199298, 0.00131578953515,\n        0.0013157894850158, 0.5689375067691976, 0.5644437607363535,\n        0.3122483054701862, 0.0001751012145748, 0.4466199505358615,\n        0.4394219318961712], dtype=object))\n\n\n\n\n\nFurther, we are going to tokenize the song lyrics and make it available for training. Basically, we split the sentence into words and assign number to each unique word.\n\nfrom torchtext.data.utils import get_tokenizer\nfrom torchtext.vocab import build_vocab_from_iterator\n\ntokenizer = get_tokenizer('basic_english')\n\n\ndef yield_tokens(data_iter):\n    for text, features, _ in data_iter:\n        yield tokenizer(text)\n\nvocab = build_vocab_from_iterator(yield_tokens(train_data), specials=[\"&lt;unk&gt;\"])\nvocab.set_default_index(vocab[\"&lt;unk&gt;\"])\n\n\n\n\nWe construct the function that is going to actually pass a batch of data to our training loop. The collation function will return three elements each set: the lyrics, the label (genre), and the features.\n\nimport torch\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmax_len = 100\nnum_tokens = len(vocab.get_itos())\ndef text_pipeline(x):\n    tokens = vocab(tokenizer(x))\n    y = torch.zeros(max_len, dtype=torch.int64) + num_tokens\n    if len(tokens) &gt; max_len:\n        tokens = tokens[0:max_len]\n    y[0:len(tokens)] = torch.tensor(tokens,dtype=torch.int64)\n    return y\n\nlabel_pipeline = lambda x: int(x)\n\n\ndef collate_batch(batch):\n    label_list, text_list, feature_list = [], [], []\n\n    for (_text, _label, _features) in batch:\n\n        # add label to list\n         label_list.append(label_pipeline(_label))\n\n         # add text (as sequence of integers) to list\n         processed_text = text_pipeline(_text)\n         text_list.append(processed_text)\n\n        # add feature as float to list\n         feature_list.append(torch.tensor(_features.astype(float), dtype=torch.float32))\n\n    label_list = torch.tensor(label_list, dtype=torch.int64)\n    text_list = torch.stack(text_list)\n    feature_list = torch.stack(feature_list)\n\n    return text_list.to(device), label_list.to(device), feature_list.to(device)\n\ntrain_loader = DataLoader(train_data, batch_size=8, shuffle=True, collate_fn=collate_batch)\nval_loader = DataLoader(val_data, batch_size=8, shuffle=True, collate_fn=collate_batch)\n\n\n\n\n\nimport time\n\ndef train(model, dataloader, use_lyrics = True, use_features = False):\n    epoch_start_time = time.time()\n    # keep track of some counts for measuring accuracy\n    total_acc, total_count = 0, 0\n    log_interval = 300\n    start_time = time.time()\n\n\n    for idx, (text, label, features) in enumerate(dataloader):\n\n        # zero gradients\n        optimizer.zero_grad()\n\n        # customize model input based on conditions\n        if use_lyrics and not use_features:\n          input = text\n        elif use_features and not use_lyrics:\n          input = features\n        else:\n          input = (text, features)\n\n        # form prediction on batch\n        predicted_label = model(input)\n\n        # evaluate loss on prediction\n        loss = loss_fn(predicted_label, label)\n\n        # compute gradient\n        loss.backward()\n        # take an optimization step\n        optimizer.step()\n\n        # for printing accuracy\n        total_acc   += (predicted_label.argmax(1) == label).sum().item()\n        total_count += label.size(0)\n\n    print(f'| epoch {epoch:3d} | train accuracy {total_acc/total_count:8.3f} | time: {time.time() - epoch_start_time:5.2f}s')\n\ndef evaluate(model, dataloader, use_lyrics = True, use_features = False):\n\n    total_acc, total_count = 0, 0\n\n    with torch.no_grad():\n        for idx, (text, label, features) in enumerate(dataloader):\n\n            if use_lyrics and not use_features:\n              input = text\n            elif use_features and not use_lyrics:\n              input = features\n            else:\n              input = (text, features)\n\n            predicted_label = model(input)\n            total_acc += (predicted_label.argmax(1) == label).sum().item()\n            total_count += label.size(0)\n    return total_acc/total_count\n\n\n\n\n\nWe are training our model only using the lyrics. The neural network architecture I used closely aligns to Professor Chodrow’s lecture note on Text Classification and Word Embedding. This network consists of an input embedding layer and an output linear layer.\n\nfrom torch import nn\n\n# first neural network that uses lyrics\nclass TextClassificationModel(nn.Module):\n\n    def __init__(self, vocab_size, embedding_dim, max_len, num_class):\n        super().__init__()\n        self.embedding = nn.Embedding(vocab_size+1, embedding_dim)\n        self.fc   = nn.Linear(max_len*embedding_dim, num_class)\n\n    def forward(self, x):\n        x = self.embedding(x)\n        x = torch.flatten(x, 1)\n        x = self.fc(x)\n        return(x)\n\nWe set the word embedding dimension to be 4.\n\nvocab_size = len(vocab)\nembedding_dim = 4\n\n# initialize the model\nlyricsModel = TextClassificationModel(vocab_size, embedding_dim, max_len, 7).to(device)\n\n\n# initialize ADAM optimizer and loss function\noptimizer = torch.optim.Adam(lyricsModel.parameters(), lr=.001)\nloss_fn = torch.nn.CrossEntropyLoss()\n\n# set epochs to be 30\nEPOCHS = 30\nfor epoch in range(1, EPOCHS + 1):\n    train(lyricsModel, train_loader, use_lyrics=True, use_features=False)\n\n| epoch   1 | train accuracy    0.207 | time: 24.12s\n| epoch   2 | train accuracy    0.238 | time: 25.01s\n| epoch   3 | train accuracy    0.258 | time: 25.73s\n| epoch   4 | train accuracy    0.281 | time: 26.31s\n| epoch   5 | train accuracy    0.301 | time: 24.88s\n| epoch   6 | train accuracy    0.334 | time: 24.94s\n| epoch   7 | train accuracy    0.357 | time: 25.92s\n| epoch   8 | train accuracy    0.379 | time: 24.62s\n| epoch   9 | train accuracy    0.410 | time: 25.32s\n| epoch  10 | train accuracy    0.438 | time: 25.38s\n| epoch  11 | train accuracy    0.458 | time: 24.86s\n| epoch  12 | train accuracy    0.484 | time: 24.88s\n| epoch  13 | train accuracy    0.505 | time: 25.81s\n| epoch  14 | train accuracy    0.525 | time: 25.73s\n| epoch  15 | train accuracy    0.546 | time: 26.38s\n| epoch  16 | train accuracy    0.564 | time: 24.84s\n| epoch  17 | train accuracy    0.582 | time: 24.96s\n| epoch  18 | train accuracy    0.599 | time: 24.71s\n| epoch  19 | train accuracy    0.615 | time: 25.89s\n| epoch  20 | train accuracy    0.636 | time: 26.10s\n| epoch  21 | train accuracy    0.653 | time: 25.23s\n| epoch  22 | train accuracy    0.665 | time: 25.25s\n| epoch  23 | train accuracy    0.679 | time: 25.18s\n| epoch  24 | train accuracy    0.694 | time: 27.20s\n| epoch  25 | train accuracy    0.704 | time: 24.96s\n| epoch  26 | train accuracy    0.717 | time: 26.47s\n| epoch  27 | train accuracy    0.729 | time: 24.36s\n| epoch  28 | train accuracy    0.738 | time: 25.45s\n| epoch  29 | train accuracy    0.750 | time: 25.02s\n| epoch  30 | train accuracy    0.762 | time: 24.65s\n\n\nWe see the training accuracy increases over epochs and arrives at 76.2% accuracy. Now we evaluate the model on the testing data.\n\nevaluate(lyricsModel, val_loader, use_lyrics=True, use_features=False)\n\n0.26343612334801764\n\n\nThe accuracy of the first neural network using only lyric only gives us 26.3% accuracy, which is only slightly larger than the base rate. Compare with 76.2% for training data, we suspect that the model was overfitted.\n\n\n\nNow, we only use engineered features to train the neural network. The neural network architecture I am building here contains have five linear layers.\n\ndf_train, df_val = train_test_split(df,shuffle = True, test_size = 0.2)\ntrain_data = TextDataFromDF(df_train)\nval_data   = TextDataFromDF(df_val)\n\ntrain_loader = DataLoader(train_data, batch_size=16, shuffle=True, collate_fn=collate_batch)\nval_loader = DataLoader(val_data, batch_size=16, shuffle=True, collate_fn=collate_batch)\n\n\nimport torch.nn as nn\nfrom  torch.nn import ReLU\n\n# second neural network that uses features\nclass EngFeaturesClassModel(nn.Module):\n    def __init__(self, num_features, num_labels):\n        super().__init__()\n        self.pipeline = nn.Sequential(\n            nn.Flatten(),\n            nn.Linear(num_features, 128),\n            nn.ReLU(),\n            nn.Dropout(0.2),\n            nn.Linear(128, 64),\n            nn.ReLU(),\n            nn.Dropout(0.2),\n            nn.Linear(64, 32),\n            nn.ReLU(),\n            nn.Dropout(0.2),\n            nn.Linear(32, 16),\n            nn.ReLU(),\n            nn.Dropout(0.2),\n            nn.Linear(16, num_labels)\n        )\n    def forward(self, x):\n      return self.pipeline(x)\n\n\nnum_features = 22\nnum_labels = 7\nfeature_model = EngFeaturesClassModel(num_features, num_labels).to(device)\n\n# initialize optimizer and loss function\noptimizer = torch.optim.Adam(feature_model.parameters(), lr=.001)\nloss_fn = torch.nn.CrossEntropyLoss()\n\n# set epoch to be 50\nEPOCHS = 50\nfor epoch in range(1, EPOCHS + 1):\n    train(feature_model, train_loader, use_lyrics=False, use_features=True)\n\n| epoch   1 | train accuracy    0.286 | time: 14.22s\n| epoch   2 | train accuracy    0.334 | time: 14.05s\n| epoch   3 | train accuracy    0.339 | time: 14.42s\n| epoch   4 | train accuracy    0.345 | time: 13.28s\n| epoch   5 | train accuracy    0.353 | time: 13.61s\n| epoch   6 | train accuracy    0.356 | time: 13.83s\n| epoch   7 | train accuracy    0.358 | time: 15.70s\n| epoch   8 | train accuracy    0.360 | time: 16.88s\n| epoch   9 | train accuracy    0.364 | time: 17.78s\n| epoch  10 | train accuracy    0.364 | time: 14.22s\n| epoch  11 | train accuracy    0.369 | time: 14.71s\n| epoch  12 | train accuracy    0.366 | time: 13.95s\n| epoch  13 | train accuracy    0.370 | time: 14.64s\n| epoch  14 | train accuracy    0.372 | time: 15.71s\n| epoch  15 | train accuracy    0.373 | time: 15.22s\n| epoch  16 | train accuracy    0.372 | time: 14.03s\n| epoch  17 | train accuracy    0.374 | time: 14.18s\n| epoch  18 | train accuracy    0.379 | time: 18.42s\n| epoch  19 | train accuracy    0.375 | time: 14.13s\n| epoch  20 | train accuracy    0.383 | time: 13.89s\n| epoch  21 | train accuracy    0.377 | time: 14.06s\n| epoch  22 | train accuracy    0.380 | time: 15.05s\n| epoch  23 | train accuracy    0.378 | time: 14.89s\n| epoch  24 | train accuracy    0.384 | time: 15.13s\n| epoch  25 | train accuracy    0.381 | time: 19.34s\n| epoch  26 | train accuracy    0.386 | time: 18.07s\n| epoch  27 | train accuracy    0.383 | time: 14.48s\n| epoch  28 | train accuracy    0.383 | time: 14.99s\n| epoch  29 | train accuracy    0.384 | time: 14.70s\n| epoch  30 | train accuracy    0.389 | time: 13.15s\n| epoch  31 | train accuracy    0.383 | time: 14.29s\n| epoch  32 | train accuracy    0.384 | time: 14.56s\n| epoch  33 | train accuracy    0.386 | time: 15.49s\n| epoch  34 | train accuracy    0.390 | time: 16.97s\n| epoch  35 | train accuracy    0.391 | time: 14.01s\n| epoch  36 | train accuracy    0.391 | time: 15.62s\n| epoch  37 | train accuracy    0.392 | time: 15.12s\n| epoch  38 | train accuracy    0.390 | time: 14.01s\n| epoch  39 | train accuracy    0.390 | time: 14.20s\n| epoch  40 | train accuracy    0.389 | time: 14.39s\n| epoch  41 | train accuracy    0.389 | time: 14.24s\n| epoch  42 | train accuracy    0.391 | time: 14.64s\n| epoch  43 | train accuracy    0.392 | time: 15.28s\n| epoch  44 | train accuracy    0.391 | time: 14.67s\n| epoch  45 | train accuracy    0.395 | time: 14.80s\n| epoch  46 | train accuracy    0.398 | time: 14.28s\n| epoch  47 | train accuracy    0.395 | time: 16.61s\n| epoch  48 | train accuracy    0.390 | time: 14.63s\n| epoch  49 | train accuracy    0.396 | time: 14.35s\n| epoch  50 | train accuracy    0.396 | time: 14.11s\n\n\n\nevaluate(feature_model, val_loader, use_lyrics=False, use_features=True)\n\n0.3744493392070485\n\n\nThe final accuracy is around 37.4%, which is a larger improvement from predicting the genre only using lyrics.\n\n\n\nBuilding the final neural network, we consider both lyrics and engineered features. The network has three pipelines, one for lyrics, one for features, and the other one for the combined, which is for the concatenated result of the lyrics and the features.\n\nclass CombinedNet(nn.Module):\n\n    def __init__(self, num_labels, vocab_size, embedding_dim, num_features, max_len):\n        super().__init__()\n\n        # lyrics model\n        self.lyric_pipeline = torch.nn.Sequential(\n            nn.Embedding(vocab_size+1, embedding_dim),\n            nn.Flatten(),\n            nn.Dropout(0.2),\n            nn.Linear(max_len*embedding_dim, num_labels))\n\n        # feature model\n        self.feature_pipeline = nn.Sequential(\n            nn.Flatten(),\n            nn.Linear(num_features, 512),\n            nn.ReLU(),\n            nn.Dropout(0.2),\n            nn.Linear(512, 128),\n            nn.ReLU(),\n            nn.Dropout(0.2),\n            nn.Linear(128, 32),\n            nn.ReLU(),\n            nn.Dropout(0.2),\n            nn.Linear(32, num_labels))\n\n        # Combined Layers\n        self.combine_model = nn.Sequential(\n            nn.Linear(14, 512),\n            nn.Linear(512, 256),\n            nn.Linear(256, 64),\n            nn.Linear(64, num_labels))\n\n\n    def forward(self, x):\n        # separate x into x_1 (text features) and x_2 (engineered features)\n        x_lyrics, x_features = x\n        # apply lyric pipeline\n        x_1 = self.lyric_pipeline(x_lyrics)\n\n        # apply feature pipeline\n        x_2 = self.feature_pipeline(x_features)\n\n        # ensure that both x_1 and x_2 are 2-d tensors, flattening if necessary\n        # then, combine them with:\n        x = torch.cat((x_1, x_2), 1)\n        # pass x through a couple more fully-connected layers and return output\n        return self.combine_model(x)\n\n\ntrain_loader = DataLoader(train_data, batch_size=16, shuffle=True, collate_fn=collate_batch)\nval_loader = DataLoader(val_data, batch_size=16, shuffle=True, collate_fn=collate_batch)\n\n\nvocab_size = len(vocab)\nembedding_dim = 3\nnum_features = 22\nnum_labels = 7\n\ncombined_model = CombinedNet(num_labels, vocab_size, embedding_dim, num_features, max_len).to(device)\n\noptimizer = torch.optim.Adam(combined_model.parameters(), lr=.001)\nloss_fn = torch.nn.CrossEntropyLoss()\n\n\nEPOCHS = 40\nfor epoch in range(1, EPOCHS + 1):\n     train(combined_model, train_loader, use_lyrics=True, use_features=True)\n\n| epoch   1 | train accuracy    0.309 | time: 26.07s\n| epoch   2 | train accuracy    0.354 | time: 26.07s\n| epoch   3 | train accuracy    0.361 | time: 26.26s\n| epoch   4 | train accuracy    0.370 | time: 27.73s\n| epoch   5 | train accuracy    0.381 | time: 26.10s\n| epoch   6 | train accuracy    0.381 | time: 31.87s\n| epoch   7 | train accuracy    0.392 | time: 31.11s\n| epoch   8 | train accuracy    0.407 | time: 27.16s\n| epoch   9 | train accuracy    0.414 | time: 28.44s\n| epoch  10 | train accuracy    0.420 | time: 28.45s\n| epoch  11 | train accuracy    0.432 | time: 26.51s\n| epoch  12 | train accuracy    0.436 | time: 28.85s\n| epoch  13 | train accuracy    0.447 | time: 27.98s\n| epoch  14 | train accuracy    0.453 | time: 28.38s\n| epoch  15 | train accuracy    0.464 | time: 27.27s\n| epoch  16 | train accuracy    0.472 | time: 26.10s\n| epoch  17 | train accuracy    0.485 | time: 26.36s\n| epoch  18 | train accuracy    0.493 | time: 28.43s\n| epoch  19 | train accuracy    0.498 | time: 26.11s\n| epoch  20 | train accuracy    0.511 | time: 26.91s\n| epoch  21 | train accuracy    0.517 | time: 25.98s\n| epoch  22 | train accuracy    0.523 | time: 25.67s\n| epoch  23 | train accuracy    0.535 | time: 25.22s\n| epoch  24 | train accuracy    0.545 | time: 28.83s\n| epoch  25 | train accuracy    0.547 | time: 31.67s\n| epoch  26 | train accuracy    0.558 | time: 31.40s\n| epoch  27 | train accuracy    0.566 | time: 31.25s\n| epoch  28 | train accuracy    0.573 | time: 29.74s\n| epoch  29 | train accuracy    0.580 | time: 27.59s\n| epoch  30 | train accuracy    0.590 | time: 28.27s\n| epoch  31 | train accuracy    0.599 | time: 26.19s\n| epoch  32 | train accuracy    0.602 | time: 26.82s\n| epoch  33 | train accuracy    0.612 | time: 27.40s\n| epoch  34 | train accuracy    0.619 | time: 27.20s\n| epoch  35 | train accuracy    0.627 | time: 27.24s\n| epoch  36 | train accuracy    0.630 | time: 27.64s\n| epoch  37 | train accuracy    0.640 | time: 27.41s\n| epoch  38 | train accuracy    0.644 | time: 29.35s\n| epoch  39 | train accuracy    0.651 | time: 27.29s\n| epoch  40 | train accuracy    0.655 | time: 27.47s\n\n\nWe train the model for 40 epoch and arrives at 65.5% training accuracy.\n\nevaluate(combined_model, val_loader, use_lyrics=True, use_features=True)\n\n0.3490748898678414\n\n\nWe evaluate the third model and get a testing accuracy of 34.9%. The large discrepancy between training data accuracy and testing accuracy suggests an overfitting, which is similar what we have seen the first model. The accuracy of the third model is slightly lower than the second model that is only based on engineered features, implying that including more features does not necessarily improve the accuracy of deep learning neural network model.\n\n\nHere I will visualize the word embedding learned by my lyricsModel. We employ PCA to lower-dimensionalize higher-dimension embedding space to a two dimensional one.\n\nembedding_matrix = lyricsModel.embedding.cpu().weight.data.numpy()\n\n# extract words from our vocabularies\ntokens = vocab.get_itos()\n\n# import PCA to get 2 dimensional representation\nfrom sklearn.decomposition import PCA\npca = PCA(n_components=2)\nweights = pca.fit_transform(embedding_matrix)\n\n# plot\ntokens = vocab.get_itos()\ntokens.append(\" \")\nembedding_df = pd.DataFrame({\n    'word' : tokens,\n    'x0'   : weights[:,0],\n    'x1'   : weights[:,1]\n})\n\nembedding_df\n\n\n\n  \n    \n\n\n\n\n\n\nword\nx0\nx1\n\n\n\n\n0\n&lt;unk&gt;\n0.945412\n0.005695\n\n\n1\nknow\n-0.579282\n-1.324462\n\n\n2\nlike\n-0.241303\n-2.381207\n\n\n3\ntime\n-0.322583\n-1.412938\n\n\n4\ncome\n-0.570501\n1.544789\n\n\n...\n...\n...\n...\n\n\n45635\n한번쯤은\n-0.188535\n-0.387275\n\n\n45636\n함께라는\n-0.594271\n-1.230378\n\n\n45637\n힙부심뿐인\n0.249295\n-0.804218\n\n\n45638\nﬁnished\n0.815232\n0.135954\n\n\n45639\n\n-0.308244\n-0.421865\n\n\n\n\n45640 rows × 3 columns\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\n\n\nimport numpy as np\nimport plotly.express as px\nimport plotly.io as pio\n\nfig = px.scatter(embedding_df,\n                 x = \"x0\",\n                 y = \"x1\",\n                 size = list(np.ones(len(embedding_df))),\n                 size_max = 10,\n                 hover_name = \"word\")\n\nfig.show()\n\n\n\n\nWord Embedding Plot\n\n\nDots that represent words of the lyrics are everwhere. Personally, I am not very familiar with the lyrics used in different genres in English, so I do not see a clear pattern. If our neural network model trained on lyrics has learned word embeddings, the scatter plot should have 7 corners that represent the 7 genres, and words that are at the end of each corner are the most representative words for that genre. However, we do not see the corners here. This also explains why our neural network model trained only using lyrics does not have a good performance.\n\n\n\n\nWe constructed and trained three neural networks to classify music genre in this blog post. The three networks use (1) only lyrics, (2) only engineered features of the songs from Spotify, and (3) both lyrics and the engineered features. The base rate of the dataset for predicting a genre is 24.8%. For the result, these three models give us 26.3%, 37.4%, and 34.9% accuracy on testing data. Visualizing the word embedding plot, we found that the first model trained based on lyrics did not really learned the connection between words and how words could represent genres.\nWriting this blog post, I had the opportunity to construct a neural network architecture and implement it for the first time. By reconstructing the network and adjusting learning rate, I see how adding layers and different learning rates could affect the learning process of the machine. I also learned how to combine two pipelines together when constructing my third neural network, which will be very useful when we train a model using different types of features. Writing only one data loader class and one training/evaluation loop stretches me to code more efficiently."
  },
  {
    "objectID": "posts/deepMusic-post/index.html#abstract",
    "href": "posts/deepMusic-post/index.html#abstract",
    "title": "Deep Music Genre Classification",
    "section": "",
    "text": "In this blog post, we will construct and implement deep learning neural network to classify song genres. The dataset we will be using is collected by researchers from Spotify and is available through Kaggle. The blog will build and train three different neural networks: one only based on lyrics, one only based on engineered features, and one on both lyrics and features. We will do training and testing and compare the performance of these three models."
  },
  {
    "objectID": "posts/deepMusic-post/index.html#preparation-steps",
    "href": "posts/deepMusic-post/index.html#preparation-steps",
    "title": "Deep Music Genre Classification",
    "section": "",
    "text": "First, we acquire data from the url. Here are some first rows of the data.\n\nimport pandas as pd\n\nurl = \"https://raw.githubusercontent.com/PhilChodrow/PIC16B/master/datasets/tcc_ceds_music.csv\"\ndf = pd.read_csv(url)\n\n\ndf.head()\n\n\n\n  \n    \n\n\n\n\n\n\nUnnamed: 0\nartist_name\ntrack_name\nrelease_date\ngenre\nlyrics\nlen\ndating\nviolence\nworld/life\n...\nsadness\nfeelings\ndanceability\nloudness\nacousticness\ninstrumentalness\nvalence\nenergy\ntopic\nage\n\n\n\n\n0\n0\nmukesh\nmohabbat bhi jhoothi\n1950\npop\nhold time feel break feel untrue convince spea...\n95\n0.000598\n0.063746\n0.000598\n...\n0.380299\n0.117175\n0.357739\n0.454119\n0.997992\n0.901822\n0.339448\n0.137110\nsadness\n1.0\n\n\n1\n4\nfrankie laine\ni believe\n1950\npop\nbelieve drop rain fall grow believe darkest ni...\n51\n0.035537\n0.096777\n0.443435\n...\n0.001284\n0.001284\n0.331745\n0.647540\n0.954819\n0.000002\n0.325021\n0.263240\nworld/life\n1.0\n\n\n2\n6\njohnnie ray\ncry\n1950\npop\nsweetheart send letter goodbye secret feel bet...\n24\n0.002770\n0.002770\n0.002770\n...\n0.002770\n0.225422\n0.456298\n0.585288\n0.840361\n0.000000\n0.351814\n0.139112\nmusic\n1.0\n\n\n3\n10\npérez prado\npatricia\n1950\npop\nkiss lips want stroll charm mambo chacha merin...\n54\n0.048249\n0.001548\n0.001548\n...\n0.225889\n0.001548\n0.686992\n0.744404\n0.083935\n0.199393\n0.775350\n0.743736\nromantic\n1.0\n\n\n4\n12\ngiorgos papadopoulos\napopse eida oneiro\n1950\npop\ntill darling till matter know till dream live ...\n48\n0.001350\n0.001350\n0.417772\n...\n0.068800\n0.001350\n0.291671\n0.646489\n0.975904\n0.000246\n0.597073\n0.394375\nromantic\n1.0\n\n\n\n\n5 rows × 31 columns\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\n\nHere are the engineered features of the data. Each song receives a score below each category.\n\nengineered_features = ['dating', 'violence', 'world/life', 'night/time','shake the audience','family/gospel', 'romantic', 'communication','obscene', 'music', 'movement/places', 'light/visual perceptions','family/spiritual', 'like/girls', 'sadness', 'feelings', 'danceability','loudness', 'acousticness', 'instrumentalness', 'valence', 'energy']\n\nHere are the genres of music in the dataset. We first want to encode the genre. Here I use LabelEncoder from scikit learn.\n\ndf[\"genre\"].unique()\n\narray(['pop', 'country', 'blues', 'jazz', 'reggae', 'rock', 'hip hop'],\n      dtype=object)\n\n\n\nfrom sklearn.preprocessing import LabelEncoder\n\n# Creating a instance of label Encoder.\nle = LabelEncoder()\n\n# Using .fit_transform function to fit label encoder and return encoded label\nlabel = le.fit_transform(df['genre'])\n\ndf.drop(\"genre\", axis=1, inplace=True)\n\n# Appending the array to our dataFrame with column name 'Purchased'\ndf[\"genre\"] = label\n\n\ndf['genre'].unique()\n\narray([4, 1, 0, 3, 5, 6, 2])\n\n\nHere is the percentage of each genre in the entire dataset. The music genre that occupies the largest proportion is Pop music (around 24.8%). It sets the base classification rate to be 24.8%.\n\ndf.groupby(\"genre\").size() / len(df)\n\ngenre\n0    0.162273\n1    0.191915\n2    0.031862\n3    0.135521\n4    0.248202\n5    0.088045\n6    0.142182\ndtype: float64\n\n\n\n\n\nNext, we want to define a data loader class, which we will use to load only batches of the data at a time. The indexer methods in TextDataFromDF class will return columns of lyrics, genre, and engineered features.\n\nfrom torch.utils.data import Dataset, DataLoader\n\nclass TextDataFromDF(Dataset):\n    def __init__(self, df):\n        self.df = df\n\n    def __getitem__(self, index):\n        # 4 is lyrics, 30 is genre, 6-28 are features\n        return self.df.iloc[index, 4], self.df.iloc[index, 30], self.df.iloc[index, 6:28].values\n\n    def __len__(self):\n        return len(self.df)\n\n\nfrom sklearn.model_selection import train_test_split\n\ndf_train, df_val = train_test_split(df,shuffle = True, test_size = 0.2)\ntrain_data = TextDataFromDF(df_train)\nval_data   = TextDataFromDF(df_val)\n\n\ntrain_data[194]\n\n('schmit couldn reason felt kind blue watch hello sweet friend come sing lullaby amend mama lookin acan start anew search finally come realize tellin lie livin afraid read line different today shin know couldn change mind twist point view kind choose reason lose lookin silly game',\n 4,\n array([0.0013157896412805, 0.0013157895964813, 0.322708678411067,\n        0.0013157896186701, 0.0013157894770102, 0.0279231073879107,\n        0.00131578966616, 0.2961412546260131, 0.1707160272570204,\n        0.0959860879895431, 0.001315789547777, 0.0423188121300265,\n        0.0284165576046225, 0.0013157895199298, 0.00131578953515,\n        0.0013157894850158, 0.5689375067691976, 0.5644437607363535,\n        0.3122483054701862, 0.0001751012145748, 0.4466199505358615,\n        0.4394219318961712], dtype=object))\n\n\n\n\n\nFurther, we are going to tokenize the song lyrics and make it available for training. Basically, we split the sentence into words and assign number to each unique word.\n\nfrom torchtext.data.utils import get_tokenizer\nfrom torchtext.vocab import build_vocab_from_iterator\n\ntokenizer = get_tokenizer('basic_english')\n\n\ndef yield_tokens(data_iter):\n    for text, features, _ in data_iter:\n        yield tokenizer(text)\n\nvocab = build_vocab_from_iterator(yield_tokens(train_data), specials=[\"&lt;unk&gt;\"])\nvocab.set_default_index(vocab[\"&lt;unk&gt;\"])\n\n\n\n\nWe construct the function that is going to actually pass a batch of data to our training loop. The collation function will return three elements each set: the lyrics, the label (genre), and the features.\n\nimport torch\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmax_len = 100\nnum_tokens = len(vocab.get_itos())\ndef text_pipeline(x):\n    tokens = vocab(tokenizer(x))\n    y = torch.zeros(max_len, dtype=torch.int64) + num_tokens\n    if len(tokens) &gt; max_len:\n        tokens = tokens[0:max_len]\n    y[0:len(tokens)] = torch.tensor(tokens,dtype=torch.int64)\n    return y\n\nlabel_pipeline = lambda x: int(x)\n\n\ndef collate_batch(batch):\n    label_list, text_list, feature_list = [], [], []\n\n    for (_text, _label, _features) in batch:\n\n        # add label to list\n         label_list.append(label_pipeline(_label))\n\n         # add text (as sequence of integers) to list\n         processed_text = text_pipeline(_text)\n         text_list.append(processed_text)\n\n        # add feature as float to list\n         feature_list.append(torch.tensor(_features.astype(float), dtype=torch.float32))\n\n    label_list = torch.tensor(label_list, dtype=torch.int64)\n    text_list = torch.stack(text_list)\n    feature_list = torch.stack(feature_list)\n\n    return text_list.to(device), label_list.to(device), feature_list.to(device)\n\ntrain_loader = DataLoader(train_data, batch_size=8, shuffle=True, collate_fn=collate_batch)\nval_loader = DataLoader(val_data, batch_size=8, shuffle=True, collate_fn=collate_batch)\n\n\n\n\n\nimport time\n\ndef train(model, dataloader, use_lyrics = True, use_features = False):\n    epoch_start_time = time.time()\n    # keep track of some counts for measuring accuracy\n    total_acc, total_count = 0, 0\n    log_interval = 300\n    start_time = time.time()\n\n\n    for idx, (text, label, features) in enumerate(dataloader):\n\n        # zero gradients\n        optimizer.zero_grad()\n\n        # customize model input based on conditions\n        if use_lyrics and not use_features:\n          input = text\n        elif use_features and not use_lyrics:\n          input = features\n        else:\n          input = (text, features)\n\n        # form prediction on batch\n        predicted_label = model(input)\n\n        # evaluate loss on prediction\n        loss = loss_fn(predicted_label, label)\n\n        # compute gradient\n        loss.backward()\n        # take an optimization step\n        optimizer.step()\n\n        # for printing accuracy\n        total_acc   += (predicted_label.argmax(1) == label).sum().item()\n        total_count += label.size(0)\n\n    print(f'| epoch {epoch:3d} | train accuracy {total_acc/total_count:8.3f} | time: {time.time() - epoch_start_time:5.2f}s')\n\ndef evaluate(model, dataloader, use_lyrics = True, use_features = False):\n\n    total_acc, total_count = 0, 0\n\n    with torch.no_grad():\n        for idx, (text, label, features) in enumerate(dataloader):\n\n            if use_lyrics and not use_features:\n              input = text\n            elif use_features and not use_lyrics:\n              input = features\n            else:\n              input = (text, features)\n\n            predicted_label = model(input)\n            total_acc += (predicted_label.argmax(1) == label).sum().item()\n            total_count += label.size(0)\n    return total_acc/total_count"
  },
  {
    "objectID": "posts/deepMusic-post/index.html#first-neural-network-only-using-lyrics",
    "href": "posts/deepMusic-post/index.html#first-neural-network-only-using-lyrics",
    "title": "Deep Music Genre Classification",
    "section": "",
    "text": "We are training our model only using the lyrics. The neural network architecture I used closely aligns to Professor Chodrow’s lecture note on Text Classification and Word Embedding. This network consists of an input embedding layer and an output linear layer.\n\nfrom torch import nn\n\n# first neural network that uses lyrics\nclass TextClassificationModel(nn.Module):\n\n    def __init__(self, vocab_size, embedding_dim, max_len, num_class):\n        super().__init__()\n        self.embedding = nn.Embedding(vocab_size+1, embedding_dim)\n        self.fc   = nn.Linear(max_len*embedding_dim, num_class)\n\n    def forward(self, x):\n        x = self.embedding(x)\n        x = torch.flatten(x, 1)\n        x = self.fc(x)\n        return(x)\n\nWe set the word embedding dimension to be 4.\n\nvocab_size = len(vocab)\nembedding_dim = 4\n\n# initialize the model\nlyricsModel = TextClassificationModel(vocab_size, embedding_dim, max_len, 7).to(device)\n\n\n# initialize ADAM optimizer and loss function\noptimizer = torch.optim.Adam(lyricsModel.parameters(), lr=.001)\nloss_fn = torch.nn.CrossEntropyLoss()\n\n# set epochs to be 30\nEPOCHS = 30\nfor epoch in range(1, EPOCHS + 1):\n    train(lyricsModel, train_loader, use_lyrics=True, use_features=False)\n\n| epoch   1 | train accuracy    0.207 | time: 24.12s\n| epoch   2 | train accuracy    0.238 | time: 25.01s\n| epoch   3 | train accuracy    0.258 | time: 25.73s\n| epoch   4 | train accuracy    0.281 | time: 26.31s\n| epoch   5 | train accuracy    0.301 | time: 24.88s\n| epoch   6 | train accuracy    0.334 | time: 24.94s\n| epoch   7 | train accuracy    0.357 | time: 25.92s\n| epoch   8 | train accuracy    0.379 | time: 24.62s\n| epoch   9 | train accuracy    0.410 | time: 25.32s\n| epoch  10 | train accuracy    0.438 | time: 25.38s\n| epoch  11 | train accuracy    0.458 | time: 24.86s\n| epoch  12 | train accuracy    0.484 | time: 24.88s\n| epoch  13 | train accuracy    0.505 | time: 25.81s\n| epoch  14 | train accuracy    0.525 | time: 25.73s\n| epoch  15 | train accuracy    0.546 | time: 26.38s\n| epoch  16 | train accuracy    0.564 | time: 24.84s\n| epoch  17 | train accuracy    0.582 | time: 24.96s\n| epoch  18 | train accuracy    0.599 | time: 24.71s\n| epoch  19 | train accuracy    0.615 | time: 25.89s\n| epoch  20 | train accuracy    0.636 | time: 26.10s\n| epoch  21 | train accuracy    0.653 | time: 25.23s\n| epoch  22 | train accuracy    0.665 | time: 25.25s\n| epoch  23 | train accuracy    0.679 | time: 25.18s\n| epoch  24 | train accuracy    0.694 | time: 27.20s\n| epoch  25 | train accuracy    0.704 | time: 24.96s\n| epoch  26 | train accuracy    0.717 | time: 26.47s\n| epoch  27 | train accuracy    0.729 | time: 24.36s\n| epoch  28 | train accuracy    0.738 | time: 25.45s\n| epoch  29 | train accuracy    0.750 | time: 25.02s\n| epoch  30 | train accuracy    0.762 | time: 24.65s\n\n\nWe see the training accuracy increases over epochs and arrives at 76.2% accuracy. Now we evaluate the model on the testing data.\n\nevaluate(lyricsModel, val_loader, use_lyrics=True, use_features=False)\n\n0.26343612334801764\n\n\nThe accuracy of the first neural network using only lyric only gives us 26.3% accuracy, which is only slightly larger than the base rate. Compare with 76.2% for training data, we suspect that the model was overfitted."
  },
  {
    "objectID": "posts/deepMusic-post/index.html#second-neural-network-only-using-features",
    "href": "posts/deepMusic-post/index.html#second-neural-network-only-using-features",
    "title": "Deep Music Genre Classification",
    "section": "",
    "text": "Now, we only use engineered features to train the neural network. The neural network architecture I am building here contains have five linear layers.\n\ndf_train, df_val = train_test_split(df,shuffle = True, test_size = 0.2)\ntrain_data = TextDataFromDF(df_train)\nval_data   = TextDataFromDF(df_val)\n\ntrain_loader = DataLoader(train_data, batch_size=16, shuffle=True, collate_fn=collate_batch)\nval_loader = DataLoader(val_data, batch_size=16, shuffle=True, collate_fn=collate_batch)\n\n\nimport torch.nn as nn\nfrom  torch.nn import ReLU\n\n# second neural network that uses features\nclass EngFeaturesClassModel(nn.Module):\n    def __init__(self, num_features, num_labels):\n        super().__init__()\n        self.pipeline = nn.Sequential(\n            nn.Flatten(),\n            nn.Linear(num_features, 128),\n            nn.ReLU(),\n            nn.Dropout(0.2),\n            nn.Linear(128, 64),\n            nn.ReLU(),\n            nn.Dropout(0.2),\n            nn.Linear(64, 32),\n            nn.ReLU(),\n            nn.Dropout(0.2),\n            nn.Linear(32, 16),\n            nn.ReLU(),\n            nn.Dropout(0.2),\n            nn.Linear(16, num_labels)\n        )\n    def forward(self, x):\n      return self.pipeline(x)\n\n\nnum_features = 22\nnum_labels = 7\nfeature_model = EngFeaturesClassModel(num_features, num_labels).to(device)\n\n# initialize optimizer and loss function\noptimizer = torch.optim.Adam(feature_model.parameters(), lr=.001)\nloss_fn = torch.nn.CrossEntropyLoss()\n\n# set epoch to be 50\nEPOCHS = 50\nfor epoch in range(1, EPOCHS + 1):\n    train(feature_model, train_loader, use_lyrics=False, use_features=True)\n\n| epoch   1 | train accuracy    0.286 | time: 14.22s\n| epoch   2 | train accuracy    0.334 | time: 14.05s\n| epoch   3 | train accuracy    0.339 | time: 14.42s\n| epoch   4 | train accuracy    0.345 | time: 13.28s\n| epoch   5 | train accuracy    0.353 | time: 13.61s\n| epoch   6 | train accuracy    0.356 | time: 13.83s\n| epoch   7 | train accuracy    0.358 | time: 15.70s\n| epoch   8 | train accuracy    0.360 | time: 16.88s\n| epoch   9 | train accuracy    0.364 | time: 17.78s\n| epoch  10 | train accuracy    0.364 | time: 14.22s\n| epoch  11 | train accuracy    0.369 | time: 14.71s\n| epoch  12 | train accuracy    0.366 | time: 13.95s\n| epoch  13 | train accuracy    0.370 | time: 14.64s\n| epoch  14 | train accuracy    0.372 | time: 15.71s\n| epoch  15 | train accuracy    0.373 | time: 15.22s\n| epoch  16 | train accuracy    0.372 | time: 14.03s\n| epoch  17 | train accuracy    0.374 | time: 14.18s\n| epoch  18 | train accuracy    0.379 | time: 18.42s\n| epoch  19 | train accuracy    0.375 | time: 14.13s\n| epoch  20 | train accuracy    0.383 | time: 13.89s\n| epoch  21 | train accuracy    0.377 | time: 14.06s\n| epoch  22 | train accuracy    0.380 | time: 15.05s\n| epoch  23 | train accuracy    0.378 | time: 14.89s\n| epoch  24 | train accuracy    0.384 | time: 15.13s\n| epoch  25 | train accuracy    0.381 | time: 19.34s\n| epoch  26 | train accuracy    0.386 | time: 18.07s\n| epoch  27 | train accuracy    0.383 | time: 14.48s\n| epoch  28 | train accuracy    0.383 | time: 14.99s\n| epoch  29 | train accuracy    0.384 | time: 14.70s\n| epoch  30 | train accuracy    0.389 | time: 13.15s\n| epoch  31 | train accuracy    0.383 | time: 14.29s\n| epoch  32 | train accuracy    0.384 | time: 14.56s\n| epoch  33 | train accuracy    0.386 | time: 15.49s\n| epoch  34 | train accuracy    0.390 | time: 16.97s\n| epoch  35 | train accuracy    0.391 | time: 14.01s\n| epoch  36 | train accuracy    0.391 | time: 15.62s\n| epoch  37 | train accuracy    0.392 | time: 15.12s\n| epoch  38 | train accuracy    0.390 | time: 14.01s\n| epoch  39 | train accuracy    0.390 | time: 14.20s\n| epoch  40 | train accuracy    0.389 | time: 14.39s\n| epoch  41 | train accuracy    0.389 | time: 14.24s\n| epoch  42 | train accuracy    0.391 | time: 14.64s\n| epoch  43 | train accuracy    0.392 | time: 15.28s\n| epoch  44 | train accuracy    0.391 | time: 14.67s\n| epoch  45 | train accuracy    0.395 | time: 14.80s\n| epoch  46 | train accuracy    0.398 | time: 14.28s\n| epoch  47 | train accuracy    0.395 | time: 16.61s\n| epoch  48 | train accuracy    0.390 | time: 14.63s\n| epoch  49 | train accuracy    0.396 | time: 14.35s\n| epoch  50 | train accuracy    0.396 | time: 14.11s\n\n\n\nevaluate(feature_model, val_loader, use_lyrics=False, use_features=True)\n\n0.3744493392070485\n\n\nThe final accuracy is around 37.4%, which is a larger improvement from predicting the genre only using lyrics."
  },
  {
    "objectID": "posts/deepMusic-post/index.html#both-lyrics-and-engineered-features",
    "href": "posts/deepMusic-post/index.html#both-lyrics-and-engineered-features",
    "title": "Deep Music Genre Classification",
    "section": "",
    "text": "Building the final neural network, we consider both lyrics and engineered features. The network has three pipelines, one for lyrics, one for features, and the other one for the combined, which is for the concatenated result of the lyrics and the features.\n\nclass CombinedNet(nn.Module):\n\n    def __init__(self, num_labels, vocab_size, embedding_dim, num_features, max_len):\n        super().__init__()\n\n        # lyrics model\n        self.lyric_pipeline = torch.nn.Sequential(\n            nn.Embedding(vocab_size+1, embedding_dim),\n            nn.Flatten(),\n            nn.Dropout(0.2),\n            nn.Linear(max_len*embedding_dim, num_labels))\n\n        # feature model\n        self.feature_pipeline = nn.Sequential(\n            nn.Flatten(),\n            nn.Linear(num_features, 512),\n            nn.ReLU(),\n            nn.Dropout(0.2),\n            nn.Linear(512, 128),\n            nn.ReLU(),\n            nn.Dropout(0.2),\n            nn.Linear(128, 32),\n            nn.ReLU(),\n            nn.Dropout(0.2),\n            nn.Linear(32, num_labels))\n\n        # Combined Layers\n        self.combine_model = nn.Sequential(\n            nn.Linear(14, 512),\n            nn.Linear(512, 256),\n            nn.Linear(256, 64),\n            nn.Linear(64, num_labels))\n\n\n    def forward(self, x):\n        # separate x into x_1 (text features) and x_2 (engineered features)\n        x_lyrics, x_features = x\n        # apply lyric pipeline\n        x_1 = self.lyric_pipeline(x_lyrics)\n\n        # apply feature pipeline\n        x_2 = self.feature_pipeline(x_features)\n\n        # ensure that both x_1 and x_2 are 2-d tensors, flattening if necessary\n        # then, combine them with:\n        x = torch.cat((x_1, x_2), 1)\n        # pass x through a couple more fully-connected layers and return output\n        return self.combine_model(x)\n\n\ntrain_loader = DataLoader(train_data, batch_size=16, shuffle=True, collate_fn=collate_batch)\nval_loader = DataLoader(val_data, batch_size=16, shuffle=True, collate_fn=collate_batch)\n\n\nvocab_size = len(vocab)\nembedding_dim = 3\nnum_features = 22\nnum_labels = 7\n\ncombined_model = CombinedNet(num_labels, vocab_size, embedding_dim, num_features, max_len).to(device)\n\noptimizer = torch.optim.Adam(combined_model.parameters(), lr=.001)\nloss_fn = torch.nn.CrossEntropyLoss()\n\n\nEPOCHS = 40\nfor epoch in range(1, EPOCHS + 1):\n     train(combined_model, train_loader, use_lyrics=True, use_features=True)\n\n| epoch   1 | train accuracy    0.309 | time: 26.07s\n| epoch   2 | train accuracy    0.354 | time: 26.07s\n| epoch   3 | train accuracy    0.361 | time: 26.26s\n| epoch   4 | train accuracy    0.370 | time: 27.73s\n| epoch   5 | train accuracy    0.381 | time: 26.10s\n| epoch   6 | train accuracy    0.381 | time: 31.87s\n| epoch   7 | train accuracy    0.392 | time: 31.11s\n| epoch   8 | train accuracy    0.407 | time: 27.16s\n| epoch   9 | train accuracy    0.414 | time: 28.44s\n| epoch  10 | train accuracy    0.420 | time: 28.45s\n| epoch  11 | train accuracy    0.432 | time: 26.51s\n| epoch  12 | train accuracy    0.436 | time: 28.85s\n| epoch  13 | train accuracy    0.447 | time: 27.98s\n| epoch  14 | train accuracy    0.453 | time: 28.38s\n| epoch  15 | train accuracy    0.464 | time: 27.27s\n| epoch  16 | train accuracy    0.472 | time: 26.10s\n| epoch  17 | train accuracy    0.485 | time: 26.36s\n| epoch  18 | train accuracy    0.493 | time: 28.43s\n| epoch  19 | train accuracy    0.498 | time: 26.11s\n| epoch  20 | train accuracy    0.511 | time: 26.91s\n| epoch  21 | train accuracy    0.517 | time: 25.98s\n| epoch  22 | train accuracy    0.523 | time: 25.67s\n| epoch  23 | train accuracy    0.535 | time: 25.22s\n| epoch  24 | train accuracy    0.545 | time: 28.83s\n| epoch  25 | train accuracy    0.547 | time: 31.67s\n| epoch  26 | train accuracy    0.558 | time: 31.40s\n| epoch  27 | train accuracy    0.566 | time: 31.25s\n| epoch  28 | train accuracy    0.573 | time: 29.74s\n| epoch  29 | train accuracy    0.580 | time: 27.59s\n| epoch  30 | train accuracy    0.590 | time: 28.27s\n| epoch  31 | train accuracy    0.599 | time: 26.19s\n| epoch  32 | train accuracy    0.602 | time: 26.82s\n| epoch  33 | train accuracy    0.612 | time: 27.40s\n| epoch  34 | train accuracy    0.619 | time: 27.20s\n| epoch  35 | train accuracy    0.627 | time: 27.24s\n| epoch  36 | train accuracy    0.630 | time: 27.64s\n| epoch  37 | train accuracy    0.640 | time: 27.41s\n| epoch  38 | train accuracy    0.644 | time: 29.35s\n| epoch  39 | train accuracy    0.651 | time: 27.29s\n| epoch  40 | train accuracy    0.655 | time: 27.47s\n\n\nWe train the model for 40 epoch and arrives at 65.5% training accuracy.\n\nevaluate(combined_model, val_loader, use_lyrics=True, use_features=True)\n\n0.3490748898678414\n\n\nWe evaluate the third model and get a testing accuracy of 34.9%. The large discrepancy between training data accuracy and testing accuracy suggests an overfitting, which is similar what we have seen the first model. The accuracy of the third model is slightly lower than the second model that is only based on engineered features, implying that including more features does not necessarily improve the accuracy of deep learning neural network model.\n\n\nHere I will visualize the word embedding learned by my lyricsModel. We employ PCA to lower-dimensionalize higher-dimension embedding space to a two dimensional one.\n\nembedding_matrix = lyricsModel.embedding.cpu().weight.data.numpy()\n\n# extract words from our vocabularies\ntokens = vocab.get_itos()\n\n# import PCA to get 2 dimensional representation\nfrom sklearn.decomposition import PCA\npca = PCA(n_components=2)\nweights = pca.fit_transform(embedding_matrix)\n\n# plot\ntokens = vocab.get_itos()\ntokens.append(\" \")\nembedding_df = pd.DataFrame({\n    'word' : tokens,\n    'x0'   : weights[:,0],\n    'x1'   : weights[:,1]\n})\n\nembedding_df\n\n\n\n  \n    \n\n\n\n\n\n\nword\nx0\nx1\n\n\n\n\n0\n&lt;unk&gt;\n0.945412\n0.005695\n\n\n1\nknow\n-0.579282\n-1.324462\n\n\n2\nlike\n-0.241303\n-2.381207\n\n\n3\ntime\n-0.322583\n-1.412938\n\n\n4\ncome\n-0.570501\n1.544789\n\n\n...\n...\n...\n...\n\n\n45635\n한번쯤은\n-0.188535\n-0.387275\n\n\n45636\n함께라는\n-0.594271\n-1.230378\n\n\n45637\n힙부심뿐인\n0.249295\n-0.804218\n\n\n45638\nﬁnished\n0.815232\n0.135954\n\n\n45639\n\n-0.308244\n-0.421865\n\n\n\n\n45640 rows × 3 columns\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\n\n\nimport numpy as np\nimport plotly.express as px\nimport plotly.io as pio\n\nfig = px.scatter(embedding_df,\n                 x = \"x0\",\n                 y = \"x1\",\n                 size = list(np.ones(len(embedding_df))),\n                 size_max = 10,\n                 hover_name = \"word\")\n\nfig.show()\n\n\n\n\nWord Embedding Plot\n\n\nDots that represent words of the lyrics are everwhere. Personally, I am not very familiar with the lyrics used in different genres in English, so I do not see a clear pattern. If our neural network model trained on lyrics has learned word embeddings, the scatter plot should have 7 corners that represent the 7 genres, and words that are at the end of each corner are the most representative words for that genre. However, we do not see the corners here. This also explains why our neural network model trained only using lyrics does not have a good performance."
  },
  {
    "objectID": "posts/deepMusic-post/index.html#conclusion",
    "href": "posts/deepMusic-post/index.html#conclusion",
    "title": "Deep Music Genre Classification",
    "section": "",
    "text": "We constructed and trained three neural networks to classify music genre in this blog post. The three networks use (1) only lyrics, (2) only engineered features of the songs from Spotify, and (3) both lyrics and the engineered features. The base rate of the dataset for predicting a genre is 24.8%. For the result, these three models give us 26.3%, 37.4%, and 34.9% accuracy on testing data. Visualizing the word embedding plot, we found that the first model trained based on lyrics did not really learned the connection between words and how words could represent genres.\nWriting this blog post, I had the opportunity to construct a neural network architecture and implement it for the first time. By reconstructing the network and adjusting learning rate, I see how adding layers and different learning rates could affect the learning process of the machine. I also learned how to combine two pipelines together when constructing my third neural network, which will be very useful when we train a model using different types of features. Writing only one data loader class and one training/evaluation loop stretches me to code more efficiently."
  },
  {
    "objectID": "posts/perceptron/index.html",
    "href": "posts/perceptron/index.html",
    "title": "Perceptron Algorithm",
    "section": "",
    "text": "Implementing the Perceptron Algorithm\n\nAbstract\nIn this post, we are going to implement the perceptron algorithm under different conditions of raw data. The perceptron algorithm is a binary linear classifier that learns to construct a decision boundary that classifies two classes of data points in a feature space. The algorithm works when the two classes of data are linearly separable: the perceptron converges to a line that completely separates the two classes as the error of missclassification converges to zero. In the post we will show that property applies to not only two 2 dimensional linear separable data but also multidimensional linear separable data. However, for data points that are not linearly separable, the perceptron will find it hard to adjust and converge to a line which has zero error in classifying the data. To make sure that the error converges for data that are not linearly separable, we introduce Minibatch Perceptron algorithm so that the algorithm could arrive at a line that separates the two classes with the least error. The more detailed version of the classical and Minibatch perceptron algorithm is presented in these two files ‘perceptron.py’ and ‘MBperceptron.py’.\n\n%load_ext autoreload\n%autoreload 2\nfrom perceptron import Perceptron, PerceptronOptimizer\nfrom MBperceptron import MBPerceptron, MBPerceptronOptimizer\n\nIntel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\nIntel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n\n\n\n\nPart A: Implement Perceptron\nFirst, we want to create some data points of two categories that are linearly separable for the algorithm to run on. Here is the code to generate random data points of two categories that are linearly separable.\n\nimport torch\nfrom matplotlib import pyplot as plt\nplt.style.use('seaborn-v0_8-whitegrid')\n\ntorch.manual_seed(12989)\n\ndef perceptron_data(n_points = 100, noise = 0.23, p_dims = 2):\n    \n    y = torch.arange(n_points) &gt;= int(n_points/2)\n    X = y[:, None] + torch.normal(0.0, noise, size = (n_points,p_dims))\n    X = torch.cat((X, torch.ones((X.shape[0], 1))), 1)\n\n    # convert y from {0, 1} to {-1, 1}\n    y = 2*y - 1\n\n    return X, y\n\nX, y = perceptron_data(n_points = 100, noise = 0.2)\n\ndef plot_perceptron_data(X, y, ax):\n    assert X.shape[1] == 3, \"This function only works for data created with p_dims == 2\"\n    targets = [-1, 1]\n    markers = [\"o\" , \",\"]\n    for i in range(2):\n        ix = y == targets[i]\n        ax.scatter(X[ix,0], X[ix,1], s = 20,  c = y[ix], facecolors = \"none\", edgecolors = \"darkgrey\", cmap = \"BrBG\", vmin = -2, vmax = 2, alpha = 0.5, marker = markers[i])\n    ax.set(xlabel = r\"$x_1$\", ylabel = r\"$x_2$\")\n\nfig, ax = plt.subplots(1, 1)\nX, y = perceptron_data()\nplot_perceptron_data(X, y, ax)\n\n\n\n\n\n\n\n\nThese are \\(n = 100\\) points of data. Each data point \\(i\\) has two features \\(x_{i1}\\), \\(x_{i2}\\), and a target variable \\(y_i\\). Each data points stack onto each other to form these two matrices \\(\\mathbf{X}\\) and \\(\\mathbf{y}\\). \\[\n\\mathbf{X} = \\left[\\begin{matrix} & - & \\mathbf{x}_1 & - \\\\\n& - & \\mathbf{x}_2 & - \\\\\n& \\vdots & \\vdots & \\vdots \\\\\n& - & \\mathbf{x}_{n} & - \\end{matrix}\\right]\n\\] \\[\n\\mathbf{y} = (y_1, \\ldots, y_{n})^T \\in \\{-1,1\\}^{n}\n\\] In this data set, the target variable has components equal to either \\(-1\\) or \\(1\\).\nNow, we want to implement the classical perceptron algorithm. Recall from the lecture note provided by Professor Chodrow that describes the steps of the perceptron algorithm:\n\nThe perceptron algorithm aims to find a good choice of \\(\\mathbf{w}\\) that makes the loss small using the following algorithm: 1. Start with a random \\(\\mathbf{w}^{(0)}\\). 2. “Until we’re done,” in each time-step \\(t\\), - Pick a random data point \\(i \\in \\{1,\\ldots,n\\}\\). - Compute \\(s^{(t)}_i = \\langle \\mathbf{w}^{(t)}, \\mathbf{x}_i \\rangle\\). - If \\(s^{(t)}_i y_i &gt; 0\\), then point \\(i\\) is currently correctly classified – do nothing! - Else, if \\(s^{(t)}_i y_i &lt; 0\\), then perform the update \\[\\mathbf{w}^{(t+1)} = \\mathbf{w}^{(t)} + y_i \\mathbf{x}_i\\;. \\tag{1}\\]\n\nThe steps above are functions Perceptron.grad() and PerceptronOptimizer.step() in the file ‘perceptron.py’. Here, I will walk you through the Perceptron.grad() algorithm through comments by the code.\n\ndef grad(self, X, y):\n    s_i = X@self.w  # matrix calculation of the weight and the data point x to get the score using the torch @ operator\n    y_ = 2 * y - 1  # convert the target feature value from 0 and 1 to -1 and 1. \n    v = s_i*y_      # if all the points are classified correctly\n    return (v &lt; 0).float() * X * y_   # if all of them are correct, then no need to update the weight vector. If not, then we return a grad that will be used to update the weight vector and the decsion boundary\n\nTo verify that our classical perceptron algorithm is working, we want to implement a minimal training loop using the data we have already generated in the beginning of the post.\n\n# instantiate a model and an optimizer\np = Perceptron() \nopt = PerceptronOptimizer(p)\np.loss(X, y)\n\n# define loss variable\nloss = 1.0\n\n# for keeping track of loss values\nloss_vec = []\niter = 0\n\nn = X.size()[0]\n\nwhile loss &gt; 0: # dangerous -- only terminates if data is linearly separable\n    \n    # not part of the update: just for tracking our progress    \n    loss = p.loss(X, y) \n    loss_vec.append(loss)\n    \n    # pick a random data point\n    i = torch.randint(n, size = (1,))\n    x_i = X[[i],:]\n    y_i = y[i]\n    \n    # perform a perceptron update using the random data point\n    opt.step(x_i, y_i)\n    iter += 1\n\nprint(iter)\n\n115\n\n\nThe loop terminates repeating 115 times and therefore the loss converges to 0. My algorithm works! Now let’s move to the experiment part where we use the perceptron algorithm to different types of data points.\n\n\nPart B: Experiments\n\nPart B.1: Linearly Separable 2D data\nWe will use the original data points we used for the minimal training loop above to illustrate the evolution of the loss function and the final separation line that the algorithm converges to.\n\nplt.plot(loss_vec, color = \"slategrey\")\nplt.scatter(torch.arange(len(loss_vec)), loss_vec, color = \"slategrey\")\nlabs = plt.gca().set(xlabel = \"Perceptron Iteration (Updates Only)\", ylabel = \"loss\")\nplt.title(\"Evolution of Loss Function\")\n\nText(0.5, 1.0, 'Evolution of Loss Function')\n\n\n\n\n\n\n\n\n\nAbove is the graph that shows the evolution of the loss function. It is interesting to see that the loss function was very close to 0 between step 7 to step 110 but later iater increases. It demonstrates the randomness of the perceptron algorithm when selecting data points for adjustment. It may take the algorithm a while to find the data point that the algorithm fails to classify correctly.\nNext, we want to see how does the line that separates the two classes apart looks like.\n\ndef draw_line(w, x_min, x_max, ax, **kwargs):\n    w_ = w.flatten()\n    x = torch.linspace(x_min, x_max, 101)\n    y = -(w_[0]*x + w_[2])/w_[1]\n    l = ax.plot(x, y, **kwargs)\n\nfig, ax = plt.subplots(1, 1, figsize = (4, 4))\nax.set(xlim = (-1, 2), ylim = (-1, 2))\nplot_perceptron_data(X, y, ax)\ndraw_line(p.w, -1, 2, ax, color = \"black\")\nplt.title(\"Final Decision Boundary\")\n\nText(0.5, 1.0, 'Final Decision Boundary')\n\n\n\n\n\n\n\n\n\nA line that has perfectly separated the two classes. How have this line changed throughout the training process? The following figure illustrates the perceptron algorithm in action over several iterations. The code is adopted from the lecture note provided by Professor Chodrow.\n\ntorch.manual_seed(127)\n\n# initialize a perceptron \np = Perceptron()\nopt = PerceptronOptimizer(p)\np.loss(X, y)\n\n# set up the figure\nplt.rcParams[\"figure.figsize\"] = (7, 5)\nfig, axarr = plt.subplots(3, 3, sharex = True, sharey = True)\nmarkers = [\"o\", \",\"]\nmarker_map = {-1 : 0, 1 : 1}\n\n# initialize for main loop\ncurrent_ax = 0\nloss = 1\nloss_vec = []\n\nwhile loss &gt; 0:\n\n    i = torch.randint(n, size = (1,))\n    x_i = X[[i],:]\n    y_i = y[i]\n\n    ax = axarr.ravel()[current_ax]\n\n    # save the old value of w for plotting later\n    old_w = torch.clone(p.w)\n\n    # make an optimization step -- this is where the update actually happens\n    # now p.w is the new value \n    local_loss = opt.step(x_i, y_i)\n\n    # if a change was made, plot the old and new decision boundaries\n    # also add the new loss to loss_vec for plotting below\n    if local_loss &gt; 0:\n        plot_perceptron_data(X, y, ax)\n        draw_line(old_w, x_min = -1, x_max = 2, ax = ax, color = \"black\", linestyle = \"dashed\")\n        loss = p.loss(X, y).item()\n        loss_vec.append(loss)\n        draw_line(p.w, x_min = -1, x_max = 2, ax = ax, color = \"black\")\n        ax.scatter(X[i,0],X[i,1], color = \"black\", facecolors = \"none\", edgecolors = \"black\", marker = markers[marker_map[y_i.item()]])\n        # draw_line(w, -10, 10, ax, color = \"black\")\n        ax.set_title(f\"loss = {loss:.3f}\")\n        ax.set(xlim = (-1, 2), ylim = (-1, 2))\n        current_ax += 1\n\nplt.tight_layout()\n\n\n\n\n\n\n\n\n\n\nPart B.2: Non-linearly Separable 2D data\nIt is often rare in reality to have two classes of data that are linearly separable. What will happen if the perceptron data handles a non-linearly separable 2D data? First, we want to create data points that are nonlinearly seperable. To do that, I increase the noise parameter in the function.\n\ndef nonseperable_perceptron_data(n_points=300, noise=0.7, p_dims=2):\n    y = torch.arange(n_points) &gt;= int(n_points / 2)\n    X = y[:, None] + torch.normal(0.0, noise, size=(n_points, p_dims))\n    X = torch.cat((X, torch.ones((X.shape[0], 1))), 1)\n\n    # convert y from {0, 1} to {-1, 1}\n    y = 2 * y - 1\n\n    return X, y\n\ndef plot_perceptron_data(X, y, ax):\n    assert X.shape[1] == 3, \"This function only works for data created with p_dims == 2\"\n    targets = [-1, 1]\n    markers = [\"o\" , \",\"]\n    for i in range(2):\n        ix = y == targets[i]\n        ax.scatter(X[ix,0], X[ix,1], s = 20,  c = y[ix], facecolors = \"none\", edgecolors = \"darkgrey\", cmap = \"BrBG\", vmin = -2, vmax = 2, alpha = 0.5, marker = markers[i])\n    ax.set(xlabel = r\"$x_1$\", ylabel = r\"$x_2$\")\n\nfig, ax = plt.subplots(1, 1)\nX_1, y_1 = nonseperable_perceptron_data()\nplot_perceptron_data(X_1, y_1, ax)\n\n\n\n\n\n\n\n\nThe two classes of data are mixed. Now we want to implement the perceptron algorithm. We know that the loss function will not converge to 0, so we limit the iteration of the loop to 1000 times.\n\n# instantiate a model and an optimizer\np_1 = Perceptron() \nopt = PerceptronOptimizer(p_1)\np_1.loss(X_1, y_1)\n\nloss = 1.0\n\n# for keeping track of loss values\nloss_vec = []\n\nn = X_1.size()[0]\niteration = 0\n\nwhile loss &gt; 0 and iteration &lt; 1000: # dangerous -- only terminates if data is linearly separable\n    \n    # not part of the update: just for tracking our progress    \n    loss = p_1.loss(X_1, y_1) \n    loss_vec.append(loss)\n    \n    # pick a random data point\n    i = torch.randint(n, size = (1,))\n    x_i = X_1[[i],:]\n    y_i = y_1[i]\n    \n    # perform a perceptron update using the random data point\n    opt.step(x_i, y_i)\n\n    iteration += 1\n\n\nplt.plot(loss_vec, color = \"slategrey\")\nplt.scatter(torch.arange(len(loss_vec)), loss_vec, color = \"slategrey\")\nlabs = plt.gca().set(xlabel = \"Perceptron Iteration (Updates Only)\", ylabel = \"loss\")\nplt.title(\"Evolution of Loss Function\")\n\nText(0.5, 1.0, 'Evolution of Loss Function')\n\n\n\n\n\n\n\n\n\nThe loss oscillates throughout the iteration and does not have a pattern of convergence. How does the final separation line looks?\n\nfig, ax = plt.subplots(1, 1, figsize = (4, 4))\nax.set(xlim = (-2, 3), ylim = (-2, 3))\nplot_perceptron_data(X_1, y_1, ax)\ndraw_line(p_1.w, -2, 3, ax, color = \"black\")\nplt.title(\"Final Decision Boundary\")\n\nText(0.5, 1.0, 'Final Decision Boundary')\n\n\n\n\n\n\n\n\n\n\n\nPart B.2: Linearly Separable 5D data\nIt is easier for us to understand how the algorithm work and visualize its process in a 2 dimensional space. The perceptron algorithm could work in higher dimensions as well! Now, we want to create data points with 5 features and use the graph of the loss function to determine whether the data is linearly separable or not.\n\ntorch.manual_seed(989)\n\ndef d5_perceptron_data(n_points=150, noise=0.2, p_dims=5):\n    y = torch.arange(n_points) &gt;= int(n_points / 2)\n    X = y[:, None] + torch.normal(0.0, noise, size=(n_points, p_dims))\n    X = torch.cat((X, torch.ones((X.shape[0], 1))), 1)\n\n    # convert y from {0, 1} to {-1, 1}\n    y = 2 * y - 1\n\n    return X, y\n\nX_5, y_5 = d5_perceptron_data()\n\n\n# instantiate a model and an optimizer\np_5 = Perceptron() \nopt = PerceptronOptimizer(p_5)\np_5.loss(X_5, y_5)\n\nloss = 1.0\n\n# for keeping track of loss values\nloss_vec = []\n\nn = X_5.size()[0]\niteration = 0\n\nwhile loss &gt; 0 and iteration &lt; 1000: # dangerous -- only terminates if data is linearly separable\n    \n    # not part of the update: just for tracking our progress    \n    loss = p_5.loss(X_5, y_5) \n    loss_vec.append(loss)\n    \n    # pick a random data point\n    i = torch.randint(n, size = (1,))\n    x_i = X_5[[i],:]\n    y_i = y_5[i]\n    \n    # perform a perceptron update using the random data point\n    opt.step(x_i, y_i)\n\n    iteration += 1\n\nplt.plot(loss_vec, color = \"slategrey\")\nplt.scatter(torch.arange(len(loss_vec)), loss_vec, color = \"slategrey\")\nlabs = plt.gca().set(xlabel = \"Perceptron Iteration (Updates Only)\", ylabel = \"loss\")\nplt.title(\"Evolution of Loss Function\")\n\nText(0.5, 1.0, 'Evolution of Loss Function')\n\n\n\n\n\n\n\n\n\nThe loss function converges after 61 iterations! Since the algorithm terminates and the loss converges to zero, we know that the two classes are linearly separable in this 5 dimensional space.\n\n\n\nPart C: Minibatch Perceptron Experiments\nNow, I am introducing a new perceptron algorithm called Minibatch Perceptron. Different from the classic perceptron algorithm that updates the weight vector and the linear separation line with only one random data point, the Minibatch perceptron algorithm updates them with \\(k\\) random data points. Mathematically, in each step\n\nPick \\(k\\) random indices from \\(i \\in \\{1,\\ldots,n\\}\\).\nPerform the update \\[\\mathbf{w}^{(t+1)} = \\mathbf{w}^{(t)} + \\frac{\\alpha}{k} \\sum_{j=1}^k \\mathbb{1} [\\langle \\mathbf{w}^{(t)}, \\mathbf{x}_{i_j} \\rangle y_{i_j} &lt; 0] y_{i_j} \\mathbf{x}_{i_j} \\]\n\nThe equation above computes \\(k\\) perceptron increments, average them, and apply it to the weight vector. The hyperparameter \\(\\alpha\\) is a learning rate that determines how quickly the weight vector changes in each iteration. The learning rate can be changed to get ideal result.\nThe code for the algorithm is embedded in the file MBperceptron.py. I want to thank Professor Chodrow and Lima Smith for providing me inspiration and help on improving this algorithm. Here, I am creating a function for implementing Minibatch Experiment.\n\ndef MB_experiment(X, y, k, alpha):\n    torch.manual_seed(8972)\n\n    # instantiate a model and an optimizer\n    mb_p = MBPerceptron() \n    mb_opt = MBPerceptronOptimizer(mb_p)\n    mb_p.loss(X, y)\n\n    loss_mb = 1.0\n\n    # for keeping track of loss values\n    loss_vec_mb = []\n    iter = 0\n\n    while loss_mb &gt; 0: \n    \n        # tracking our progress    \n        loss_mb = mb_p.loss(X, y) \n        loss_vec_mb.append(loss_mb)\n\n        # pick several random data points\n        ix = torch.randperm(X.size(0))[:k]\n        X_i = X[ix,:]\n        y_i = y[ix]\n    \n        # perform a perceptron update using the random data point\n        mb_opt.step(X_i, y_i, k, alpha)\n\n        # set iteration limit\n        iter += 1\n        if iter &gt;= 5000:\n            break\n\n    return loss_vec_mb, mb_p.w\n\n\nPart C.1: \\(k = 1\\)\nIn a Minibatch Perceptron, when \\(k= 1\\), it should perform just like the classic perceptron algorithm as it is updated using 1 random points in each iteration\n\nloss_vec_mb_k1, w_k1 = MB_experiment(X, y, k = 1, alpha = 1)\n\n\nplt.plot(loss_vec_mb_k1, color = \"slategrey\")\nplt.scatter(torch.arange(len(loss_vec_mb_k1)), loss_vec_mb_k1, color = \"slategrey\")\nlabs = plt.gca().set(xlabel = \"Minibatch Perceptron Iteration\", ylabel = \"loss\")\nplt.title(\"Evolution of Loss Function\")\n\nText(0.5, 1.0, 'Evolution of Loss Function')\n\n\n\n\n\n\n\n\n\n\nfig, ax = plt.subplots(1, 1, figsize = (4, 4))\nax.set(xlim = (-1, 2), ylim = (-1, 2))\nplot_perceptron_data(X, y, ax)\ndraw_line(w_k1, -1, 2, ax, color = \"black\")\nplt.title(\"Final Decision Boundary\")\n\nText(0.5, 1.0, 'Final Decision Boundary')\n\n\n\n\n\n\n\n\n\n\n\nPart C.2: \\(k = 10\\)\nNow, when \\(k = 10\\), the algorithm selects randomly 10 points and average their loss to update the linear separation line.\n\nloss_vec_mb_k10, w_k10 = MB_experiment(X, y, k = 10, alpha = 1)\n\n\nplt.plot(loss_vec_mb_k10, color = \"slategrey\")\nplt.scatter(torch.arange(len(loss_vec_mb_k10)), loss_vec_mb_k10, color = \"slategrey\")\nlabs = plt.gca().set(xlabel = \"Minibatch Perceptron Iteration\", ylabel = \"loss\")\nplt.title(\"Evolution of Loss Function\")\n\nText(0.5, 1.0, 'Evolution of Loss Function')\n\n\n\n\n\n\n\n\n\n\nfig, ax = plt.subplots(1, 1, figsize = (4, 4))\nax.set(xlim = (-1, 2), ylim = (-1, 2))\nplot_perceptron_data(X, y, ax)\ndraw_line(w_k10, -1, 2, ax, color = \"black\")\nplt.title(\"Final Decision Boundary\")\n\nText(0.5, 1.0, 'Final Decision Boundary')\n\n\n\n\n\n\n\n\n\nWhen \\(k = 10\\), the loss function converges faster and has less oscillation. The algorithm is still able to find a linear separation line between the two classes.\n\n\nPart C.3: Non-linearly Separable Data and Minibatch Perceptron\nCompared with the classic perceptron algorithm, minibatch perceptron can converge even when the data is not linearly separable. Though it is impossible for the loss to converge to 0 (perfectly classifying the data) since the data points are not linearly separable, it can help us to find a line that can classify the data points with the highest accuracy. In this part, we set \\(k = n\\), where \\(n\\) is the number of total data points, and tune the learning rate \\(\\alpha\\). First, let \\(\\alpha = 1\\)\n\nloss_vec_mb_kn, w_kn = MB_experiment(X_1, y_1, k = X.size()[0], alpha = 1)\n\n\nplt.plot(loss_vec_mb_kn, color = \"slategrey\")\nplt.scatter(torch.arange(len(loss_vec_mb_kn)), loss_vec_mb_kn, color = \"slategrey\")\nlabs = plt.gca().set(xlabel = \"Minibatch Perceptron Iteration\", ylabel = \"loss\")\nplt.title(\"Evolution of Loss Function\")\n\nText(0.5, 1.0, 'Evolution of Loss Function')\n\n\n\n\n\n\n\n\n\nWe see that the loss function is still oscillating (just like how it was before) and does not seem to converge within 1000 iterations. Now, we rigorously decrease \\(\\alpha\\) and let \\(\\alpha = 0.001\\).\n\nloss_vec_mb_kn_0_001, w_kn_0_001 = MB_experiment(X_1, y_1, k = X.size()[0], alpha = 0.001)\nplt.plot(loss_vec_mb_kn_0_001, color = \"slategrey\")\nplt.scatter(torch.arange(len(loss_vec_mb_kn_0_001)), loss_vec_mb_kn_0_001, color = \"slategrey\")\nlabs = plt.gca().set(xlabel = \"Minibatch Perceptron Iteration\", ylabel = \"loss\")\nplt.title(\"Evolution of Loss Function\")\n\nText(0.5, 1.0, 'Evolution of Loss Function')\n\n\n\n\n\n\n\n\n\nIn this graph, the loss function seems to decrease gradually as we iterate the algorithm. After 1000 iterations, the loss decreases to 0.4, which is the best result among all iterations. Let’s see how the decision boundary looks like.\n\nfig, ax = plt.subplots(1, 1, figsize = (4, 4))\nax.set(xlim = (-1, 2), ylim = (-1, 2))\nplot_perceptron_data(X_1, y_1, ax)\ndraw_line(w_kn_0_001, -1, 2, ax, color = \"black\")\nplt.title(\"Final Decision Boundary\")\n\nText(0.5, 1.0, 'Final Decision Boundary')\n\n\n\n\n\n\n\n\n\n\n\n\nPart D: Conclusion\nIn this blog post, we practiced writing down the gradient and step functions for the perceptron algorithm and implementing them in various data point scenarios. We experimented with the algorithm using 2D data that is linearly separable, non-linearly separable, and data points with more than 5 features. We found that when the data points are linearly separable, regardless of the number of dimensions, the loss function will converge to 0, and we will obtain a decision boundary that perfectly classifies the two classes. However, when the data is not linearly separable, the algorithm fails to converge to a decision boundary. To improve the algorithm’s performance on non-linearly separable data points, we introduced the Minibatch Perceptron, allowing us to choose the learning rate of the algorithm, \\(\\alpha\\), and the number of random points used when evaluating and updating the decision boundary of the two classes, \\(k\\). When \\(k=1\\), the algorithm behaves similarly to the classic one. For non-linearly separable data, we found that when \\(k = n\\) and a sufficiently small learning rate \\(\\alpha\\) is chosen, the algorithm can converge to a decision boundary that effectively separates the two classes. The blog post improved my understanding of the algorithm and helped me to learn the strength and weakness of the perceptron algorithm."
  },
  {
    "objectID": "posts/penguins/index.html",
    "href": "posts/penguins/index.html",
    "title": "Classifying Penguin Species",
    "section": "",
    "text": "Classifying Penguin Species\n\nIntroduction\nIn this post, we will be using the Palmer Penguins data set collected by Dr. Kristen Gorman and the Palmer Station, Antarctica LTER, a member of the Long Term Ecological Research Network. It was first published by Gorman, Williams, and Fraser (2014) and was nicely packaged and released for use in the data science community by Horst, Hill, and Gorman (2020). The data contains physiological measurements for a number of individuals from each of three species of penguin. The goal of this post is to investigate different machine learning models’ performance to classify penguins’ species based on penguins’ quantitative and qualitative features.\nFirst, we access the training data\n\nimport pandas as pd\nimport numpy as np\nimport warnings\n\nwarnings.filterwarnings(\"ignore\")\n\n# load dataset\ntrain_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/train.csv\"\ntrain = pd.read_csv(train_url)\ntrain.shape\n\n(275, 17)\n\n\n\n\nData Preparation\n\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nle.fit(train[\"Species\"])\n\n# function to prepare X and y. \ndef prepare_data(df):\n  df = df.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\", \"Comments\", \"Region\"], axis = 1)\n  df = df[df[\"Sex\"] != \".\"]\n  df = df.dropna()\n  y = le.transform(df[\"Species\"])\n  df = df.drop([\"Species\"], axis = 1)\n  df = pd.get_dummies(df)\n  return df, y\n\nX_train, y_train = prepare_data(train)\n\n\n# get the one vocab name for the species. \ntrain[\"Species\"] = train[\"Species\"].str.split().str.get(0)\ntrain\n\n\n\n\n\n\n\n\n\nstudyName\nSample Number\nSpecies\nRegion\nIsland\nStage\nIndividual ID\nClutch Completion\nDate Egg\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nSex\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nComments\n\n\n\n\n0\nPAL0809\n31\nChinstrap\nAnvers\nDream\nAdult, 1 Egg Stage\nN63A1\nYes\n11/24/08\n40.9\n16.6\n187.0\n3200.0\nFEMALE\n9.08458\n-24.54903\nNaN\n\n\n1\nPAL0809\n41\nChinstrap\nAnvers\nDream\nAdult, 1 Egg Stage\nN74A1\nYes\n11/24/08\n49.0\n19.5\n210.0\n3950.0\nMALE\n9.53262\n-24.66867\nNaN\n\n\n2\nPAL0708\n4\nGentoo\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN32A2\nYes\n11/27/07\n50.0\n15.2\n218.0\n5700.0\nMALE\n8.25540\n-25.40075\nNaN\n\n\n3\nPAL0708\n15\nGentoo\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN38A1\nYes\n12/3/07\n45.8\n14.6\n210.0\n4200.0\nFEMALE\n7.79958\n-25.62618\nNaN\n\n\n4\nPAL0809\n34\nChinstrap\nAnvers\nDream\nAdult, 1 Egg Stage\nN65A2\nYes\n11/24/08\n51.0\n18.8\n203.0\n4100.0\nMALE\n9.23196\n-24.17282\nNaN\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n270\nPAL0910\n100\nGentoo\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN21A2\nYes\n11/18/09\n51.1\n16.5\n225.0\n5250.0\nMALE\n8.20660\n-26.36863\nNaN\n\n\n271\nPAL0809\n69\nAdelie\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN32A1\nNo\n11/11/08\n35.9\n16.6\n190.0\n3050.0\nFEMALE\n8.47781\n-26.07821\nNest never observed with full clutch.\n\n\n272\nPAL0708\n33\nAdelie\nAnvers\nDream\nAdult, 1 Egg Stage\nN22A1\nYes\n11/9/07\n39.5\n17.8\n188.0\n3300.0\nFEMALE\n9.66523\n-25.06020\nNaN\n\n\n273\nPAL0708\n5\nAdelie\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN3A1\nYes\n11/16/07\n36.7\n19.3\n193.0\n3450.0\nFEMALE\n8.76651\n-25.32426\nNaN\n\n\n274\nPAL0708\n21\nChinstrap\nAnvers\nDream\nAdult, 1 Egg Stage\nN73A1\nYes\n12/3/07\n42.4\n17.3\n181.0\n3600.0\nFEMALE\n9.35138\n-24.68790\nNaN\n\n\n\n\n275 rows × 17 columns\n\n\n\n\n\n\nData Exploration and Visualization\n\nQuantitative Feature Exploration\nWe want to understand how different species and sexs of penguins have different physical characteristics. After some primary data exploration of the dataset, we find that there is one incidence of Gentoo penguin whose sex is unknown. I decided to drop that row.\n\ntrain = train[train['Sex'] != \".\"]\n\nDuring the lecture, we have seen the scatterplot between Culmen Depth (mm) and Culmen Length (mm). I want to investigate the other two quantitative features included in the dataset which are Flipper Length (mm) and Body Mass (g). I will find the mean values of the flipper length and body mass for sexes of each penguin species.\n\n# group by species and sex and find the mean flipper length. \nmean_flipper = train.groupby([\"Species\", \"Sex\"])[\"Flipper Length (mm)\"].mean().reset_index()\nprint(mean_flipper)\n\n     Species     Sex  Flipper Length (mm)\n0     Adelie  FEMALE           187.924528\n1     Adelie    MALE           192.327869\n2  Chinstrap  FEMALE           192.064516\n3  Chinstrap    MALE           200.692308\n4     Gentoo  FEMALE           212.836735\n5     Gentoo    MALE           221.204545\n\n\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n#plot species vs flipper length\nplt.figure(figsize=(7, 4))\nsns.barplot(data=mean_flipper, x = \"Species\", y = \"Flipper Length (mm)\", hue=\"Sex\")\nplt.title(\"Mean Filpper Length by Species and Sex\")\n\nText(0.5, 1.0, 'Mean Filpper Length by Species and Sex')\n\n\n\n\n\n\n\n\n\n\n# group by species and sex and find the mean body mass. \nmean_mass = train.groupby([\"Species\", \"Sex\"])[\"Body Mass (g)\"].mean().reset_index()\nprint(mean_mass)\n\n     Species     Sex  Body Mass (g)\n0     Adelie  FEMALE    3350.471698\n1     Adelie    MALE    4052.868852\n2  Chinstrap  FEMALE    3523.387097\n3  Chinstrap    MALE    4005.769231\n4     Gentoo  FEMALE    4684.693878\n5     Gentoo    MALE    5476.704545\n\n\n\n# plot mean body mass by species and sex\nplt.figure(figsize=(7, 4))\nsns.barplot(data=mean_mass, x = \"Species\", y = \"Body Mass (g)\", hue=\"Sex\")\nplt.title(\"Mean Body Mass by Species and Sex\")\n\nText(0.5, 1.0, 'Mean Body Mass by Species and Sex')\n\n\n\n\n\n\n\n\n\nFrom these two barplot, we can see that compare to Flipper Length (mm), the difference of mean Body Mass (g) between species seems to be more significant, if we don’t concern the scales and units of the two measures. Both sexes of Gentoo have obviously larger average weight than the other two penguin species.\nNext, I construct two scatterplots: Flipper Length vs. Culmen Length and Body Mass vs. Culmen Length. It would be ideal to see the dots of each penguin species clustering in a region, and that region does not contain other species.\n\n# plott scatter plot of the species in different variables. \nfig, ax = plt.subplots(1, 2, figsize = (10, 3.5))\n\np1 = sns.scatterplot(train, x = \"Culmen Length (mm)\", y = \"Flipper Length (mm)\", hue = \"Species\", ax = ax[0])\np2 = sns.scatterplot(train, x = \"Culmen Length (mm)\", y = \"Body Mass (g)\", hue = \"Species\", ax = ax[1])\n\n\n\n\n\n\n\n\nFrom the scatterplots above, it seems that both Flipper Length (mm) and Body Mass (g) are good at differentiating Gentoo from the other two species.\n\n\nQualitative Feature Exploration\nHere I want to explore how Island and Clutch Completion could help us in differentiating between penguin species.\n\n# count how many recorded individuals are on each island\nisland_species = train.groupby([\"Island\", \"Species\"]).size().reset_index(name='count')\nisland_species\n\n\n\n\n\n\n\n\n\nIsland\nSpecies\ncount\n\n\n\n\n0\nBiscoe\nAdelie\n33\n\n\n1\nBiscoe\nGentoo\n97\n\n\n2\nDream\nAdelie\n45\n\n\n3\nDream\nChinstrap\n57\n\n\n4\nTorgersen\nAdelie\n42\n\n\n\n\n\n\n\n\nThis table shows that Gentoo is only populated in Biscoe Island and Chinstrap is populated in Dream Island. Adelie is populated in the three islands. Therefore, Island is a good qualitative feature to distinguish between Gentoo and Chinstrap Island.\n\n# count clutch for different species\nclutch_species = train.groupby([\"Clutch Completion\", \"Species\"]).size().reset_index(name = 'count')\nclutch_species\n\n\n\n\n\n\n\n\n\nClutch Completion\nSpecies\ncount\n\n\n\n\n0\nNo\nAdelie\n12\n\n\n1\nNo\nChinstrap\n10\n\n\n2\nNo\nGentoo\n8\n\n\n3\nYes\nAdelie\n108\n\n\n4\nYes\nChinstrap\n47\n\n\n5\nYes\nGentoo\n89\n\n\n\n\n\n\n\n\nFrom the table above, we cannot see a very clear distinction between penguin species and their clutch completion status. Therefore, we are not using this feature as part of our feature combination.\n\n\n\nChoosing Features by Logistic Regression\n\nfrom itertools import combinations\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_val_score\n\n\nLR = LogisticRegression()\n\n# these are not actually all the columns: you'll \n# need to add any of the other ones you want to search for\nall_qual_cols = [\"Island\"]\nall_quant_cols = ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Flipper Length (mm)', 'Body Mass (g)']\n\n# initialize best score and best features. \nbest_score = 0\nbest_features = []\n\n# loop through combinations of columns to find the best one. \nfor qual in all_qual_cols: \n  qual_cols = [col for col in X_train.columns if qual in col ]\n  for pair in combinations(all_quant_cols, 2):\n    cols = qual_cols + list(pair) \n    LR.fit(X_train[cols], y_train)\n    cv_scores_LR = cross_val_score(LR, X_train[cols], y_train, cv = 5).mean()\n\n    # select the combination that has the best score. \n    if cv_scores_LR &gt; best_score:\n            best_score = cv_scores_LR\n            best_columns = cols\n     \nprint(\"Best columns:\", best_columns)\nprint(\"Best score:\", best_score)\n\nBest columns: ['Island_Biscoe', 'Island_Dream', 'Island_Torgersen', 'Culmen Length (mm)', 'Culmen Depth (mm)']\nBest score: 0.9922322775263952\n\n\nThe best features for the logistic regression model are Island, Culmen Length (mm), and Culmen Depth (mm). It means that only using this combination of features could easily help us differentiate between penguin species.\n\n\nModel Choices\nWe used Logistic Regression to choose the feature combination. We will use that combination for our model selection. First, here is the accuracy of Logistic Regression, which we have shown above.\n\n# fit logistic regression to training data\nLR.fit(X_train[best_columns], y_train)\ncv_scores_LR = cross_val_score(LR, X_train[best_columns], y_train, cv = 5).mean()\ncv_scores_LR\n\n0.9922322775263952\n\n\nWe have several other models to choose from. These models are Decision Trees, Random Forest, and SVC. We first import these models from sklearn. Let’s build these models and see their performance on the training data themselves. Similar to the section above, we would employ cross-validation and calculate the mean accuracy to evaluate the performance of the model.\n\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\n\nDecision Tree\n\n# initialize decison tree classifier\nDT = DecisionTreeClassifier(max_depth=5, random_state=30)\n# fit decision tree classifier to training data. \nDT.fit(X_train[best_columns], y_train)\ncv_Scores_DT = cross_val_score(DT, X_train[best_columns], y_train, cv = 5).mean()\ncv_Scores_DT\n\n0.9687028657616892\n\n\nRandom Forest\n\n# initialize random forest\nRF = RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1, random_state=42)\n# fit the model to training data\nRF.fit(X_train[best_columns], y_train)\ncv_Scores_RF = cross_val_score(RF, X_train[best_columns], y_train, cv = 5).mean()\ncv_Scores_RF\n\n0.9883107088989442\n\n\nSupport Vector Machine\n\n# generate gamma values using logspace. \ngamma_values = np.logspace(-3, 2, num=6)\nprint(gamma_values)\n# loop over different gamma values and find the one that gives us th best result\nfor gma in gamma_values:\n    svc = SVC(gamma=gma, C=1, random_state=42)\n    svc.fit(X_train[best_columns], y_train)\n    cv_Scores_SVC = cross_val_score(svc, X_train[best_columns], y_train, cv = 5).mean()\n    print(cv_Scores_SVC)\n\n[1.e-03 1.e-02 1.e-01 1.e+00 1.e+01 1.e+02]\n0.9217948717948717\n0.9648567119155353\n0.9687782805429863\n0.9610105580693815\n0.7578431372549019\n0.4530920060331825\n\n\nI will choose gamma = 0.1 as the parameter for the SVC model.\n\n\nTesting\n\ntest_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/test.csv\"\ntest = pd.read_csv(test_url)\n# separate variables. \nX_test, y_test = prepare_data(test)\n# test scores for different models. \nLR_test_score = LR.score(X_test[best_columns], y_test)\nDT_test_score = DT.score(X_test[best_columns], y_test)\nRF_test_score = RF.score(X_test[best_columns], y_test)\n# support vector machine and its test score\nsvc = SVC(gamma=0.1, C=1, random_state=42)\nsvc.fit(X_train[best_columns], y_train)\nSVC_test_score = svc.score(X_test[best_columns], y_test)\n\nprint(\"Logistic Regression Score:\", LR_test_score)\nprint(\"Decision Tree Score:\", DT_test_score)\nprint(\"Random Forest Score:\", RF_test_score)\nprint(\"SVC Score:\", SVC_test_score)\n\nLogistic Regression Score: 1.0\nDecision Tree Score: 0.9852941176470589\nRandom Forest Score: 1.0\nSVC Score: 0.9852941176470589\n\n\nOur testig results show that logiistic regression model and random forest model both give us 100% accuracy for the testing data!\n\n\nPlotting Decision Regions\n\n# Since my best columns' first three elements are qualitative. I modified the function. \n\nfrom matplotlib.patches import Patch\n# function to plot decision regions. \ndef plot_regions(model, X, y):\n    \n    x0 = X[X.columns[3]]\n    x1 = X[X.columns[4]]\n    qual_features = X.columns[0:3]\n    \n    fig, axarr = plt.subplots(1, len(qual_features), figsize = (7, 3))\n\n    # create a grid\n    grid_x = np.linspace(x0.min(),x0.max(),501)\n    grid_y = np.linspace(x1.min(),x1.max(),501)\n    xx, yy = np.meshgrid(grid_x, grid_y)\n    \n    XX = xx.ravel()\n    YY = yy.ravel()\n\n    for i in range(len(qual_features)):\n      XY = pd.DataFrame({\n          X.columns[3] : XX,\n          X.columns[4] : YY\n      })\n\n      for j in qual_features:\n        XY[j] = 0\n\n      XY[qual_features[i]] = 1\n\n      XY = XY[X.columns]\n\n      p = model.predict(XY)\n      p = p.reshape(xx.shape)\n      \n      \n      # use contour plot to visualize the predictions\n      axarr[i].contourf(xx, yy, p, cmap = \"jet\", alpha = 0.2, vmin = 0, vmax = 2)\n      \n      ix = X[qual_features[i]] == 1\n      # plot the data\n      axarr[i].scatter(x0[ix], x1[ix], c = y[ix], cmap = \"jet\", vmin = 0, vmax = 2)\n      \n      axarr[i].set(xlabel = X.columns[3], \n            ylabel  = X.columns[4], \n            title = qual_features[i])\n      \n      patches = []\n      for color, spec in zip([\"red\", \"green\", \"blue\"], [\"Adelie\", \"Chinstrap\", \"Gentoo\"]):\n        patches.append(Patch(color = color, label = spec))\n\n      plt.legend(title = \"Species\", handles = patches, loc = \"best\")\n      \n      plt.tight_layout()\n\nHere is the decision regions of Logistic Regression Model, which gave us 100% accuracy rate.\n\nplot_regions(LR, X_train[best_columns], y_train)\n\n\n\n\n\n\n\n\nHere is the decision regions of Random Forest Model, which gave us 100% accuracy rate.\n\nplot_regions(RF, X_train[best_columns], y_train)\n\n\n\n\n\n\n\n\nIt is noticeable that the decision regions for Chinstrap and Adelie penguins got more restrictive and small. Only regions that are very close to the training data are included as the decision region for these two species. The boundary of the region got more cursive than it was when \\(\\mathbf{gamma} = 0.1\\)\n\n\nConfusion Matrix\nConfusion Matrix for Logistic Regression\n\nfrom sklearn.metrics import confusion_matrix\n\ny_test_pred_LR = LR.predict(X_test[best_columns])\nLR_C = confusion_matrix(y_test, y_test_pred_LR)\nLR_C\n\narray([[31,  0,  0],\n       [ 0, 11,  0],\n       [ 0,  0, 26]])\n\n\nConfusion Matrix for Decision Trees\n\ny_test_pred_DT = DT.predict(X_test[best_columns])\nDT_C = confusion_matrix(y_test, y_test_pred_DT)\nDT_C\n\narray([[30,  0,  1],\n       [ 0, 11,  0],\n       [ 0,  0, 26]])\n\n\n\nfor i in range(3):\n    for j in range(3):\n        print(f\"There were {DT_C[i,j]} {le.classes_[i]} penguin(s) who were classified as {le.classes_[j]}.\")\n\nThere were 30 Adelie Penguin (Pygoscelis adeliae) penguin(s) who were classified as Adelie Penguin (Pygoscelis adeliae).\nThere were 0 Adelie Penguin (Pygoscelis adeliae) penguin(s) who were classified as Chinstrap penguin (Pygoscelis antarctica).\nThere were 1 Adelie Penguin (Pygoscelis adeliae) penguin(s) who were classified as Gentoo penguin (Pygoscelis papua).\nThere were 0 Chinstrap penguin (Pygoscelis antarctica) penguin(s) who were classified as Adelie Penguin (Pygoscelis adeliae).\nThere were 11 Chinstrap penguin (Pygoscelis antarctica) penguin(s) who were classified as Chinstrap penguin (Pygoscelis antarctica).\nThere were 0 Chinstrap penguin (Pygoscelis antarctica) penguin(s) who were classified as Gentoo penguin (Pygoscelis papua).\nThere were 0 Gentoo penguin (Pygoscelis papua) penguin(s) who were classified as Adelie Penguin (Pygoscelis adeliae).\nThere were 0 Gentoo penguin (Pygoscelis papua) penguin(s) who were classified as Chinstrap penguin (Pygoscelis antarctica).\nThere were 26 Gentoo penguin (Pygoscelis papua) penguin(s) who were classified as Gentoo penguin (Pygoscelis papua).\n\n\nOne Adelie Penguin is misidentified as a Gentoo Penguin by the Decision Trees Model.\nConfusion Matrix for Random Forest\n\ny_test_pred_RF = RF.predict(X_test[best_columns])\nRF_C = confusion_matrix(y_test, y_test_pred_RF)\nRF_C\n\narray([[31,  0,  0],\n       [ 0, 11,  0],\n       [ 0,  0, 26]])\n\n\nConfusion Matrix for SVC\n\ny_test_pred_svc = svc.predict(X_test[best_columns])\nsvc_C = confusion_matrix(y_test, y_test_pred_svc)\nsvc_C\n\narray([[31,  0,  0],\n       [ 0, 11,  0],\n       [ 0,  1, 25]])\n\n\n\nfor i in range(3):\n    for j in range(3):\n        print(f\"There were {svc_C[i,j]} {le.classes_[i]} penguin(s) who were classified as {le.classes_[j]}.\")\n\nThere were 31 Adelie Penguin (Pygoscelis adeliae) penguin(s) who were classified as Adelie Penguin (Pygoscelis adeliae).\nThere were 0 Adelie Penguin (Pygoscelis adeliae) penguin(s) who were classified as Chinstrap penguin (Pygoscelis antarctica).\nThere were 0 Adelie Penguin (Pygoscelis adeliae) penguin(s) who were classified as Gentoo penguin (Pygoscelis papua).\nThere were 0 Chinstrap penguin (Pygoscelis antarctica) penguin(s) who were classified as Adelie Penguin (Pygoscelis adeliae).\nThere were 11 Chinstrap penguin (Pygoscelis antarctica) penguin(s) who were classified as Chinstrap penguin (Pygoscelis antarctica).\nThere were 0 Chinstrap penguin (Pygoscelis antarctica) penguin(s) who were classified as Gentoo penguin (Pygoscelis papua).\nThere were 0 Gentoo penguin (Pygoscelis papua) penguin(s) who were classified as Adelie Penguin (Pygoscelis adeliae).\nThere were 1 Gentoo penguin (Pygoscelis papua) penguin(s) who were classified as Chinstrap penguin (Pygoscelis antarctica).\nThere were 25 Gentoo penguin (Pygoscelis papua) penguin(s) who were classified as Gentoo penguin (Pygoscelis papua).\n\n\nOne Gentoo penguin is misidentified as a Chinstrap penguin by the SVC model.\n\n\nDiscussion\nIn this post, I made some further investigation of the Penguin Dataset and tried out different machine learning models for classification. I selected the three features for classification using some preliminary data exploration and testing iterations of feature combinations on the Logistic Regression model to find the combination that gives me the best accuracy in the model. Then I tested Decision Tree, Random Forest, and Support Vector Machine models using the cross-validation on the training data and plotted their decision regions. At the final testing step, I used all these models and found that Logistic Regression and Random Forest models both give me a 100% accuracy.\nWhen choosing the parameters at the modeling building step, I ran into risk of overfitting my model, even though a large portion of the risk is avoided by cross-validation. For example, when selecting the gamma value, I chose the gamma that gives me the best result in the training data.\nIt is very interesting to see the decision regions of different models. The shapes of the regions reflect how different models work and classify the data. The boundaries of the tree models’ decision regions are parallel to the axis, while the logistic regression and support vector machine has linear and non-linear lines to seperate different decision regions. Also, when \\(\\mathbf{gamma}\\) gets larger, the model becomes more prudent in drawing decision regions for two penguin species, risking of overfitting the model."
  },
  {
    "objectID": "posts/whocost/index.html",
    "href": "posts/whocost/index.html",
    "title": "‘Optimal’ Decision-Making",
    "section": "",
    "text": "‘Optimal’ Decision-Making\n\n2024 Mar 1 Yide (Alex) Xu\nIn this blog post, I extened on what we have learned during the lecture on making (binary) decisions based on a linear score function. During the class, we simplified the process by using only two features and failing to make an attempt to find an optimal vector of weights. In this blog post, I will explore a way to find the optimal weight vector \\(\\mathbf{w}\\) using the logistic regression method and incorporated formulas that calculate the bank’s gain/loss on each loan to find the threshold that would help the bank to make the most profitable decision on loan granting. After testing the model that I build on a testing data set, I found that my model would less likely to grant loans to people within the 20 to 30 year age range and more favorable to people with higher income and lower interest rate.\n\n\nPart A: Grab the Data\nFirst, we want to download the training data.\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\nimport warnings\n\nwarnings.filterwarnings(\"ignore\")\n\n\nurl = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/credit-risk/train.csv\"\ndf_train = pd.read_csv(url)\ndf_train.head()\n\nIntel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\nIntel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n\n\n\n\n\n\n\n\n\n\nperson_age\nperson_income\nperson_home_ownership\nperson_emp_length\nloan_intent\nloan_grade\nloan_amnt\nloan_int_rate\nloan_status\nloan_percent_income\ncb_person_default_on_file\ncb_person_cred_hist_length\n\n\n\n\n0\n25\n43200\nRENT\nNaN\nVENTURE\nB\n1200\n9.91\n0\n0.03\nN\n4\n\n\n1\n27\n98000\nRENT\n3.0\nEDUCATION\nC\n11750\n13.47\n0\n0.12\nY\n6\n\n\n2\n22\n36996\nRENT\n5.0\nEDUCATION\nA\n10000\n7.51\n0\n0.27\nN\n4\n\n\n3\n24\n26000\nRENT\n2.0\nMEDICAL\nC\n1325\n12.87\n1\n0.05\nN\n4\n\n\n4\n29\n53004\nMORTGAGE\n2.0\nHOMEIMPROVEMENT\nA\n15000\n9.63\n0\n0.28\nN\n10\n\n\n\n\n\n\n\n\nThe columns in this data are:\n\nperson_age, the age of the prospective borrower.\nperson_income, the income of the prospective borrower at time of application.\nperson_home_ownership, the home ownership status of the prospective borrower at time of application. Possible values are MORTGAGE, OWN, RENT, and OTHER.\nperson_emp_length, the length of most recent employment for the prospective borrower, in years.\nloan_intent, the purpose of the loan request.\nloan_grade, a composite measure of the likelihood of the borrower to repay the loan.\nloan_amnt, the amount of the loan.\nloan_int_rate, the annual interest rate on the loan.\nloan_status, whether or not the borrower defaulted on the loan. This is the target variable.\nloan_percent_income, the amount of the loan as a percentage of the prospective borrower’s personal income.\ncb_person_default_on_file, whether the prospective borrower has previously defaulted on a loan in the records of a credit bureau.\ncb_person_cred_hist_length, the length of credit history of the prospective borrower.\n\n\n\nPart B: Explore the Data\nFrom this data, I first want to know how does different loan intent vary with age? People within different age range may have different needs for money. I would like to see how the data set says about how different age group would have different purpose in borrowing money from the bank.\nFist, I would classify each individual in the data set into different age range.\n\nage_ranges = [(10, 20), (20, 30), (30, 40), (40, 50), (50, 60), (60, 70), (70, 80)]\n\ndef assign_age_range(age):\n    for lower, upper in age_ranges:\n        if lower &lt;= age &lt; upper:\n            return f\"{lower}-{upper}\"\n\ndf_train['Age_Range'] = df_train['person_age'].apply(assign_age_range)\ndf_train.head()\n\n\n\n\n\n\n\n\n\nperson_age\nperson_income\nperson_home_ownership\nperson_emp_length\nloan_intent\nloan_grade\nloan_amnt\nloan_int_rate\nloan_status\nloan_percent_income\ncb_person_default_on_file\ncb_person_cred_hist_length\nAge_Range\n\n\n\n\n0\n25\n43200\nRENT\nNaN\nVENTURE\nB\n1200\n9.91\n0\n0.03\nN\n4\n20-30\n\n\n1\n27\n98000\nRENT\n3.0\nEDUCATION\nC\n11750\n13.47\n0\n0.12\nY\n6\n20-30\n\n\n2\n22\n36996\nRENT\n5.0\nEDUCATION\nA\n10000\n7.51\n0\n0.27\nN\n4\n20-30\n\n\n3\n24\n26000\nRENT\n2.0\nMEDICAL\nC\n1325\n12.87\n1\n0.05\nN\n4\n20-30\n\n\n4\n29\n53004\nMORTGAGE\n2.0\nHOMEIMPROVEMENT\nA\n15000\n9.63\n0\n0.28\nN\n10\n20-30\n\n\n\n\n\n\n\n\n\nage_intent = df_train.groupby([\"Age_Range\", 'loan_intent']).size().reset_index(name='count')\n\n\nplt.figure(figsize=(10, 6))\nsns.barplot(data=age_intent, x='Age_Range', y='count', hue='loan_intent')\nplt.title('Loan Intent by Age Range')\nplt.xlabel('Age Range')\nplt.ylabel('Count')\nplt.legend(bbox_to_anchor=(0.9, 0.9), loc='upper right')\nplt.show()\n\n\n\n\n\n\n\n\nThe number of individuals within 20 to 30 years old who are seeking loans is significantly higher than all other age groups. There are smaller proportion of this age group population that are seeking home improvement loans comparing to other loan intentions. As the age increases, there are less incident of people looking for loan from the bank.\nNext, I want to see how interest rates are set given people with different income level. Also, how does an individual’s credit history affect the interest rate.\n\ndf_train['person_emp_length'].fillna(0, inplace=True)\ndf_train = df_train[df_train['person_emp_length'] &lt;= 50]\ndf_train = df_train[df_train['person_income'] &lt;= 3000000]\n\nplt.figure(figsize=(10, 6))\nsns.scatterplot(data=df_train, x='person_income', y='loan_int_rate', hue='cb_person_default_on_file', alpha=0.3)\nplt.title(\"Interest Rate vs. Income by Person's Default History\")\nplt.xlabel('Person Income')\nplt.ylabel('Interest Rate')\nplt.legend(title='Whether Defaulted Before')\nplt.show()\n\n\n\n\n\n\n\n\nIt is apparent that people who have defaulted before receive higher interest rates. In fact, almost all of the individuals who have defaulted before will have an interest rate that is higher than 12.5 percent. In terms of interest rates’ relation with income, people with higher income usually receive lower interest rate. Last but not least, I want to see how interest rate is affected by other variables, namely age, home ownership, and loan intention.\n\nplt.figure(figsize=(12, 8))\n\nplt.subplot(411)\nsns.boxplot(data=df_train, x='loan_int_rate', y='Age_Range', color='turquoise', width=0.5)\nplt.title('Distribution of Interest Rates by Age Range')\nplt.ylabel('Age Range')\n\nplt.subplot(412)\nsns.boxplot(data=df_train, y='person_home_ownership', x='loan_int_rate', color='turquoise', width=0.5)\nplt.ylabel('Home Ownership')\n\n\nplt.subplot(413)\nsns.boxplot(data=df_train, y='loan_intent', x='loan_int_rate', color='turquoise', width=0.5)\nplt.ylabel('Loan Intent')\n\nplt.subplot(414)\nsns.boxplot(data=df_train, y='cb_person_default_on_file', x='loan_int_rate', color='turquoise', width=0.5)\nplt.ylabel('Default History')\n\nText(0, 0.5, 'Default History')\n\n\n\n\n\n\n\n\n\nCompared to the interest rate’s difference in the defult history variable, the differences of interest rates within other variables (age, home ownership, and loan intent) are not that significant.\n\n\nBuild a Model\nBefore we dive into building a model, we want to prepare the data for training. We first seperate our target variable from the training data and create dummy variables.\n\nfrom itertools import combinations\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_val_score\n\n\ndef prepare_data(df_train):\n  df_train = df_train.dropna()\n  y = df_train[\"loan_status\"]\n  df = df_train.drop([\"loan_status\"], axis = 1)\n  df = pd.get_dummies(df)\n  return df, y\n\nX_train, y_train = prepare_data(df_train)\nX_train.head(10)\n\n\n\n\n\n\n\n\n\nperson_age\nperson_income\nperson_emp_length\nloan_amnt\nloan_int_rate\nloan_percent_income\ncb_person_cred_hist_length\nperson_home_ownership_MORTGAGE\nperson_home_ownership_OTHER\nperson_home_ownership_OWN\n...\nloan_grade_F\nloan_grade_G\ncb_person_default_on_file_N\ncb_person_default_on_file_Y\nAge_Range_20-30\nAge_Range_30-40\nAge_Range_40-50\nAge_Range_50-60\nAge_Range_60-70\nAge_Range_70-80\n\n\n\n\n0\n25\n43200\n0.0\n1200\n9.91\n0.03\n4\nFalse\nFalse\nFalse\n...\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n1\n27\n98000\n3.0\n11750\n13.47\n0.12\n6\nFalse\nFalse\nFalse\n...\nFalse\nFalse\nFalse\nTrue\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n2\n22\n36996\n5.0\n10000\n7.51\n0.27\n4\nFalse\nFalse\nFalse\n...\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n3\n24\n26000\n2.0\n1325\n12.87\n0.05\n4\nFalse\nFalse\nFalse\n...\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n4\n29\n53004\n2.0\n15000\n9.63\n0.28\n10\nTrue\nFalse\nFalse\n...\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n6\n21\n21700\n2.0\n5500\n14.91\n0.25\n2\nFalse\nFalse\nFalse\n...\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n7\n39\n43000\n3.0\n6250\n7.68\n0.15\n14\nTrue\nFalse\nFalse\n...\nFalse\nFalse\nTrue\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\n\n\n8\n36\n59004\n11.0\n10000\n7.51\n0.17\n15\nTrue\nFalse\nFalse\n...\nFalse\nFalse\nTrue\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\n\n\n9\n22\n34500\n2.0\n5000\n16.89\n0.14\n3\nFalse\nFalse\nFalse\n...\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n10\n22\n33640\n4.0\n12000\n10.65\n0.36\n3\nTrue\nFalse\nFalse\n...\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n\n\n10 rows × 32 columns\n\n\n\n\nI would like to fit a Logistic Regression model as the score-based machine learning model to the data. First, I want to select my features. According to my exploration, I found that people who have defaulted their previous loan(s) are more likely to have higher interest rate, which could lead to higher risk of defaulting on a new loan. Therefore, I include both the default history and the loan interest rate as the features to predict loan defaulting. Next, I include loan_percent_income because it reflects the ability for an individual to pay the interest using its source of income. Lastly, I include the individual’s home ownerhsip status as a factor because unstable shelter status could deprecate one’s ability to pay back the loan.\n\nLR = LogisticRegression()\n\nquant = [\"loan_percent_income\", \"loan_int_rate\"]\nselected_columns = [\"loan_percent_income\", \"loan_int_rate\", \"person_home_ownership_MORTGAGE\", \"person_home_ownership_OTHER\", \"person_home_ownership_OWN\", \"person_home_ownership_RENT\", \"cb_person_default_on_file_N\", \"cb_person_default_on_file_Y\"]\n\nLR.fit(X_train[selected_columns], y_train)\ncv_scores_LR = cross_val_score(LR, X_train[selected_columns], y_train, cv = 5).mean()\n\ncv_scores_LR\n\n0.8436196450239821\n\n\nHere we have the weight vector \\(\\mathbf{w}\\)\n\nLR.coef_[0]\n\narray([ 8.55153096,  0.27487555, -0.04240708,  0.29801174, -1.11860094,\n        0.9429623 , -0.01229506,  0.09226108])\n\n\n\n\nPart D: Find a Threshold\n\ndef score_function(coef, cols, df):\n    scores = np.dot(df[cols].values, coef[0])\n    return scores\n\nX_train['new_score'] = score_function(LR.coef_, selected_columns, X_train)\n\n\nbest_profit = 0\nbest_t = 0\nX_train[\"real_default\"] = y_train\nT = np.linspace(X_train['new_score'].min()-0.1, X_train['new_score'].max()+0.1, 120)\nprofit_frame = pd.DataFrame(columns=['threshold', 'profit'])\n\n\nfor t in np.linspace(X_train['new_score'].min()-0.1, X_train['new_score'].max()+0.1, 120):\n    #y_pred = X_train['new_score'] &gt;= t\n\n    TrueNegative = X_train[(X_train['new_score'] &lt; t) & (X_train['real_default'] == 0)]\n    TrueNegative_list = TrueNegative[\"loan_amnt\"] * (1 + 0.25 * TrueNegative[\"loan_int_rate\"]/100)**10 - TrueNegative[\"loan_amnt\"]\n    TrueNegative_gain = TrueNegative_list.sum()\n\n    FalseNegative = X_train[(X_train['new_score'] &lt; t) & (X_train['real_default'] == 1)]\n    FalseNegative_list = FalseNegative[\"loan_amnt\"] * (1 + 0.25 * FalseNegative[\"loan_int_rate\"]/100)**3 - 1.7 * FalseNegative[\"loan_amnt\"]\n    FalseNegative_cost = FalseNegative_list.sum()\n\n    #TNR = np.zeros(len(np.linspace(0, 12, 1201)))\n    #FNR = np.zeros(len(np.linspace(0, 12, 1201)))\n    profit = TrueNegative_gain + FalseNegative_cost\n    \n    if profit &gt; best_profit:\n            best_t = t\n            best_profit = profit\n    \n    new_row = pd.Series({'threshold': t, 'profit': profit})\n    profit_frame.loc[len(profit_frame)] = new_row\n\nprint(\"Best profit\", best_profit)\nprint(\"is obtained when the threshold is\", best_t)\nprofit_frame\n\nBest profit 32676339.955216546\nis obtained when the threshold is 6.554722944025221\n\n\n\n\n\n\n\n\n\n\nthreshold\nprofit\n\n\n\n\n0\n0.429960\n0.000000e+00\n\n\n1\n0.522760\n0.000000e+00\n\n\n2\n0.615559\n6.050850e+02\n\n\n3\n0.708358\n1.681992e+03\n\n\n4\n0.801158\n5.463633e+03\n\n\n...\n...\n...\n\n\n115\n11.101895\n1.879761e+07\n\n\n116\n11.194695\n1.878627e+07\n\n\n117\n11.287494\n1.878627e+07\n\n\n118\n11.380294\n1.876960e+07\n\n\n119\n11.473093\n1.876960e+07\n\n\n\n\n120 rows × 2 columns\n\n\n\n\n\n\nplt.plot(profit_frame['threshold'], profit_frame['profit'])\n\nplt.xlabel('Threshold')\nplt.ylabel('Profit')\nplt.title('Profit vs. Threshold')\n\nText(0.5, 1.0, 'Profit vs. Threshold')\n\n\n\n\n\n\n\n\n\nFrom this diagram, we can see that the profit peaks when we set the threshold to be around 6. In fact, my code has determined that the when we set the threshold to be 6.55, the best profit can be reached. At \\(t = 6.55\\), the profit per borrower is\n\nbest_profit/len(X_train)\n\n1387.0591712036908\n\n\n\n\nEvaluate Your Model from the Bank’s Perspective\nHere, I am accessing my test data and use the same method for training data.\n\nurl = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/credit-risk/test.csv\"\ndf_test = pd.read_csv(url)\ndf_test.dropna(inplace=True)\ndf_test\n\n\n\n\n\n\n\n\n\nperson_age\nperson_income\nperson_home_ownership\nperson_emp_length\nloan_intent\nloan_grade\nloan_amnt\nloan_int_rate\nloan_status\nloan_percent_income\ncb_person_default_on_file\ncb_person_cred_hist_length\n\n\n\n\n0\n21\n42000\nRENT\n5.0\nVENTURE\nD\n1000\n15.58\n1\n0.02\nN\n4\n\n\n1\n32\n51000\nMORTGAGE\n2.0\nDEBTCONSOLIDATION\nB\n15000\n11.36\n0\n0.29\nN\n9\n\n\n2\n35\n54084\nRENT\n2.0\nDEBTCONSOLIDATION\nC\n3000\n12.61\n0\n0.06\nN\n6\n\n\n3\n28\n66300\nMORTGAGE\n11.0\nMEDICAL\nD\n12000\n14.11\n1\n0.15\nN\n6\n\n\n4\n22\n70550\nRENT\n0.0\nMEDICAL\nE\n7000\n15.88\n1\n0.08\nN\n3\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n6511\n29\n78000\nMORTGAGE\n2.0\nPERSONAL\nA\n18000\n6.62\n0\n0.23\nN\n5\n\n\n6513\n27\n44640\nRENT\n0.0\nMEDICAL\nB\n12800\n11.83\n0\n0.29\nN\n9\n\n\n6514\n24\n48000\nOWN\n5.0\nVENTURE\nA\n10400\n7.37\n0\n0.22\nN\n3\n\n\n6515\n26\n65000\nMORTGAGE\n6.0\nEDUCATION\nA\n6000\n9.07\n0\n0.09\nN\n3\n\n\n6516\n29\n61000\nRENT\n12.0\nVENTURE\nD\n10000\n16.07\n0\n0.16\nN\n9\n\n\n\n\n5731 rows × 12 columns\n\n\n\n\nHere, I use my weight vector, w, and my threshold t to compute the expected profit per loan for the bank using the test data.\n\ndf_test_forModel = pd.get_dummies(df_test)\ndf_test_forModel = df_test_forModel[selected_columns]\n\nt=6.55\n\ndf_test['new_score'] = score_function(LR.coef_, selected_columns, df_test_forModel)\ndf_test['predicted_loan_status'] = df_test['new_score'] &gt;= t\n\n\nTrueNegative = df_test[(df_test['predicted_loan_status'] == False) & (df_test['loan_status'] == 0)]\nTrueNegative_list = TrueNegative[\"loan_amnt\"] * (1 + 0.25 * TrueNegative[\"loan_int_rate\"]/100)**10 - TrueNegative[\"loan_amnt\"]\nTrueNegative_gain = TrueNegative_list.sum()\n\nFalseNegative = df_test[(df_test['predicted_loan_status'] == False) & (df_test['loan_status'] == 1)]\nFalseNegative_list = FalseNegative[\"loan_amnt\"] * (1 + 0.25 * FalseNegative[\"loan_int_rate\"]/100)**3 - 1.7 * FalseNegative[\"loan_amnt\"]\nFalseNegative_cost = FalseNegative_list.sum()\n\n(FalseNegative_cost+TrueNegative_gain)/len(df_test)\n\n1325.9668532238354\n\n\nUsing my model, the bank can expect to make about $1325.97 per loan.\n\n\nEvaluate Your Model From the Borrower’s Perspective\nIs it more difficult for people in certain age groups to access credit under my proposed system? Let’s see the result.\n\ndf_test['Age_Range'] = df_test['person_age'].apply(assign_age_range)\ndf_test.groupby([\"Age_Range\",\"predicted_loan_status\"]).size().reset_index(name='count')\n\n\n\n\n\n\n\n\n\nAge_Range\npredicted_loan_status\ncount\n\n\n\n\n0\n20-30\nFalse\n3468\n\n\n1\n20-30\nTrue\n673\n\n\n2\n30-40\nFalse\n1107\n\n\n3\n30-40\nTrue\n167\n\n\n4\n40-50\nFalse\n233\n\n\n5\n40-50\nTrue\n27\n\n\n6\n50-60\nFalse\n38\n\n\n7\n50-60\nTrue\n4\n\n\n8\n60-70\nFalse\n6\n\n\n9\n60-70\nTrue\n6\n\n\n10\n70-80\nFalse\n2\n\n\n\n\n\n\n\n\n\npercentage = df_test.groupby(\"Age_Range\")[\"predicted_loan_status\"].agg(lambda x: (x == True).mean() * 100).reset_index() # adopted from ChatGPT\n\n\n\n\n\n\n\n\n\nAge_Range\npredicted_loan_status\n\n\n\n\n0\n20-30\n16.252113\n\n\n1\n30-40\n13.108320\n\n\n2\n40-50\n10.384615\n\n\n3\n50-60\n9.523810\n\n\n4\n60-70\n50.000000\n\n\n5\n70-80\n0.000000\n\n\n\n\n\n\n\n\nThe predicted_loan_status column above shows the percentage of people that the model will grant a loan among different age groups. Although 50 percent of people are predicted to be defaulted for the age 50 to 60 group, the number of sample in this group is very small (only 4 person). As the age range increases, the probability that my model predict an individual to default on a loan decreases. Now, we want to see how different loan intents may affect how my model would be more likely or less likely to give them grants.\n\ndf_test.groupby([\"loan_intent\",\"predicted_loan_status\"]).size().reset_index(name='count')\n\n\n\n\n\n\n\n\n\nloan_intent\npredicted_loan_status\ncount\n\n\n\n\n0\nDEBTCONSOLIDATION\nFalse\n756\n\n\n1\nDEBTCONSOLIDATION\nTrue\n148\n\n\n2\nEDUCATION\nFalse\n1008\n\n\n3\nEDUCATION\nTrue\n168\n\n\n4\nHOMEIMPROVEMENT\nFalse\n535\n\n\n5\nHOMEIMPROVEMENT\nTrue\n81\n\n\n6\nMEDICAL\nFalse\n897\n\n\n7\nMEDICAL\nTrue\n176\n\n\n8\nPERSONAL\nFalse\n834\n\n\n9\nPERSONAL\nTrue\n164\n\n\n10\nVENTURE\nFalse\n824\n\n\n11\nVENTURE\nTrue\n140\n\n\n\n\n\n\n\n\n\npercentage_1 = df_test.groupby(\"loan_intent\")[\"predicted_loan_status\"].agg(lambda x: (x == True).mean() * 100).reset_index()\n\n\n\n\n\n\n\n\n\nloan_intent\npredicted_loan_status\n\n\n\n\n0\nDEBTCONSOLIDATION\n16.371681\n\n\n1\nEDUCATION\n14.285714\n\n\n2\nHOMEIMPROVEMENT\n13.149351\n\n\n3\nMEDICAL\n16.402610\n\n\n4\nPERSONAL\n16.432866\n\n\n5\nVENTURE\n14.522822\n\n\n\n\n\n\n\n\nThe predicted_loan_status column above shows the percentage of people that the model would grant loan for different intents. According to the prediction of my model, the probability for a loan to be defaulted across different loaning intent is similar. Home imporvement loans see a slightly lower probability of defaulting. Next, we want to see what is is the count of default for loans with different intents in reality.\n\ndf_test.groupby([\"loan_intent\",\"loan_status\"]).size().reset_index(name='count')\n\n\n\n\n\n\n\n\n\nloan_intent\nloan_status\ncount\n\n\n\n\n0\nDEBTCONSOLIDATION\n0\n644\n\n\n1\nDEBTCONSOLIDATION\n1\n260\n\n\n2\nEDUCATION\n0\n979\n\n\n3\nEDUCATION\n1\n197\n\n\n4\nHOMEIMPROVEMENT\n0\n462\n\n\n5\nHOMEIMPROVEMENT\n1\n154\n\n\n6\nMEDICAL\n0\n768\n\n\n7\nMEDICAL\n1\n305\n\n\n8\nPERSONAL\n0\n778\n\n\n9\nPERSONAL\n1\n220\n\n\n10\nVENTURE\n0\n823\n\n\n11\nVENTURE\n1\n141\n\n\n\n\n\n\n\n\n\ndf_test.groupby(\"loan_intent\")[\"loan_status\"].agg(lambda x: (x == 1).mean() * 100).reset_index()\n\n\n\n\n\n\n\n\n\nloan_intent\nloan_status\n\n\n\n\n0\nDEBTCONSOLIDATION\n28.761062\n\n\n1\nEDUCATION\n16.751701\n\n\n2\nHOMEIMPROVEMENT\n25.000000\n\n\n3\nMEDICAL\n28.424977\n\n\n4\nPERSONAL\n22.044088\n\n\n5\nVENTURE\n14.626556\n\n\n\n\n\n\n\n\nLoans borrowed for debt consolidation and medical purposes in reality have higher probability of defaulting. On the other hand, loans borrowed fro education and venture purposes have lower probability of defaulting. Next, I will use a graph to show whether the income level and the interest rate impact the ease with which an individual can access credit under my decision system.\n\nplt.figure(figsize=(10, 6))\nsns.scatterplot(data=df_test, x='person_income', y='loan_int_rate', hue= 'predicted_loan_status', alpha = 0.3)\nplt.title(\"Interest Rate vs. Income by Prediction on Loan Status\")\nplt.xlabel('Person Income')\nplt.ylabel('Interest Rate')\nplt.legend(title='Default Prediction')\nplt.show()\n\n\n\n\n\n\n\n\n\nHere, we can see that most of the individuals that my model predict will default has lower income. Specifically, individuals who have low income and high interest rate have large probability of defaulting according to my model’s prediction. People with higher income do have more ease when my model is deciding whether to grant them a loan: my model is not predicting any individual will default a loan when its income is higher than 200k.\n\n\nConclusion\nRefering back to my previous blog post, I was using different types of machine learning models to classify penguin species using their physiological features, and one of the models I used is logistic regression model. In this post, instead of letting the model to classify for us automatically, we utilized the weight coefficient vectors, calculate the scores, and assigned a threshold using a relatively simplified profit calculating model to maximize bank’s profit. I learned that the decision-making of a machine learning model is not always motivated by higher accuracy; instead, we can intervene the model and make the model to generate prediction result that would meet our specific needs, such as profit in this case.\nThis also leads to a discussion about the fairness of decision making. From my perspective, fariness decision is made involving a holistic review of the situation, and it should be humane. An arbitrary model may not be fair. Among all these loans, there are some loans that have medical intentions. These loans may be crucial in saving ones life. The decision that the bank make is directly linked to whether this person would have a change to live or sentence to death. In this case, if our model is driven by soely profit, then from the table above there are 16.4% of people who asked for medical loan won’t get the money. In our group discussion during the class, we stated that a fairness decision is made after both rational and emotional evaluation. In this loan-granting case, I believe that there should be more emotional component involved in the loan granting, especially when it it about saving one’s life."
  },
  {
    "objectID": "posts/logisticregression/index.html",
    "href": "posts/logisticregression/index.html",
    "title": "Implementing Logistic Regression",
    "section": "",
    "text": "Implementing Logistic Regression\n\nAbstract\nIn this blog post, I explore the strength and limite of gradient descent algorithm on logistic regression method on empirical risk minimization problems under different data scenarios. First, I implement gradient descent for logistic regression in an object-oriented paradigm. Next, I introduce a momentum term to the gradient descent function and see how it could make the algorithm converge faster compared with the classical gradient descent without the momentum term. Last, I will experiment the model on data with higher dimensions and demonstrate its potential for overfitting. The object-oriented paradigm of the logistic regression could be seen here logistic.py.\n\nimport torch\n%load_ext autoreload\n%autoreload 2\nfrom logistic import LogisticRegression, GradientDescentOptimizer\n\nIntel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\nIntel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n\n\n\n\nExperiments\nFirst, we want to generate data for the classification problem. Here is a function that would help us to generate data of two classes. We can determine the noise of the data, which represents to what extent these two classes of data are mixed together. Here, I am creating a two dimensional experimental data with noise equals to 0.3.\n\ndef classification_data(n_points = 300, noise = 0.2, p_dims = 2):\n    \n    y = torch.arange(n_points) &gt;= int(n_points/2)\n    y = 1.0*y\n    X = y[:, None] + torch.normal(0.0, noise, size = (n_points,p_dims))\n    X = torch.cat((X, torch.ones((X.shape[0], 1))), 1)\n    \n    return X, y\ntorch.manual_seed(123989)\nX, y = classification_data(noise = 0.3)\n\nHere is how the data looks like when displayed on a coordinate system.\n\nfrom matplotlib import pyplot as plt\ndef plot_data(X, y, ax):\n    assert X.shape[1] == 3, \"This function only works for data created with p_dims == 2\"\n    targets = [0, 1]\n    markers = [\"o\" , \",\"]\n    for i in range(2):\n        ix = y == targets[i]\n        ax.scatter(X[ix,0], X[ix,1], s = 20,  c = y[ix], facecolors = \"none\", edgecolors = \"darkgrey\", cmap = \"BrBG\", vmin = -2, vmax = 2, alpha = 0.5, marker = markers[i])\n    ax.set(xlabel = r\"$x_1$\", ylabel = r\"$x_2$\")\n\nfig, ax = plt.subplots(1, 1)\nplot_data(X, y, ax)\nax.set_title(\"Distribution of Experimental Data\")\n\nText(0.5, 1.0, 'Distribution of Experimental Data')\n\n\n\n\n\n\n\n\n\nNow, we want to do the experiment and train our model. First, we initiate the Logistic Regression model and the optimization fnction and implement a gradient descent loop. We want to keep track about the loss value to see how the training process gradually takes us to a decision boundary that best classifies the two groups of data.\n\nExperiment 1: Vanilla gradient descent\nIn the first experiment, we let \\(\\beta = 0\\) so there is no momentum term in the step function (we call this the “Vanilla” case). Set the learning rate to be \\(\\alpha = 0.01\\).\n\n#initiate logistic regression and optimization function\nLR = LogisticRegression() \nopt = GradientDescentOptimizer(LR)\n\n# for keeping track of loss values\nloss_vec = []\n\nalpha = 0.01\n\nfor _ in range(9000):\n    # add other stuff to e.g. keep track of the loss over time.\n    \n    opt.step(X, y, alpha, beta = 0)\n\n    loss = LR.loss(X, y) \n    loss_vec.append(loss)\n\nHere is how the loss fuction has evolved after 1000 iterations.\n\nplt.plot(loss_vec, color = \"slategrey\")\nplt.scatter(torch.arange(len(loss_vec)), loss_vec, color = \"slategrey\")\nlabs = plt.gca().set(xlabel = \"Perceptron Iteration (Updates Only)\", ylabel = \"loss\")\nplt.title(\"Evolution of Loss Function ('Vanilla' Case)\")\n\nText(0.5, 1.0, \"Evolution of Loss Function ('Vanilla' Case)\")\n\n\n\n\n\n\n\n\n\nHere is how the decision boundary looks like after 1000 iterations of training.\n\ndef draw_line(w, x_min, x_max, ax, **kwargs):\n    w_ = w.flatten()\n    x = torch.linspace(x_min, x_max, 101)\n    y = -(w_[0]*x + w_[2])/w_[1]\n    l = ax.plot(x, y, **kwargs)\n\nfig, ax = plt.subplots(1, 1, figsize = (6, 5))\nax.set(xlim = (-2, 3), ylim = (-2, 3))\nplot_data(X, y, ax)\ndraw_line(LR.w, -1, 2, ax, color = \"black\")\nax.set_title(\"Decision Boundary\")\n\nText(0.5, 1.0, 'Decision Boundary')\n\n\n\n\n\n\n\n\n\n\n\nExperiment 2: Benefits of momentum\nNow, we want to increase the paramter \\(\\beta\\) to 0.9 to activate the momentum term. This modification will theoretically let the weight vector \\(\\mathbf{w}\\) to converge faster to the best decision boundary. In other words, compared with the “Vanilla” gradient descent, the momentum descent will take fewer steps to arrive at the same loss function.\nSimilar to the process above, we initiate the logistic regression model and optimizer, as well as creating a loss vector that will keep track with how the weight vector has evolved through iteratios.\n\n# for keeping track of loss values\nloss_vec_mom = []\n\n# set seed\n#torch.manual_seed(1234)\n\n# initialize logistic regression model and optimizer\nLR = LogisticRegression() \nopt = GradientDescentOptimizer(LR)\n\nfor _ in range(9000):\n    opt.step(X, y, alpha, beta = 0.9)\n    \n    loss_1= LR.loss(X, y) \n    loss_vec_mom.append(loss_1)\n\nHere is how the loss function’s evolution in “Vanilla” and “Momentum” cases compare with each other.\n\n# Plot loss_vec\nplt.plot(loss_vec, color=\"slategrey\", label=\"Loss without momentum\")\nplt.scatter(torch.arange(len(loss_vec)), loss_vec, color=\"slategrey\")\n\n# Plot loss_vec_mom\nplt.plot(loss_vec_mom, color=\"orange\", label=\"Loss with momentum\")\nplt.scatter(torch.arange(len(loss_vec_mom)), loss_vec_mom, color=\"orange\")\n\n# Set labels and title\nplt.xlabel(\"Iteration\")\nplt.ylabel(\"Loss\")\nplt.title(\"Evolution of Loss Function - Comparison\")\n\n# Add legend\nplt.legend()\n\n# Show plot\nplt.show()\n\n\n\n\n\n\n\n\nIt is obvious tha the loss function with momentum term decreases much faster than that without the momentum term. The reult resonates with our hypothesis before that the momentum gradient descent will take fewer steps to arrive at the same loss function comparing with the “Vanilla” gradient descent.\n\n\nExperiemnt 3: Overfitting\nTo show the potential danger of overfitting using logistic regression gradient descent on modeling, we generate two data sets that have higher dimension than the number of points p_dim &gt; n_points. In my case, my two data sets both have 20 points, and each data points has a dimension of 30. I call on data set to be my training data X_train and y_train, upon which I will train my model. I expect the logistic regression gradient descent model will give me a 100% training accuracy. The other data set will be my testing data X_test and y_test, upon which I will I apply the 100% accurate model on my trainig model to it.\nAs usual, we generate data, train the model by performing the gradient descent for 100 iterations, and record the loss function vector.\n\ntorch.manual_seed(8999)\n\nn_points = 20\nnoise = 0.6\np_dims = 30\n\nX_train, y_train = classification_data(n_points, noise, p_dims)\nX_test, y_test = classification_data(n_points, noise, p_dims)\n\n# initialize logistic regression model and optimizer\nLR_1 = LogisticRegression() \nopt = GradientDescentOptimizer(LR_1)\n\nloss_vec_ovft = []\n\nfor _ in range(100):\n\n    opt.step(X_train, y_train, alpha = 0.4, beta = 0.9)\n    \n    loss_1 = LR_1.loss(X_train, y_train).item()\n    loss_vec_ovft.append(loss_1)\n\n\nplt.plot(loss_vec_ovft, color = \"slategrey\")\nplt.scatter(torch.arange(len(loss_vec_ovft)), loss_vec_ovft, color = \"slategrey\")\nlabs = plt.gca().set(xlabel = \"Iteration\", ylabel = \"loss\")\nplt.title(\"Evolution of Loss Function - Overfit Test Data\")\n\nText(0.5, 1.0, 'Evolution of Loss Function - Overfit Test Data')\n\n\n\n\n\n\n\n\n\nLet us see how is the accuracy on the training data is alike.\n\npreds = LR_1.predict(X_train)\ncorrect_preds = (preds == y_train).sum()\ntrain_acc = correct_preds / len(y_train)\ntrain_acc.item()\n\n1.0\n\n\nIt is 100% correct! How about we apply this to the testing data?\n\ntest_pred = LR_1.predict(X_test)\ncorrect_test_preds = (test_pred == y_test).sum()\ntest_acc = (correct_test_preds / len(y_test))\ntest_acc.item()\n\n0.8999999761581421\n\n\nThe accuracy decreases to 90%. The difference of accuracy demonstrates that the model trained using the logistic regression gradient descent could be over-trained and thus overfits the training data.\n\n\n\nConclusion\nThrough this blog post, I understood the underlying mechanisms and equations that are used for the logistic regression gradient descent. Different from the perceptron algorithm that adjusts the boundary layer using one or some points in the data, the logistic regression gradient descent adjusts the decision boundary using all data points of the data set. The convexity property of the logistic function is applied so that we know the gradient descent will eventually lead us to a minimum loss value, which means the algorithm with converges to the best decision boundary. We further applied “Spicy Gradient Descent”, which adds a momentum term to the logistic regression function and helps us to get to the minimum loss faster. We also showed the danger of overfitting for logistic regression gradient descent: though we can train the model to find the best separation boundary for the training data, the quality of classification may not apply to the testing data, especially when the dimension of the data is higher."
  },
  {
    "objectID": "posts/ADAM-post/index.html",
    "href": "posts/ADAM-post/index.html",
    "title": "ADAM Algorithm for Optimization",
    "section": "",
    "text": "The Adam Algorithm for Optimization\n\nAbstract\nIn this blog post, I explore ADAM, an optimization algorithm that is the backbone of deep learning. It is a stochastic objective function with first-order gradient-based optimization. It is computationally efficient, has little memory requirements, and is suitable for classification task that involves large number of features and parameters. I will compare ADAM gradient descent with the classic stochastic gradient descent in this blog post.\n\nimport torch\n%load_ext autoreload\n%autoreload 2\nfrom logistic import LogisticRegression, GradientDescentOptimizer\n\nThe autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n\n\n\n\nBasic Experiments\nFirst, we want to generate data for the basic classification problem. Similar to my previous Logistic Regression post, here is a function that would help us to generate data of two classes. We can determine the noise of the data, which represents to what extent these two classes of data are mixed together. Here, I am creating a two dimensional experimental data with noise equals to 0.3.\n\ndef classification_data(n_points = 300, noise = 0.2, p_dims = 2):\n    \n    y = torch.arange(n_points) &gt;= int(n_points/2)\n    y = 1.0*y\n    X = y[:, None] + torch.normal(0.0, noise, size = (n_points,p_dims))\n    X = torch.cat((X, torch.ones((X.shape[0], 1))), 1)\n    \n    return X, y\ntorch.manual_seed(123989)\nX, y = classification_data(noise = 0.3)\n\nHere is how the data looks like when displayed on a coordinate system.\n\nfrom matplotlib import pyplot as plt\ndef plot_data(X, y, ax):\n    assert X.shape[1] == 3, \"This function only works for data created with p_dims == 2\"\n    targets = [0, 1]\n    markers = [\"o\" , \",\"]\n    for i in range(2):\n        ix = y == targets[i]\n        ax.scatter(X[ix,0], X[ix,1], s = 20,  c = y[ix], facecolors = \"none\", edgecolors = \"darkgrey\", cmap = \"BrBG\", vmin = -2, vmax = 2, alpha = 0.5, marker = markers[i])\n    ax.set(xlabel = r\"$x_1$\", ylabel = r\"$x_2$\")\n\nfig, ax = plt.subplots(1, 1)\nplot_data(X, y, ax)\nax.set_title(\"Distribution of Experimental Data\")\n\nText(0.5, 1.0, 'Distribution of Experimental Data')\n\n\n\n\n\n\n\n\n\nNow, we start implementing ADAM. The object-oriented paradigm of the logistic regression could be seen here adam.py. The script corresponds to the comments in Alg. 1 of Kingma and Ba’s (2017) work.\n\nfrom adam import LogisticRegression, AdamOptimizer\n\n#initiate logistic regression and optimization function\nLR = LogisticRegression() \nADAM = AdamOptimizer(LR)\n\n# keep track of loss values\nloss_vec_1 = []\n\n# specify batch size\nbatch_size = 60\n\nfor _ in range(2000):\n    \n    # choose a random batch of indices\n    i = torch.randint(0, X.shape[0], (batch_size,))\n    \n    ADAM.step(X[i], y[i], alpha = 0.006, beta1 = 0.9, beta2 = 0.9)\n\n    loss = LR.loss(X[i], y[i]) \n    loss_vec_1.append(loss)\n    if loss == 0:\n        break\n\n\nfrom logistic import LogisticRegression, GradientDescentOptimizer\nLR_SGD = LogisticRegression()\nopt = GradientDescentOptimizer(LR_SGD)\n\n# keep track of loss values\nloss_vec_2 = []\n\n# specify batch size\nbatch_size = 60\n\nfor _ in range(2000):\n    \n    # choose a random batch of indices\n    i = torch.randint(0, X.shape[0], (batch_size,))\n    \n    opt.step(X[i], y[i], alpha = 0.006, beta = 0.0)\n\n    loss = LR_SGD.loss(X[i], y[i]) \n    loss_vec_2.append(loss)\n\n    if loss == 0:\n        break\n\n\nplt.plot(loss_vec_2, color = \"slategrey\", label = \"SGD\")\nplt.plot(loss_vec_1, color = \"orange\", label = \"ADAM\")\nlabs = plt.gca().set(xlabel = \"Iterations\", ylabel = \"loss\")\nplt.title(\"Evolution of Loss Function (Standard SGD v.s. ADAM)\")\nplt.legend()\n\n\n\n\n\n\n\n\nIt took both ADAM and SGD 0.7 second to finish 2000 iterations. Compare the evolutions of two algorithms’ loss functions, we find that ADAM flattens earlier than SGD. In fact, SGD has not yet converged at its 2000 iteration. The magnitude of oscillation for ADAM is smaller than that of SGD.\n\n\n\nPerform a Digit Experiment\nNext, we are going to compare the performance of Stochastic Gradient Descent and ADAM using the Digit dataset from scikit-learn. First, we select two classes from the dataset and seperate the explanatory and target variables.\n\nfrom sklearn import datasets\nimport numpy as np\nimport torch\n\n# Load the digits dataset\ndigits = datasets.load_digits(n_class=2)\n\n# Extract features (X) and target (y)\nX1 = digits.data.astype(np.double)\ny1 = digits.target.astype(np.double)\n\nX = torch.Tensor(X1)\ny = torch.Tensor(y1)\n\nX = X.double()\ny = y.double()\n\n\nThe dataset has double as datatype, but our logistic.py and adam.py only accept float values. Hence, I create new python scripts logisticDouble.py and adam.py that could accomodate this datatype difference. Now, we implement the both of the alogorith.\n\nfrom adamDouble import LogisticRegression, AdamOptimizer\n\ntorch.manual_seed(997)\n#initiate logistic regression and optimization function for ADAM\nLR = LogisticRegression() \nADAM = AdamOptimizer(LR)\n\n# keep track of loss values for ADAM\nloss_vec_3 = []\n\n# specify batch size\nbatch_size = 30\n\nfor _ in range(800):\n    \n    # choose a random batch of indices\n    i = torch.randint(0, X.shape[0], (batch_size,))\n    \n    g = ADAM.step(X[i], y[i], alpha = 0.01, beta1 = 0.9, beta2 = 0.99)\n    loss_vec_3.append(g)\n\n\n\nfrom logisticDouble import LogisticRegression, GradientDescentOptimizer\n\ntorch.manual_seed(47)\n#initiate logistic regression and optimization function for Stochastic Gradient Descent Logistic Regression\nLR_SGD = LogisticRegression()\nopt = GradientDescentOptimizer(LR_SGD)\n\n# keep track of loss values\nloss_vec_4 = []\n\n# specify batch size\nbatch_size = 30\n\nfor _ in range(800):\n    \n    # choose a random batch of indices\n    i = torch.randint(0, X.shape[0], (batch_size,))\n    \n    opt.step(X[i], y[i], alpha = 0.01)\n\n    loss = LR_SGD.loss(X[i], y[i]) \n    loss_vec_4.append(loss)\n\n\nplt.plot(loss_vec_4, color = \"slategrey\", label = \"SGD\")\nplt.plot(loss_vec_3, color = \"orange\", label = \"ADAM\")\nplt.yscale('log')\n\nplt.xlabel(\"Iteration\")\nplt.ylabel(\"Loss (logarithmic scale)\")\n\nplt.title(\"Evolution of Loss Function (Standard SGD vs ADAM)\")\nplt.legend()\n\n\n\n\n\n\n\n\nFrom the graph above, though ADAM starts from a higher loss, it converges faster than the standard stochastic gradient descent regression in the first 100 iteration and its loss function also flattens at a lower loss value.\n\nExplore my own dataset\nI found a interesting dataset on Kaggle that record the living environment of different types of crops. Here, I will use two types of logistic regression algorithm to classify two types of crops: ‘Rice’ and ‘Lentil’. First, we read the dataset, select rows that are rice and lentil, and then separate the explanatory and target variables.\n\n# Import dataset of crop recommendation\nimport pandas as pd\ncrop = pd.read_csv(\"Crop_Recommendation.csv\")\ncrop = crop[crop['Crop'].isin(['Rice', 'Lentil'])]\n# let 'rice' be 0 and 'Lentil' be 1.\ncrop['Crop'].replace(['Rice', 'Lentil'],[0, 1], inplace=True)\ncrop\n\n\n\n\n\n\n\n\n\nNitrogen\nPhosphorus\nPotassium\nTemperature\nHumidity\npH_Value\nRainfall\nCrop\n\n\n\n\n0\n90\n42\n43\n20.879744\n82.002744\n6.502985\n202.935536\n0\n\n\n1\n85\n58\n41\n21.770462\n80.319644\n7.038096\n226.655537\n0\n\n\n2\n60\n55\n44\n23.004459\n82.320763\n7.840207\n263.964248\n0\n\n\n3\n74\n35\n40\n26.491096\n80.158363\n6.980401\n242.864034\n0\n\n\n4\n78\n42\n42\n20.130175\n81.604873\n7.628473\n262.717340\n0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n895\n26\n56\n22\n23.052764\n60.424786\n7.011121\n52.602853\n1\n\n\n896\n9\n77\n17\n21.658458\n63.583371\n6.280726\n38.076594\n1\n\n\n897\n4\n59\n19\n26.250703\n67.627797\n7.621495\n40.810630\n1\n\n\n898\n34\n73\n15\n20.971953\n63.831799\n7.630424\n53.102079\n1\n\n\n899\n33\n77\n15\n23.897364\n66.321020\n7.802212\n40.745368\n1\n\n\n\n\n200 rows × 8 columns\n\n\n\n\n\nX_crop = torch.tensor(crop.iloc[:, :7].values)\ny_crop = torch.tensor(crop.iloc[:, 7].values)\n\nNow, we implement both algorithms and make graph comparing their performance.\n\nfrom logisticDouble import LogisticRegression, GradientDescentOptimizer\n\ntorch.manual_seed(47)\nLR_SGD = LogisticRegression()\nopt = GradientDescentOptimizer(LR_SGD)\n\n# keep track of loss values\nloss_vec_5 = []\n\n# specify batch size\nbatch_size = 30\n\nfor _ in range(500):\n    # choose a random batch of indices\n    i = torch.randint(0, X_crop.shape[0], (batch_size,))\n    loss = opt.step(X_crop[i], y_crop[i], alpha = 0.001)\n    loss_vec_5.append(loss)\n\n\nfrom adamDouble import LogisticRegression, AdamOptimizer\n\ntorch.manual_seed(47)\nLR = LogisticRegression() \nADAM = AdamOptimizer(LR)\n\n# keep track of loss values\nloss_vec_6 = []\n\n# specify batch size\nbatch_size = 30\n\nfor _ in range(500):\n    \n    # choose a random batch of indices\n    i = torch.randint(0, X_crop.shape[0], (batch_size,))\n    \n    loss = ADAM.step(X_crop[i], y_crop[i], alpha = 0.001, beta1 = 0.9, beta2 = 0.99)\n    loss_vec_6.append(loss)\n\n\n\nplt.plot(loss_vec_6, color = \"slategrey\", label = \"SGD\")\nplt.plot(loss_vec_5, color = \"orange\", label = \"ADAM\")\nplt.yscale('log')\nplt.xlabel(\"Iteration\")\nplt.ylabel(\"Loss (logarithmic scale)\")\n\nplt.title(\"Evolution of Loss Function (Standard SGD vs ADAM)\")\nplt.legend()\n\n\n\n\n\n\n\n\nThe plot shows similar result with the digit dataset. ADAM apparently descents at a higher rate than SGD. However, it appears that ADAM converges at a higher loss value than SGD at its 500 iteration.\n\n\nConclusion\nUsing ADAM helps the logistic regression algorithm to converge more efficiently. Different from previous logistic regression algorithms we have implemented, ADAM updates both exponential moving averages of the gradient \\(m_t\\) and the squared gradient \\(v_t\\). They estimate of the 1st moment (the mean) and the 2nd raw moment (the uncentered variance) of the gradient. Through bias correction mechanism, ADAM’s avoids initialization bias toward zero. Additionally, ADAM chooses its stepsize carefully when updating. Adam adapts the learning rates for each parameter individually based on the estimates of the first and second moments of the gradients. This adaptiveness allows Adam to converge quickly. From my experiment, ADAM does show advantage in converging at a faster speed in terms of iteration. The run time when implementing ADAM is larger than that when implementing SGD, and I suppose it is due to the computation complexity of ADAM when handling two gradient during one update."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "Project Blog Post\n\n\n\n\n\n\ncsci415\n\n\n\nPredicting Population Density with Land Cover\n\n\n\n\n\nMay 17, 2024\n\n\nManuel Fors, Liam Smith, Yide (Alex) Xu\n\n\n\n\n\n\n\n\n\n\n\n\nDeep Music Genre Classification\n\n\n\n\n\nIn this blog post, I constructed three neural networks and used them to classify music genres. \n\n\n\n\n\nMay 16, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nADAM Algorithm for Optimization\n\n\n\n\n\nIn this blog post, I implement ADAM algorithm on logistic regression. \n\n\n\n\n\nMay 12, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nImplementing Logistic Regression\n\n\n\n\n\nIn this blog post, I implement several optimization algorithms based on the gradients of functions. \n\n\n\n\n\nApr 29, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nPerceptron Algorithm\n\n\n\n\n\nMy third post of ML 0451\n\n\n\n\n\nApr 10, 2024\n\n\nYide (Alex) Xu\n\n\n\n\n\n\n\n\n\n\n\n\nClassifying Penguin Species\n\n\n\n\n\nThe first post for CSCI 0451 Machine Learning\n\n\n\n\n\nFeb 19, 2024\n\n\nYide (Alex) Xu\n\n\n\n\n\n\n\n\n\n\n\n\n‘Optimal’ Decision-Making\n\n\n\n\n\nThe first post for CSCI 0451 Machine Learning\n\n\n\n\n\nFeb 19, 2024\n\n\nYide (Alex) Xu\n\n\n\n\n\n\nNo matching items"
  }
]