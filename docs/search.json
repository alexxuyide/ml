[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I am Yide (Alex) Xu, a Geography and Mathematics double major from Middlebury College. On this webpage I post most of the classwork for CSCI 0451 Machine Learning and some machine learning works."
  },
  {
    "objectID": "posts/new-new-test-post/index.html",
    "href": "posts/new-new-test-post/index.html",
    "title": "Timnit Gebru",
    "section": "",
    "text": "from source import Perceptron\np = Perceptron()\n\nI did it!!\nnot implemented\nThis is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/new-new-test-post/index.html#math",
    "href": "posts/new-new-test-post/index.html#math",
    "title": "Timnit Gebru",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "posts/logisticregression/index.html",
    "href": "posts/logisticregression/index.html",
    "title": "Implementing Logistic Regression",
    "section": "",
    "text": "Implementing Logistic Regression\n\nAbstract\nIn this blog post, I explore the strength and limite of gradient descent algorithm on logistic regression method on empirical risk minimization problems under different data scenarios. First, I implement gradient descent for logistic regression in an object-oriented paradigm. Next, I introduce a momentum term to the gradient descent function and see how it could make the algorithm converge faster compared with the classical gradient descent without the momentum term. Last, I will experiment the model on data with higher dimensions and demonstrate its potential for overfitting. The object-oriented paradigm of the logistic regression could be seen here logistic.py.\n\nimport torch\n%load_ext autoreload\n%autoreload 2\nfrom logistic import LogisticRegression, GradientDescentOptimizer\n\nIntel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\nIntel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n\n\n\n\nPart A: Implementing Logistic Regression\n\n\nImplement LinearModel and LogisticRegression()\nIn my object-oriented paradigm of the logistic regression, we will create a new class called LogisticRegression which inherits from LinearModel. This class should have two methods:\n\nLogisticRegression.loss(X, y) computes the empirical risk \\(L(\\mathbf{w})\\) using the logistic loss function \\[\n\\begin{aligned}\n  L(\\mathbf{w}) = \\frac{1}{n} \\sum_{i = 1}^n \\left[-y_i \\log \\sigma(s_i) - (1-y_i)\\log (1-\\sigma(s_i))\\right]\n\\end{aligned}\n\\] The weight vector \\(\\mathbf{w}\\) used for this calculation should be stored as an instance variable of the class.\nLogisticRegression.grad(X, y) should compute the gradient of the empirical risk \\(L(\\mathbf{w})\\). I use the formula provided by Professor Chdrow for the gradient supplied in the lecture notes on gradient descent.\n\n\n\nImplement GradientDescentOptimizer.step()\nNext, we will implement a GradientDescentOptimizer class. For this project, we are going to implement gradient descent with momentum, also known as Spicy Gradient Descent. Let \\(\\mathbf{w}_k\\) be the estimate of the weight vector at algorithmic step \\(k\\)  \\[\n\\begin{aligned}\n    \\mathbf{w}_{k+1} \\gets \\mathbf{w}_k - \\alpha \\nabla L(\\mathbf{w}_k) + \\beta(\\mathbf{w}_k - \\mathbf{w}_{k-1})\n\\end{aligned}\n\\tag{1}\\]This description of gradient descent with momentum is based on @hardtPatternsPredictionsActions2022.\nHere, \\(\\alpha\\) and \\(\\beta\\) are two learning rate parameters. When \\(\\beta = 0\\) we have “regular” gradient descent. In practice, a choice of \\(\\beta \\approx 0.9\\) or so is common.\n\n\nExperiments\nFirst, we want to generate data for the classification problem. Here is a function that would help us to generate data of two classes. We can determine the noise of the data, which represents to what extent these two classes of data are mixed together. Here, I am creating a two dimensional experimental data with noise equals to 0.3.\n\ndef classification_data(n_points = 300, noise = 0.2, p_dims = 2):\n    \n    y = torch.arange(n_points) &gt;= int(n_points/2)\n    y = 1.0*y\n    X = y[:, None] + torch.normal(0.0, noise, size = (n_points,p_dims))\n    X = torch.cat((X, torch.ones((X.shape[0], 1))), 1)\n    \n    return X, y\ntorch.manual_seed(123989)\nX, y = classification_data(noise = 0.3)\n\nHere is how the data looks like when displayed on a coordinate system.\n\nfrom matplotlib import pyplot as plt\ndef plot_data(X, y, ax):\n    assert X.shape[1] == 3, \"This function only works for data created with p_dims == 2\"\n    targets = [0, 1]\n    markers = [\"o\" , \",\"]\n    for i in range(2):\n        ix = y == targets[i]\n        ax.scatter(X[ix,0], X[ix,1], s = 20,  c = y[ix], facecolors = \"none\", edgecolors = \"darkgrey\", cmap = \"BrBG\", vmin = -2, vmax = 2, alpha = 0.5, marker = markers[i])\n    ax.set(xlabel = r\"$x_1$\", ylabel = r\"$x_2$\")\n\nfig, ax = plt.subplots(1, 1)\nplot_data(X, y, ax)\nax.set_title(\"Distribution of Experimental Data\")\n\nText(0.5, 1.0, 'Distribution of Experimental Data')\n\n\n\n\n\n\n\n\n\nNow, we want to do the experiment and train our model. First, we initiate the Logistic Regression model and the optimization fnction and implement a gradient descent loop. We want to keep track about the loss value to see how the training process gradually takes us to a decision boundary that best classifies the two groups of data.\n\nExperiment 1: Vanilla gradient descent\nIn the first experiment, we let \\(\\beta = 0\\) so there is no momentum term in the step function (we call this the “Vanilla” case). Set the learning rate to be \\(\\alpha = 0.01\\).\n\n#initiate logistic regression and optimization function\nLR = LogisticRegression() \nopt = GradientDescentOptimizer(LR)\n\n# for keeping track of loss values\nloss_vec = []\n\nalpha = 0.01\n\nfor _ in range(1000):\n    # add other stuff to e.g. keep track of the loss over time.\n    \n    opt.step(X, y, alpha, beta = 0)\n\n    loss = LR.loss(X, y) \n    loss_vec.append(loss)\n\nHere is how the loss fuction has evolved after 1000 iterations.\n\nplt.plot(loss_vec, color = \"slategrey\")\nplt.scatter(torch.arange(len(loss_vec)), loss_vec, color = \"slategrey\")\nlabs = plt.gca().set(xlabel = \"Perceptron Iteration (Updates Only)\", ylabel = \"loss\")\nplt.title(\"Evolution of Loss Function ('Vanilla' Case)\")\n\nText(0.5, 1.0, \"Evolution of Loss Function ('Vanilla' Case)\")\n\n\n\n\n\n\n\n\n\nHere is how the decision boundary looks like after 1000 iterations of training.\n\ndef draw_line(w, x_min, x_max, ax, **kwargs):\n    w_ = w.flatten()\n    x = torch.linspace(x_min, x_max, 101)\n    y = -(w_[0]*x + w_[2])/w_[1]\n    l = ax.plot(x, y, **kwargs)\n\nfig, ax = plt.subplots(1, 1, figsize = (6, 5))\nax.set(xlim = (-2, 3), ylim = (-2, 3))\nplot_data(X, y, ax)\ndraw_line(LR.w, -1, 2, ax, color = \"black\")\nax.set_title(\"Decision Boundary\")\n\nText(0.5, 1.0, 'Decision Boundary')\n\n\n\n\n\n\n\n\n\n\n\nExperiment 2: Benefits of momentum\nNow, we want to increase the paramter \\(\\beta\\) to 0.9 to activate the momentum term. This modification will theoretically let the weight vector \\(\\mathbf{w}\\) to converge faster to the best decision boundary. In other words, compared with the “Vanilla” gradient descent, the momentum descent will take fewer steps to arrive at the same loss function.\nSimilar to the process above, we initiate the logistic regression model and optimizer, as well as creating a loss vector that will keep track with how the weight vector has evolved through iteratios.\n\n# for keeping track of loss values\nloss_vec_mom = []\n\n# set seed\n#torch.manual_seed(1234)\n\n# initialize logistic regression model and optimizer\nLR = LogisticRegression() \nopt = GradientDescentOptimizer(LR)\n\nfor _ in range(1000):\n    opt.step(X, y, alpha, beta = 0.9)\n    \n    loss_1= LR.loss(X, y) \n    loss_vec_mom.append(loss_1)\n\nHere is how the loss function’s evolution in “Vanilla” and “Momentum” cases compare with each other.\n\nplt.plot(loss_vec, color = \"slategrey\")\nplt.scatter(torch.arange(len(loss_vec)), loss_vec, color = \"slategrey\")\n\nplt.plot(loss_vec_mom, color = \"orange\")\nplt.scatter(torch.arange(len(loss_vec_mom)), loss_vec_mom, color = \"orange\")\n\nlabs = plt.gca().set(xlabel = \"Iteration\", ylabel = \"loss\")\nplt.title(\"Evolution of Loss Function - Comparison\")\n\nText(0.5, 1.0, 'Evolution of Loss Function - Comparison')\n\n\n\n\n\n\n\n\n\nIt is obvious tha the loss function with momentum term decreases much faster than that without the momentum term. The reult resonates with our hypothesis before that the momentum gradient descent will take fewer steps to arrive at the same loss function comparing with the “Vanilla” gradient descent.\n\n\nExperiemnt 3: Overfitting\nTo show the potential danger of overfitting using logistic regression gradient descent on modeling, we generate two data sets that have higher dimension than the number of points p_dim &gt; n_points. In my case, my two data sets both have 20 points, and each data points has a dimension of 30. I call on data set to be my training data X_train and y_train, upon which I will train my model. I expect the logistic regression gradient descent model will give me a 100% training accuracy. The other data set will be my testing data X_test and y_test, upon which I will I apply the 100% accurate model on my trainig model to it.\nAs usual, we generate data, train the model by performing the gradient descent for 100 iterations, and record the loss function vector.\n\ntorch.manual_seed(8999)\n\nn_points = 20\nnoise = 0.6\np_dims = 30\n\nX_train, y_train = classification_data(n_points, noise, p_dims)\nX_test, y_test = classification_data(n_points, noise, p_dims)\n\n# initialize logistic regression model and optimizer\nLR_1 = LogisticRegression() \nopt = GradientDescentOptimizer(LR_1)\n\nloss_vec_ovft = []\n\nfor _ in range(100):\n\n    opt.step(X_train, y_train, alpha = 0.4, beta = 0.9)\n    \n    loss_1 = LR_1.loss(X_train, y_train).item()\n    loss_vec_ovft.append(loss_1)\n\n\nplt.plot(loss_vec_ovft, color = \"slategrey\")\nplt.scatter(torch.arange(len(loss_vec_ovft)), loss_vec_ovft, color = \"slategrey\")\nlabs = plt.gca().set(xlabel = \"Iteration\", ylabel = \"loss\")\nplt.title(\"Evolution of Loss Function - Overfit Test Data\")\n\nText(0.5, 1.0, 'Evolution of Loss Function - Overfit Test Data')\n\n\n\n\n\n\n\n\n\nLet us see how is the accuracy on the training data is alike.\n\npreds = LR_1.predict(X_train)\ncorrect_preds = (preds == y_train).sum()\ntrain_acc = correct_preds / len(y_train)\ntrain_acc.item()\n\n1.0\n\n\nIt is 100% correct! How about we apply this to the testing data?\n\ntest_pred = LR_1.predict(X_test)\ncorrect_test_preds = (test_pred == y_test).sum()\ntest_acc = (correct_test_preds / len(y_test))\ntest_acc.item()\n\n0.8999999761581421\n\n\nThe accuracy decreases to 90%. The difference of accuracy demonstrates that the model trained using the logistic regression gradient descent could be over-trained and thus overfits the training data.\n\n\n\nConclusion\nThrough this blog post, I understood the underlying mechanisms and equations that are used for the logistic regression gradient descent. Different from the perceptron algorithm that adjusts the boundary layer using one or some points in the data, the logistic regression gradient descent adjusts the decision boundary using all data points of the data set. The convexity property of the logistic function is applied so that we know the gradient descent will eventually lead us to a minimum loss value, which means the algorithm with converges to the best decision boundary. We further applied “Spicy Gradient Descent”, which adds a momentum term to the logistic regression function and helps us to get to the minimum loss faster. We also showed the danger of overfitting for logistic regression gradient descent: though we can train the model to find the best separation boundary for the training data, the quality of classification may not apply to the testing data, especially when the dimension of the data is higher."
  },
  {
    "objectID": "posts/perceptron/index.html",
    "href": "posts/perceptron/index.html",
    "title": "Perceptron Algorithm",
    "section": "",
    "text": "Implementing the Perceptron Algorithm\n\nAbstract\nIn this post, we are going to implement the perceptron algorithm under different conditions of raw data. The perceptron algorithm is a binary linear classifier that learns to construct a decision boundary that classifies two classes of data points in a feature space. The algorithm works when the two classes of data are linearly separable: the perceptron converges to a line that completely separates the two classes as the error of missclassification converges to zero. In the post we will show that property applies to not only two 2 dimensional linear separable data but also multidimensional linear separable data. However, for data points that are not linearly separable, the perceptron will find it hard to adjust and converge to a line which has zero error in classifying the data. To make sure that the error converges for data that are not linearly separable, we introduce Minibatch Perceptron algorithm so that the algorithm could arrive at a line that separates the two classes with the least error. The more detailed version of the classical and Minibatch perceptron algorithm is presented in these two files ‘perceptron.py’ and ‘MBperceptron.py’.\n\n%load_ext autoreload\n%autoreload 2\nfrom perceptron import Perceptron, PerceptronOptimizer\nfrom MBperceptron import MBPerceptron, MBPerceptronOptimizer\n\nIntel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\nIntel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n\n\n\n\nPart A: Implement Perceptron\nFirst, we want to create some data points of two categories that are linearly separable for the algorithm to run on. Here is the code to generate random data points of two categories that are linearly separable.\n\nimport torch\nfrom matplotlib import pyplot as plt\nplt.style.use('seaborn-v0_8-whitegrid')\n\ntorch.manual_seed(12989)\n\ndef perceptron_data(n_points = 100, noise = 0.23, p_dims = 2):\n    \n    y = torch.arange(n_points) &gt;= int(n_points/2)\n    X = y[:, None] + torch.normal(0.0, noise, size = (n_points,p_dims))\n    X = torch.cat((X, torch.ones((X.shape[0], 1))), 1)\n\n    # convert y from {0, 1} to {-1, 1}\n    y = 2*y - 1\n\n    return X, y\n\nX, y = perceptron_data(n_points = 100, noise = 0.2)\n\ndef plot_perceptron_data(X, y, ax):\n    assert X.shape[1] == 3, \"This function only works for data created with p_dims == 2\"\n    targets = [-1, 1]\n    markers = [\"o\" , \",\"]\n    for i in range(2):\n        ix = y == targets[i]\n        ax.scatter(X[ix,0], X[ix,1], s = 20,  c = y[ix], facecolors = \"none\", edgecolors = \"darkgrey\", cmap = \"BrBG\", vmin = -2, vmax = 2, alpha = 0.5, marker = markers[i])\n    ax.set(xlabel = r\"$x_1$\", ylabel = r\"$x_2$\")\n\nfig, ax = plt.subplots(1, 1)\nX, y = perceptron_data()\nplot_perceptron_data(X, y, ax)\n\n\n\n\n\n\n\n\nThese are \\(n = 100\\) points of data. Each data point \\(i\\) has two features \\(x_{i1}\\), \\(x_{i2}\\), and a target variable \\(y_i\\). Each data points stack onto each other to form these two matrices \\(\\mathbf{X}\\) and \\(\\mathbf{y}\\). \\[\n\\mathbf{X} = \\left[\\begin{matrix} & - & \\mathbf{x}_1 & - \\\\\n& - & \\mathbf{x}_2 & - \\\\\n& \\vdots & \\vdots & \\vdots \\\\\n& - & \\mathbf{x}_{n} & - \\end{matrix}\\right]\n\\] \\[\n\\mathbf{y} = (y_1, \\ldots, y_{n})^T \\in \\{-1,1\\}^{n}\n\\] In this data set, the target variable has components equal to either \\(-1\\) or \\(1\\).\nNow, we want to implement the classical perceptron algorithm. Recall from the lecture note provided by Professor Chodrow that describes the steps of the perceptron algorithm:\n\nThe perceptron algorithm aims to find a good choice of \\(\\mathbf{w}\\) that makes the loss small using the following algorithm: 1. Start with a random \\(\\mathbf{w}^{(0)}\\). 2. “Until we’re done,” in each time-step \\(t\\), - Pick a random data point \\(i \\in \\{1,\\ldots,n\\}\\). - Compute \\(s^{(t)}_i = \\langle \\mathbf{w}^{(t)}, \\mathbf{x}_i \\rangle\\). - If \\(s^{(t)}_i y_i &gt; 0\\), then point \\(i\\) is currently correctly classified – do nothing! - Else, if \\(s^{(t)}_i y_i &lt; 0\\), then perform the update \\[\\mathbf{w}^{(t+1)} = \\mathbf{w}^{(t)} + y_i \\mathbf{x}_i\\;. \\tag{1}\\]\n\nThe steps above are functions Perceptron.grad() and PerceptronOptimizer.step() in the file ‘perceptron.py’. Here, I will walk you through the Perceptron.grad() algorithm through comments by the code.\n\ndef grad(self, X, y):\n    s_i = X@self.w  # matrix calculation of the weight and the data point x to get the score using the torch @ operator\n    y_ = 2 * y - 1  # convert the target feature value from 0 and 1 to -1 and 1. \n    v = s_i*y_      # if all the points are classified correctly\n    return (v &lt; 0).float() * X * y_   # if all of them are correct, then no need to update the weight vector. If not, then we return a grad that will be used to update the weight vector and the decsion boundary\n\nTo verify that our classical perceptron algorithm is working, we want to implement a minimal training loop using the data we have already generated in the beginning of the post.\n\n# instantiate a model and an optimizer\np = Perceptron() \nopt = PerceptronOptimizer(p)\np.loss(X, y)\n\n# define loss variable\nloss = 1.0\n\n# for keeping track of loss values\nloss_vec = []\niter = 0\n\nn = X.size()[0]\n\nwhile loss &gt; 0: # dangerous -- only terminates if data is linearly separable\n    \n    # not part of the update: just for tracking our progress    \n    loss = p.loss(X, y) \n    loss_vec.append(loss)\n    \n    # pick a random data point\n    i = torch.randint(n, size = (1,))\n    x_i = X[[i],:]\n    y_i = y[i]\n    \n    # perform a perceptron update using the random data point\n    opt.step(x_i, y_i)\n    iter += 1\n\nprint(iter)\n\n115\n\n\nThe loop terminates repeating 115 times and therefore the loss converges to 0. My algorithm works! Now let’s move to the experiment part where we use the perceptron algorithm to different types of data points.\n\n\nPart B: Experiments\n\nPart B.1: Linearly Separable 2D data\nWe will use the original data points we used for the minimal training loop above to illustrate the evolution of the loss function and the final separation line that the algorithm converges to.\n\nplt.plot(loss_vec, color = \"slategrey\")\nplt.scatter(torch.arange(len(loss_vec)), loss_vec, color = \"slategrey\")\nlabs = plt.gca().set(xlabel = \"Perceptron Iteration (Updates Only)\", ylabel = \"loss\")\nplt.title(\"Evolution of Loss Function\")\n\nText(0.5, 1.0, 'Evolution of Loss Function')\n\n\n\n\n\n\n\n\n\nAbove is the graph that shows the evolution of the loss function. It is interesting to see that the loss function was very close to 0 between step 7 to step 110 but later iater increases. It demonstrates the randomness of the perceptron algorithm when selecting data points for adjustment. It may take the algorithm a while to find the data point that the algorithm fails to classify correctly.\nNext, we want to see how does the line that separates the two classes apart looks like.\n\ndef draw_line(w, x_min, x_max, ax, **kwargs):\n    w_ = w.flatten()\n    x = torch.linspace(x_min, x_max, 101)\n    y = -(w_[0]*x + w_[2])/w_[1]\n    l = ax.plot(x, y, **kwargs)\n\nfig, ax = plt.subplots(1, 1, figsize = (4, 4))\nax.set(xlim = (-1, 2), ylim = (-1, 2))\nplot_perceptron_data(X, y, ax)\ndraw_line(p.w, -1, 2, ax, color = \"black\")\nplt.title(\"Final Decision Boundary\")\n\nText(0.5, 1.0, 'Final Decision Boundary')\n\n\n\n\n\n\n\n\n\nA line that has perfectly separated the two classes. How have this line changed throughout the training process? The following figure illustrates the perceptron algorithm in action over several iterations. The code is adopted from the lecture note provided by Professor Chodrow.\n\ntorch.manual_seed(127)\n\n# initialize a perceptron \np = Perceptron()\nopt = PerceptronOptimizer(p)\np.loss(X, y)\n\n# set up the figure\nplt.rcParams[\"figure.figsize\"] = (7, 5)\nfig, axarr = plt.subplots(3, 3, sharex = True, sharey = True)\nmarkers = [\"o\", \",\"]\nmarker_map = {-1 : 0, 1 : 1}\n\n# initialize for main loop\ncurrent_ax = 0\nloss = 1\nloss_vec = []\n\nwhile loss &gt; 0:\n\n    i = torch.randint(n, size = (1,))\n    x_i = X[[i],:]\n    y_i = y[i]\n\n    ax = axarr.ravel()[current_ax]\n\n    # save the old value of w for plotting later\n    old_w = torch.clone(p.w)\n\n    # make an optimization step -- this is where the update actually happens\n    # now p.w is the new value \n    local_loss = opt.step(x_i, y_i)\n\n    # if a change was made, plot the old and new decision boundaries\n    # also add the new loss to loss_vec for plotting below\n    if local_loss &gt; 0:\n        plot_perceptron_data(X, y, ax)\n        draw_line(old_w, x_min = -1, x_max = 2, ax = ax, color = \"black\", linestyle = \"dashed\")\n        loss = p.loss(X, y).item()\n        loss_vec.append(loss)\n        draw_line(p.w, x_min = -1, x_max = 2, ax = ax, color = \"black\")\n        ax.scatter(X[i,0],X[i,1], color = \"black\", facecolors = \"none\", edgecolors = \"black\", marker = markers[marker_map[y_i.item()]])\n        # draw_line(w, -10, 10, ax, color = \"black\")\n        ax.set_title(f\"loss = {loss:.3f}\")\n        ax.set(xlim = (-1, 2), ylim = (-1, 2))\n        current_ax += 1\n\nplt.tight_layout()\n\n\n\n\n\n\n\n\n\n\nPart B.2: Non-linearly Separable 2D data\nIt is often rare in reality to have two classes of data that are linearly separable. What will happen if the perceptron data handles a non-linearly separable 2D data? First, we want to create data points that are nonlinearly seperable. To do that, I increase the noise parameter in the function.\n\ndef nonseperable_perceptron_data(n_points=300, noise=0.7, p_dims=2):\n    y = torch.arange(n_points) &gt;= int(n_points / 2)\n    X = y[:, None] + torch.normal(0.0, noise, size=(n_points, p_dims))\n    X = torch.cat((X, torch.ones((X.shape[0], 1))), 1)\n\n    # convert y from {0, 1} to {-1, 1}\n    y = 2 * y - 1\n\n    return X, y\n\ndef plot_perceptron_data(X, y, ax):\n    assert X.shape[1] == 3, \"This function only works for data created with p_dims == 2\"\n    targets = [-1, 1]\n    markers = [\"o\" , \",\"]\n    for i in range(2):\n        ix = y == targets[i]\n        ax.scatter(X[ix,0], X[ix,1], s = 20,  c = y[ix], facecolors = \"none\", edgecolors = \"darkgrey\", cmap = \"BrBG\", vmin = -2, vmax = 2, alpha = 0.5, marker = markers[i])\n    ax.set(xlabel = r\"$x_1$\", ylabel = r\"$x_2$\")\n\nfig, ax = plt.subplots(1, 1)\nX_1, y_1 = nonseperable_perceptron_data()\nplot_perceptron_data(X_1, y_1, ax)\n\n\n\n\n\n\n\n\nThe two classes of data are mixed. Now we want to implement the perceptron algorithm. We know that the loss function will not converge to 0, so we limit the iteration of the loop to 1000 times.\n\n# instantiate a model and an optimizer\np_1 = Perceptron() \nopt = PerceptronOptimizer(p_1)\np_1.loss(X_1, y_1)\n\nloss = 1.0\n\n# for keeping track of loss values\nloss_vec = []\n\nn = X_1.size()[0]\niteration = 0\n\nwhile loss &gt; 0 and iteration &lt; 1000: # dangerous -- only terminates if data is linearly separable\n    \n    # not part of the update: just for tracking our progress    \n    loss = p_1.loss(X_1, y_1) \n    loss_vec.append(loss)\n    \n    # pick a random data point\n    i = torch.randint(n, size = (1,))\n    x_i = X_1[[i],:]\n    y_i = y_1[i]\n    \n    # perform a perceptron update using the random data point\n    opt.step(x_i, y_i)\n\n    iteration += 1\n\n\nplt.plot(loss_vec, color = \"slategrey\")\nplt.scatter(torch.arange(len(loss_vec)), loss_vec, color = \"slategrey\")\nlabs = plt.gca().set(xlabel = \"Perceptron Iteration (Updates Only)\", ylabel = \"loss\")\nplt.title(\"Evolution of Loss Function\")\n\nText(0.5, 1.0, 'Evolution of Loss Function')\n\n\n\n\n\n\n\n\n\nThe loss oscillates throughout the iteration and does not have a pattern of convergence. How does the final separation line looks?\n\nfig, ax = plt.subplots(1, 1, figsize = (4, 4))\nax.set(xlim = (-2, 3), ylim = (-2, 3))\nplot_perceptron_data(X_1, y_1, ax)\ndraw_line(p_1.w, -2, 3, ax, color = \"black\")\nplt.title(\"Final Decision Boundary\")\n\n\n\n\n\n\n\n\n\n\nPart B.2: Linearly Separable 5D data\nIt is easier for us to understand how the algorithm work and visualize its process in a 2 dimensional space. The perceptron algorithm could work in higher dimensions as well! Now, we want to create data points with 5 features and use the graph of the loss function to determine whether the data is linearly separable or not.\n\ntorch.manual_seed(989)\n\ndef d5_perceptron_data(n_points=150, noise=0.2, p_dims=5):\n    y = torch.arange(n_points) &gt;= int(n_points / 2)\n    X = y[:, None] + torch.normal(0.0, noise, size=(n_points, p_dims))\n    X = torch.cat((X, torch.ones((X.shape[0], 1))), 1)\n\n    # convert y from {0, 1} to {-1, 1}\n    y = 2 * y - 1\n\n    return X, y\n\nX_5, y_5 = d5_perceptron_data()\n\n\n# instantiate a model and an optimizer\np_5 = Perceptron() \nopt = PerceptronOptimizer(p_5)\np_5.loss(X_5, y_5)\n\nloss = 1.0\n\n# for keeping track of loss values\nloss_vec = []\n\nn = X_5.size()[0]\niteration = 0\n\nwhile loss &gt; 0 and iteration &lt; 1000: # dangerous -- only terminates if data is linearly separable\n    \n    # not part of the update: just for tracking our progress    \n    loss = p_5.loss(X_5, y_5) \n    loss_vec.append(loss)\n    \n    # pick a random data point\n    i = torch.randint(n, size = (1,))\n    x_i = X_5[[i],:]\n    y_i = y_5[i]\n    \n    # perform a perceptron update using the random data point\n    opt.step(x_i, y_i)\n\n    iteration += 1\n\nplt.plot(loss_vec, color = \"slategrey\")\nplt.scatter(torch.arange(len(loss_vec)), loss_vec, color = \"slategrey\")\nlabs = plt.gca().set(xlabel = \"Perceptron Iteration (Updates Only)\", ylabel = \"loss\")\nplt.title(\"Evolution of Loss Function\")\n\nText(0.5, 1.0, 'Evolution of Loss Function')\n\n\n\n\n\n\n\n\n\nThe loss function converges after 61 iterations! Since the algorithm terminates and the loss converges to zero, we know that the two classes are linearly separable in this 5 dimensional space.\n\n\n\nPart C: Minibatch Perceptron Experiments\nNow, I am introducing a new perceptron algorithm called Minibatch Perceptron. Different from the classic perceptron algorithm that updates the weight vector and the linear separation line with only one random data point, the Minibatch perceptron algorithm updates them with \\(k\\) random data points. Mathematically, in each step\n\nPick \\(k\\) random indices from \\(i \\in \\{1,\\ldots,n\\}\\).\nPerform the update \\[\\mathbf{w}^{(t+1)} = \\mathbf{w}^{(t)} + \\frac{\\alpha}{k} \\sum_{j=1}^k \\mathbb{1} [\\langle \\mathbf{w}^{(t)}, \\mathbf{x}_{i_j} \\rangle y_{i_j} &lt; 0] y_{i_j} \\mathbf{x}_{i_j} \\]\n\nThe equation above computes \\(k\\) perceptron increments, average them, and apply it to the weight vector. The hyperparameter \\(\\alpha\\) is a learning rate that determines how quickly the weight vector changes in each iteration. The learning rate can be changed to get ideal result.\nThe code for the algorithm is embedded in the file MBperceptron.py. I want to thank Professor Chodrow and Lima Smith for providing me inspiration and help on improving this algorithm. Here, I am creating a function for implementing Minibatch Experiment.\n\ndef MB_experiment(X, y, k, alpha):\n    torch.manual_seed(8972)\n\n    # instantiate a model and an optimizer\n    mb_p = MBPerceptron() \n    mb_opt = MBPerceptronOptimizer(mb_p)\n    mb_p.loss(X, y)\n\n    loss_mb = 1.0\n\n    # for keeping track of loss values\n    loss_vec_mb = []\n    iter = 0\n\n    while loss_mb &gt; 0: \n    \n        # tracking our progress    \n        loss_mb = mb_p.loss(X, y) \n        loss_vec_mb.append(loss_mb)\n\n        # pick several random data points\n        ix = torch.randperm(X.size(0))[:k]\n        X_i = X[ix,:]\n        y_i = y[ix]\n    \n        # perform a perceptron update using the random data point\n        mb_opt.step(X_i, y_i, k, alpha)\n\n        # set iteration limit\n        iter += 1\n        if iter &gt;= 1000:\n            break\n\n    return loss_vec_mb, mb_p.w\n\n\nPart C.1: \\(k = 1\\)\nIn a Minibatch Perceptron, when \\(k= 1\\), it should perform just like the classic perceptron algorithm as it is updated using 1 random points in each iteration\n\nloss_vec_mb_k1, w_k1 = MB_experiment(X, y, k = 1, alpha = 1)\n\n\nplt.plot(loss_vec_mb_k1, color = \"slategrey\")\nplt.scatter(torch.arange(len(loss_vec_mb_k1)), loss_vec_mb_k1, color = \"slategrey\")\nlabs = plt.gca().set(xlabel = \"Minibatch Perceptron Iteration\", ylabel = \"loss\")\nplt.title(\"Evolution of Loss Function\")\n\n\n\n\n\n\n\n\n\nfig, ax = plt.subplots(1, 1, figsize = (4, 4))\nax.set(xlim = (-1, 2), ylim = (-1, 2))\nplot_perceptron_data(X, y, ax)\ndraw_line(w_k1, -1, 2, ax, color = \"black\")\nplt.title(\"Final Decision Boundary\")\n\n\n\n\n\n\n\n\n\n\nPart C.2: \\(k = 10\\)\nNow, when \\(k = 10\\), the algorithm selects randomly 10 points and average their loss to update the linear separation line.\n\nloss_vec_mb_k10, w_k10 = MB_experiment(X, y, k = 10, alpha = 1)\n\n\nplt.plot(loss_vec_mb_k10, color = \"slategrey\")\nplt.scatter(torch.arange(len(loss_vec_mb_k10)), loss_vec_mb_k10, color = \"slategrey\")\nlabs = plt.gca().set(xlabel = \"Minibatch Perceptron Iteration\", ylabel = \"loss\")\nplt.title(\"Evolution of Loss Function\")\n\n\n\n\n\n\n\n\n\nfig, ax = plt.subplots(1, 1, figsize = (4, 4))\nax.set(xlim = (-1, 2), ylim = (-1, 2))\nplot_perceptron_data(X, y, ax)\ndraw_line(w_k10, -1, 2, ax, color = \"black\")\nplt.title(\"Final Decision Boundary\")\n\n\n\n\n\n\n\n\nWhen \\(k = 10\\), the loss function converges faster and has less oscillation. The algorithm is still able to find a linear separation line between the two classes.\n\n\nPart C.3: Non-linearly Separable Data and Minibatch Perceptron\nCompared with the classic perceptron algorithm, minibatch perceptron can converge even when the data is not linearly separable. Though it is impossible for the loss to converge to 0 (perfectly classifying the data) since the data points are not linearly separable, it can help us to find a line that can classify the data points with the highest accuracy. In this part, we set \\(k = n\\), where \\(n\\) is the number of total data points, and tune the learning rate \\(\\alpha\\). First, let \\(\\alpha = 1\\)\n\nloss_vec_mb_kn, w_kn = MB_experiment(X_1, y_1, k = X.size()[0], alpha = 1)\n\n\nplt.plot(loss_vec_mb_kn, color = \"slategrey\")\nplt.scatter(torch.arange(len(loss_vec_mb_kn)), loss_vec_mb_kn, color = \"slategrey\")\nlabs = plt.gca().set(xlabel = \"Minibatch Perceptron Iteration\", ylabel = \"loss\")\nplt.title(\"Evolution of Loss Function\")\n\n\n\n\n\n\n\n\nWe see that the loss function is still oscillating (just like how it was before) and does not seem to converge within 1000 iterations. Now, we rigorously decrease \\(\\alpha\\) and let \\(\\alpha = 0.001\\).\n\nloss_vec_mb_kn_0_001, w_kn_0_001 = MB_experiment(X_1, y_1, k = X.size()[0], alpha = 0.001)\nplt.plot(loss_vec_mb_kn_0_001, color = \"slategrey\")\nplt.scatter(torch.arange(len(loss_vec_mb_kn_0_001)), loss_vec_mb_kn_0_001, color = \"slategrey\")\nlabs = plt.gca().set(xlabel = \"Minibatch Perceptron Iteration\", ylabel = \"loss\")\nplt.title(\"Evolution of Loss Function\")\n\n\n\n\n\n\n\n\nIn this graph, the loss function seems to decrease gradually as we iterate the algorithm. After 1000 iterations, the loss decreases to 0.4, which is the best result among all iterations. Let’s see how the decision boundary looks like.\n\nfig, ax = plt.subplots(1, 1, figsize = (4, 4))\nax.set(xlim = (-1, 2), ylim = (-1, 2))\nplot_perceptron_data(X_1, y_1, ax)\ndraw_line(w_kn_0_001, -1, 2, ax, color = \"black\")\nplt.title(\"Final Decision Boundary\")\n\n\n\n\n\n\n\n\n\n\n\nPart D: Conclusion\nIn this blog post, we practiced writing down the gradient and step functions for the perceptron algorithm and implementing them in various data point scenarios. We experimented with the algorithm using 2D data that is linearly separable, non-linearly separable, and data points with more than 5 features. We found that when the data points are linearly separable, regardless of the number of dimensions, the loss function will converge to 0, and we will obtain a decision boundary that perfectly classifies the two classes. However, when the data is not linearly separable, the algorithm fails to converge to a decision boundary. To improve the algorithm’s performance on non-linearly separable data points, we introduced the Minibatch Perceptron, allowing us to choose the learning rate of the algorithm, \\(\\alpha\\), and the number of random points used when evaluating and updating the decision boundary of the two classes, \\(k\\). When \\(k=1\\), the algorithm behaves similarly to the classic one. For non-linearly separable data, we found that when \\(k = n\\) and a sufficiently small learning rate \\(\\alpha\\) is chosen, the algorithm can converge to a decision boundary that effectively separates the two classes. The blog post improved my understanding of the algorithm and helped me to learn the strength and weakness of the perceptron algorithm."
  },
  {
    "objectID": "posts/penguins/index.html",
    "href": "posts/penguins/index.html",
    "title": "Classifying Penguin Species",
    "section": "",
    "text": "Classifying Penguin Species\n\nIntroduction\nIn this post, we will be using the Palmer Penguins data set collected by Dr. Kristen Gorman and the Palmer Station, Antarctica LTER, a member of the Long Term Ecological Research Network. It was first published by Gorman, Williams, and Fraser (2014) and was nicely packaged and released for use in the data science community by Horst, Hill, and Gorman (2020). The data contains physiological measurements for a number of individuals from each of three species of penguin. The goal of this post is to investigate different machine learning models’ performance to classify penguins’ species based on penguins’ quantitative and qualitative features.\nFirst, we access the training data\n\nimport pandas as pd\nimport numpy as np\nimport warnings\n\nwarnings.filterwarnings(\"ignore\")\n\ntrain_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/train.csv\"\ntrain = pd.read_csv(train_url)\ntrain.shape\n\n(275, 17)\n\n\n\n\nData Preparation\n\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nle.fit(train[\"Species\"])\n\ndef prepare_data(df):\n  df = df.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\", \"Comments\", \"Region\"], axis = 1)\n  df = df[df[\"Sex\"] != \".\"]\n  df = df.dropna()\n  y = le.transform(df[\"Species\"])\n  df = df.drop([\"Species\"], axis = 1)\n  df = pd.get_dummies(df)\n  return df, y\n\nX_train, y_train = prepare_data(train)\n\n\n# train = train.dropna()\ntrain[\"Species\"] = train[\"Species\"].str.split().str.get(0)\ntrain\n\n\n\n\n\n\n\n\n\nstudyName\nSample Number\nSpecies\nRegion\nIsland\nStage\nIndividual ID\nClutch Completion\nDate Egg\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nSex\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nComments\n\n\n\n\n0\nPAL0809\n31\nChinstrap\nAnvers\nDream\nAdult, 1 Egg Stage\nN63A1\nYes\n11/24/08\n40.9\n16.6\n187.0\n3200.0\nFEMALE\n9.08458\n-24.54903\nNaN\n\n\n1\nPAL0809\n41\nChinstrap\nAnvers\nDream\nAdult, 1 Egg Stage\nN74A1\nYes\n11/24/08\n49.0\n19.5\n210.0\n3950.0\nMALE\n9.53262\n-24.66867\nNaN\n\n\n2\nPAL0708\n4\nGentoo\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN32A2\nYes\n11/27/07\n50.0\n15.2\n218.0\n5700.0\nMALE\n8.25540\n-25.40075\nNaN\n\n\n3\nPAL0708\n15\nGentoo\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN38A1\nYes\n12/3/07\n45.8\n14.6\n210.0\n4200.0\nFEMALE\n7.79958\n-25.62618\nNaN\n\n\n4\nPAL0809\n34\nChinstrap\nAnvers\nDream\nAdult, 1 Egg Stage\nN65A2\nYes\n11/24/08\n51.0\n18.8\n203.0\n4100.0\nMALE\n9.23196\n-24.17282\nNaN\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n270\nPAL0910\n100\nGentoo\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN21A2\nYes\n11/18/09\n51.1\n16.5\n225.0\n5250.0\nMALE\n8.20660\n-26.36863\nNaN\n\n\n271\nPAL0809\n69\nAdelie\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN32A1\nNo\n11/11/08\n35.9\n16.6\n190.0\n3050.0\nFEMALE\n8.47781\n-26.07821\nNest never observed with full clutch.\n\n\n272\nPAL0708\n33\nAdelie\nAnvers\nDream\nAdult, 1 Egg Stage\nN22A1\nYes\n11/9/07\n39.5\n17.8\n188.0\n3300.0\nFEMALE\n9.66523\n-25.06020\nNaN\n\n\n273\nPAL0708\n5\nAdelie\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN3A1\nYes\n11/16/07\n36.7\n19.3\n193.0\n3450.0\nFEMALE\n8.76651\n-25.32426\nNaN\n\n\n274\nPAL0708\n21\nChinstrap\nAnvers\nDream\nAdult, 1 Egg Stage\nN73A1\nYes\n12/3/07\n42.4\n17.3\n181.0\n3600.0\nFEMALE\n9.35138\n-24.68790\nNaN\n\n\n\n\n275 rows × 17 columns\n\n\n\n\n\n\nData Exploration and Visualization\n\nQuantitative Feature Exploration\nWe want to understand how different species and sexs of penguins have different physical characteristics. After some primary data exploration of the dataset, we find that there is one incidence of Gentoo penguin whose sex is unknown. I decided to drop that row.\n\ntrain = train[train['Sex'] != \".\"]\n\nDuring the lecture, we have seen the scatterplot between Culmen Depth (mm) and Culmen Length (mm). I want to investigate the other two quantitative features included in the dataset which are Flipper Length (mm) and Body Mass (g). I will find the mean values of the flipper length and body mass for sexes of each penguin species.\n\nmean_flipper = train.groupby([\"Species\", \"Sex\"])[\"Flipper Length (mm)\"].mean().reset_index()\nprint(mean_flipper)\n\n     Species     Sex  Flipper Length (mm)\n0     Adelie  FEMALE           187.924528\n1     Adelie    MALE           192.327869\n2  Chinstrap  FEMALE           192.064516\n3  Chinstrap    MALE           200.692308\n4     Gentoo  FEMALE           212.836735\n5     Gentoo    MALE           221.204545\n\n\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(7, 4))\nsns.barplot(data=mean_flipper, x = \"Species\", y = \"Flipper Length (mm)\", hue=\"Sex\")\nplt.title(\"Mean Filpper Length by Species and Sex\")\n\nText(0.5, 1.0, 'Mean Filpper Length by Species and Sex')\n\n\n\n\n\n\n\n\n\n\nmean_mass = train.groupby([\"Species\", \"Sex\"])[\"Body Mass (g)\"].mean().reset_index()\nprint(mean_mass)\n\n     Species     Sex  Body Mass (g)\n0     Adelie  FEMALE    3350.471698\n1     Adelie    MALE    4052.868852\n2  Chinstrap  FEMALE    3523.387097\n3  Chinstrap    MALE    4005.769231\n4     Gentoo  FEMALE    4684.693878\n5     Gentoo    MALE    5476.704545\n\n\n\nplt.figure(figsize=(7, 4))\nsns.barplot(data=mean_mass, x = \"Species\", y = \"Body Mass (g)\", hue=\"Sex\")\nplt.title(\"Mean Body Mass by Species and Sex\")\n\nText(0.5, 1.0, 'Mean Body Mass by Species and Sex')\n\n\n\n\n\n\n\n\n\nFrom these two barplot, we can see that compare to Flipper Length (mm), the difference of mean Body Mass (g) between species seems to be more significant, if we don’t concern the scales and units of the two measures. Both sexes of Gentoo have obviously larger average weight than the other two penguin species.\nNext, I construct two scatterplots: Flipper Length vs. Culmen Length and Body Mass vs. Culmen Length. It would be ideal to see the dots of each penguin species clustering in a region, and that region does not contain other species.\n\nfig, ax = plt.subplots(1, 2, figsize = (10, 3.5))\n\np1 = sns.scatterplot(train, x = \"Culmen Length (mm)\", y = \"Flipper Length (mm)\", hue = \"Species\", ax = ax[0])\np2 = sns.scatterplot(train, x = \"Culmen Length (mm)\", y = \"Body Mass (g)\", hue = \"Species\", ax = ax[1])\n\n\n\n\n\n\n\n\nFrom the scatterplots above, it seems that both Flipper Length (mm) and Body Mass (g) are good at differentiating Gentoo from the other two species.\n\n\nQualitative Feature Exploration\nHere I want to explore how Island and Clutch Completion could help us in differentiating between penguin species.\n\nisland_species = train.groupby([\"Island\", \"Species\"]).size().reset_index(name='count')\nisland_species\n\n\n\n\n\n\n\n\n\nIsland\nSpecies\ncount\n\n\n\n\n0\nBiscoe\nAdelie\n33\n\n\n1\nBiscoe\nGentoo\n97\n\n\n2\nDream\nAdelie\n45\n\n\n3\nDream\nChinstrap\n57\n\n\n4\nTorgersen\nAdelie\n42\n\n\n\n\n\n\n\n\nThis table shows that Gentoo is only populated in Biscoe Island and Chinstrap is populated in Dream Island. Adelie is populated in the three islands. Therefore, Island is a good qualitative feature to distinguish between Gentoo and Chinstrap Island.\n\nclutch_species = train.groupby([\"Clutch Completion\", \"Species\"]).size().reset_index(name = 'count')\nclutch_species\n\n\n\n\n\n\n\n\n\nClutch Completion\nSpecies\ncount\n\n\n\n\n0\nNo\nAdelie\n12\n\n\n1\nNo\nChinstrap\n10\n\n\n2\nNo\nGentoo\n8\n\n\n3\nYes\nAdelie\n108\n\n\n4\nYes\nChinstrap\n47\n\n\n5\nYes\nGentoo\n89\n\n\n\n\n\n\n\n\nFrom the table above, we cannot see a very clear distinction between penguin species and their clutch completion status. Therefore, we are not using this feature as part of our feature combination.\n\n\n\nChoosing Features by Logistic Regression\n\nfrom itertools import combinations\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_val_score\n\n\nLR = LogisticRegression()\n\n# these are not actually all the columns: you'll \n# need to add any of the other ones you want to search for\nall_qual_cols = [\"Island\"]\nall_quant_cols = ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Flipper Length (mm)', 'Body Mass (g)']\n\nbest_score = 0\nbest_features = []\n\nfor qual in all_qual_cols: \n  qual_cols = [col for col in X_train.columns if qual in col ]\n  for pair in combinations(all_quant_cols, 2):\n    cols = qual_cols + list(pair) \n    LR.fit(X_train[cols], y_train)\n    cv_scores_LR = cross_val_score(LR, X_train[cols], y_train, cv = 5).mean()\n\n    # select the combination that has the best score. \n    if cv_scores_LR &gt; best_score:\n            best_score = cv_scores_LR\n            best_columns = cols\n     \nprint(\"Best columns:\", best_columns)\nprint(\"Best score:\", best_score)\n\nBest columns: ['Island_Biscoe', 'Island_Dream', 'Island_Torgersen', 'Culmen Length (mm)', 'Culmen Depth (mm)']\nBest score: 0.9922322775263952\n\n\nThe best features for the logistic regression model are Island, Culmen Length (mm), and Culmen Depth (mm). It means that only using this combination of features could easily help us differentiate between penguin species.\n\n\nModel Choices\nWe used Logistic Regression to choose the feature combination. We will use that combination for our model selection. First, here is the accuracy of Logistic Regression, which we have shown above.\n\nLR.fit(X_train[best_columns], y_train)\ncv_scores_LR = cross_val_score(LR, X_train[best_columns], y_train, cv = 5).mean()\ncv_scores_LR\n\n0.9922322775263952\n\n\nWe have several other models to choose from. These models are Decision Trees, Random Forest, and SVC. We first import these models from sklearn. Let’s build these models and see their performance on the training data themselves. Similar to the section above, we would employ cross-validation and calculate the mean accuracy to evaluate the performance of the model.\n\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\n\nDecision Tree\n\nDT = DecisionTreeClassifier(max_depth=5, random_state=30)\n\nDT.fit(X_train[best_columns], y_train)\ncv_Scores_DT = cross_val_score(DT, X_train[best_columns], y_train, cv = 5).mean()\ncv_Scores_DT\n\n0.9687028657616892\n\n\nRandom Forest\n\nRF = RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1, random_state=42)\n\nRF.fit(X_train[best_columns], y_train)\ncv_Scores_RF = cross_val_score(RF, X_train[best_columns], y_train, cv = 5).mean()\ncv_Scores_RF\n\n0.9883107088989442\n\n\nSupport Vector Machine\n\ngamma_values = np.logspace(-3, 2, num=6)\nprint(gamma_values)\n\nfor gma in gamma_values:\n    svc = SVC(gamma=gma, C=1, random_state=42)\n    svc.fit(X_train[best_columns], y_train)\n    cv_Scores_SVC = cross_val_score(svc, X_train[best_columns], y_train, cv = 5).mean()\n    print(cv_Scores_SVC)\n\n[1.e-03 1.e-02 1.e-01 1.e+00 1.e+01 1.e+02]\n0.9217948717948717\n0.9648567119155353\n0.9687782805429863\n0.9610105580693815\n0.7578431372549019\n0.4530920060331825\n\n\nI will choose gamma = 0.1 as the parameter for the SVC model.\n\n\nTesting\n\ntest_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/test.csv\"\ntest = pd.read_csv(test_url)\n\nX_test, y_test = prepare_data(test)\n\n\nLR_test_score = LR.score(X_test[best_columns], y_test)\nDT_test_score = DT.score(X_test[best_columns], y_test)\nRF_test_score = RF.score(X_test[best_columns], y_test)\n\nsvc = SVC(gamma=0.1, C=1, random_state=42)\nsvc.fit(X_train[best_columns], y_train)\nSVC_test_score = svc.score(X_test[best_columns], y_test)\n\nprint(\"Logistic Regression Score:\", LR_test_score)\nprint(\"Decision Tree Score:\", DT_test_score)\nprint(\"Random Forest Score:\", RF_test_score)\nprint(\"SVC Score:\", SVC_test_score)\n\nLogistic Regression Score: 1.0\nDecision Tree Score: 0.9852941176470589\nRandom Forest Score: 1.0\nSVC Score: 0.9852941176470589\n\n\nOur testig results show that logiistic regression model and random forest model both give us 100% accuracy for the testing data!\n\n\nPlotting Decision Regions\n\n# Since my best columns' first three elements are qualitative. I modified the function. \n\nfrom matplotlib.patches import Patch\n\ndef plot_regions(model, X, y):\n    \n    x0 = X[X.columns[3]]\n    x1 = X[X.columns[4]]\n    qual_features = X.columns[0:3]\n    \n    fig, axarr = plt.subplots(1, len(qual_features), figsize = (7, 3))\n\n    # create a grid\n    grid_x = np.linspace(x0.min(),x0.max(),501)\n    grid_y = np.linspace(x1.min(),x1.max(),501)\n    xx, yy = np.meshgrid(grid_x, grid_y)\n    \n    XX = xx.ravel()\n    YY = yy.ravel()\n\n    for i in range(len(qual_features)):\n      XY = pd.DataFrame({\n          X.columns[3] : XX,\n          X.columns[4] : YY\n      })\n\n      for j in qual_features:\n        XY[j] = 0\n\n      XY[qual_features[i]] = 1\n\n      XY = XY[X.columns]\n\n      p = model.predict(XY)\n      p = p.reshape(xx.shape)\n      \n      \n      # use contour plot to visualize the predictions\n      axarr[i].contourf(xx, yy, p, cmap = \"jet\", alpha = 0.2, vmin = 0, vmax = 2)\n      \n      ix = X[qual_features[i]] == 1\n      # plot the data\n      axarr[i].scatter(x0[ix], x1[ix], c = y[ix], cmap = \"jet\", vmin = 0, vmax = 2)\n      \n      axarr[i].set(xlabel = X.columns[3], \n            ylabel  = X.columns[4], \n            title = qual_features[i])\n      \n      patches = []\n      for color, spec in zip([\"red\", \"green\", \"blue\"], [\"Adelie\", \"Chinstrap\", \"Gentoo\"]):\n        patches.append(Patch(color = color, label = spec))\n\n      plt.legend(title = \"Species\", handles = patches, loc = \"best\")\n      \n      plt.tight_layout()\n\nHere is the decision regions of Logistic Regression Model\n\nplot_regions(LR, X_train[best_columns], y_train)\n\n\n\n\n\n\n\n\nHere is the decision regions of Decision Tree Model\n\nplot_regions(DT, X_train[best_columns], y_train)\n\n\n\n\n\n\n\n\nHere is the decision regions of Random Forest Model\n\nplot_regions(RF, X_train[best_columns], y_train)\n\n\n\n\n\n\n\n\nHere is the decision regions of SVC Model\n\nplot_regions(svc, X_train[best_columns], y_train)\n\n\n\n\n\n\n\n\nI would like to do a little exploration and see how do the decision regions look like for higher gamma value for the SVC model. Here, I let gamma to be 10.\n\nsvc_10 = SVC(gamma=10, C=1, random_state=42)\nsvc_10.fit(X_train[best_columns], y_train)\nplot_regions(svc_10, X_train[best_columns], y_train)\n\n\n\n\n\n\n\n\nIt is noticeable that the decision regions for Chinstrap and Adelie penguins got more restrictive and small. Only regions that are very close to the training data are included as the decision region for these two species. The boundary of the region got more cursive than it was when \\(\\mathbf{gamma} = 0.1\\)\n\n\nConfusion Matrix\nConfusion Matrix for Logistic Regression\n\nfrom sklearn.metrics import confusion_matrix\n\ny_test_pred_LR = LR.predict(X_test[best_columns])\nLR_C = confusion_matrix(y_test, y_test_pred_LR)\nLR_C\n\narray([[31,  0,  0],\n       [ 0, 11,  0],\n       [ 0,  0, 26]])\n\n\nConfusion Matrix for Decision Trees\n\ny_test_pred_DT = DT.predict(X_test[best_columns])\nDT_C = confusion_matrix(y_test, y_test_pred_DT)\nDT_C\n\narray([[30,  0,  1],\n       [ 0, 11,  0],\n       [ 0,  0, 26]])\n\n\n\nfor i in range(3):\n    for j in range(3):\n        print(f\"There were {DT_C[i,j]} {le.classes_[i]} penguin(s) who were classified as {le.classes_[j]}.\")\n\nThere were 30 Adelie Penguin (Pygoscelis adeliae) penguin(s) who were classified as Adelie Penguin (Pygoscelis adeliae).\nThere were 0 Adelie Penguin (Pygoscelis adeliae) penguin(s) who were classified as Chinstrap penguin (Pygoscelis antarctica).\nThere were 1 Adelie Penguin (Pygoscelis adeliae) penguin(s) who were classified as Gentoo penguin (Pygoscelis papua).\nThere were 0 Chinstrap penguin (Pygoscelis antarctica) penguin(s) who were classified as Adelie Penguin (Pygoscelis adeliae).\nThere were 11 Chinstrap penguin (Pygoscelis antarctica) penguin(s) who were classified as Chinstrap penguin (Pygoscelis antarctica).\nThere were 0 Chinstrap penguin (Pygoscelis antarctica) penguin(s) who were classified as Gentoo penguin (Pygoscelis papua).\nThere were 0 Gentoo penguin (Pygoscelis papua) penguin(s) who were classified as Adelie Penguin (Pygoscelis adeliae).\nThere were 0 Gentoo penguin (Pygoscelis papua) penguin(s) who were classified as Chinstrap penguin (Pygoscelis antarctica).\nThere were 26 Gentoo penguin (Pygoscelis papua) penguin(s) who were classified as Gentoo penguin (Pygoscelis papua).\n\n\nOne Adelie Penguin is misidentified as a Gentoo Penguin by the Decision Trees Model.\nConfusion Matrix for Random Forest\n\ny_test_pred_RF = RF.predict(X_test[best_columns])\nRF_C = confusion_matrix(y_test, y_test_pred_RF)\nRF_C\n\narray([[31,  0,  0],\n       [ 0, 11,  0],\n       [ 0,  0, 26]])\n\n\nConfusion Matrix for SVC\n\ny_test_pred_svc = svc.predict(X_test[best_columns])\nsvc_C = confusion_matrix(y_test, y_test_pred_svc)\nsvc_C\n\narray([[31,  0,  0],\n       [ 0, 11,  0],\n       [ 0,  1, 25]])\n\n\n\nfor i in range(3):\n    for j in range(3):\n        print(f\"There were {svc_C[i,j]} {le.classes_[i]} penguin(s) who were classified as {le.classes_[j]}.\")\n\nThere were 31 Adelie Penguin (Pygoscelis adeliae) penguin(s) who were classified as Adelie Penguin (Pygoscelis adeliae).\nThere were 0 Adelie Penguin (Pygoscelis adeliae) penguin(s) who were classified as Chinstrap penguin (Pygoscelis antarctica).\nThere were 0 Adelie Penguin (Pygoscelis adeliae) penguin(s) who were classified as Gentoo penguin (Pygoscelis papua).\nThere were 0 Chinstrap penguin (Pygoscelis antarctica) penguin(s) who were classified as Adelie Penguin (Pygoscelis adeliae).\nThere were 11 Chinstrap penguin (Pygoscelis antarctica) penguin(s) who were classified as Chinstrap penguin (Pygoscelis antarctica).\nThere were 0 Chinstrap penguin (Pygoscelis antarctica) penguin(s) who were classified as Gentoo penguin (Pygoscelis papua).\nThere were 0 Gentoo penguin (Pygoscelis papua) penguin(s) who were classified as Adelie Penguin (Pygoscelis adeliae).\nThere were 1 Gentoo penguin (Pygoscelis papua) penguin(s) who were classified as Chinstrap penguin (Pygoscelis antarctica).\nThere were 25 Gentoo penguin (Pygoscelis papua) penguin(s) who were classified as Gentoo penguin (Pygoscelis papua).\n\n\nOne Gentoo penguin is misidentified as a Chinstrap penguin by the SVC model.\n\n\nDiscussion\nIn this post, I made some further investigation of the Penguin Dataset and tried out different machine learning models for classification. I selected the three features for classification using some preliminary data exploration and testing iterations of feature combinations on the Logistic Regression model to find the combination that gives me the best accuracy in the model. Then I tested Decision Tree, Random Forest, and Support Vector Machine models using the cross-validation on the training data and plotted their decision regions. At the final testing step, I used all these models and found that Logistic Regression and Random Forest models both give me a 100% accuracy.\nWhen choosing the parameters at the modeling building step, I ran into risk of overfitting my model, even though a large portion of the risk is avoided by cross-validation. For example, when selecting the gamma value, I chose the gamma that gives me the best result in the training data.\nIt is very interesting to see the decision regions of different models. The shapes of the regions reflect how different models work and classify the data. The boundaries of the tree models’ decision regions are parallel to the axis, while the logistic regression and support vector machine has linear and non-linear lines to seperate different decision regions. Also, when \\(\\mathbf{gamma}\\) gets larger, the model becomes more prudent in drawing decision regions for two penguin species, risking of overfitting the model."
  },
  {
    "objectID": "posts/whocost/index.html",
    "href": "posts/whocost/index.html",
    "title": "‘Optimal’ Decision-Making",
    "section": "",
    "text": "‘Optimal’ Decision-Making\n\n2024 Mar 1 Yide (Alex) Xu\nIn this blog post, I extened on what we have learned during the lecture on making (binary) decisions based on a linear score function. During the class, we simplified the process by using only two features and failing to make an attempt to find an optimal vector of weights. In this blog post, I will explore a way to find the optimal weight vector \\(\\mathbf{w}\\) using the logistic regression method and incorporated formulas that calculate the bank’s gain/loss on each loan to find the threshold that would help the bank to make the most profitable decision on loan granting. After testing the model that I build on a testing data set, I found that my model would less likely to grant loans to people within the 20 to 30 year age range and more favorable to people with higher income and lower interest rate.\n\n\nPart A: Grab the Data\nFirst, we want to download the training data.\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\nimport warnings\n\nwarnings.filterwarnings(\"ignore\")\n\n\nurl = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/credit-risk/train.csv\"\ndf_train = pd.read_csv(url)\ndf_train.head()\n\n\n\n\n\n\n\n\n\nperson_age\nperson_income\nperson_home_ownership\nperson_emp_length\nloan_intent\nloan_grade\nloan_amnt\nloan_int_rate\nloan_status\nloan_percent_income\ncb_person_default_on_file\ncb_person_cred_hist_length\n\n\n\n\n0\n25\n43200\nRENT\nNaN\nVENTURE\nB\n1200\n9.91\n0\n0.03\nN\n4\n\n\n1\n27\n98000\nRENT\n3.0\nEDUCATION\nC\n11750\n13.47\n0\n0.12\nY\n6\n\n\n2\n22\n36996\nRENT\n5.0\nEDUCATION\nA\n10000\n7.51\n0\n0.27\nN\n4\n\n\n3\n24\n26000\nRENT\n2.0\nMEDICAL\nC\n1325\n12.87\n1\n0.05\nN\n4\n\n\n4\n29\n53004\nMORTGAGE\n2.0\nHOMEIMPROVEMENT\nA\n15000\n9.63\n0\n0.28\nN\n10\n\n\n\n\n\n\n\n\nThe columns in this data are:\n\nperson_age, the age of the prospective borrower.\nperson_income, the income of the prospective borrower at time of application.\nperson_home_ownership, the home ownership status of the prospective borrower at time of application. Possible values are MORTGAGE, OWN, RENT, and OTHER.\nperson_emp_length, the length of most recent employment for the prospective borrower, in years.\nloan_intent, the purpose of the loan request.\nloan_grade, a composite measure of the likelihood of the borrower to repay the loan.\nloan_amnt, the amount of the loan.\nloan_int_rate, the annual interest rate on the loan.\nloan_status, whether or not the borrower defaulted on the loan. This is the target variable.\nloan_percent_income, the amount of the loan as a percentage of the prospective borrower’s personal income.\ncb_person_default_on_file, whether the prospective borrower has previously defaulted on a loan in the records of a credit bureau.\ncb_person_cred_hist_length, the length of credit history of the prospective borrower.\n\n\n\nPart B: Explore the Data\nFrom this data, I first want to know how does different loan intent vary with age? People within different age range may have different needs for money. I would like to see how the data set says about how different age group would have different purpose in borrowing money from the bank.\nFist, I would classify each individual in the data set into different age range.\n\nage_ranges = [(10, 20), (20, 30), (30, 40), (40, 50), (50, 60), (60, 70), (70, 80)]\n\ndef assign_age_range(age):\n    for lower, upper in age_ranges:\n        if lower &lt;= age &lt; upper:\n            return f\"{lower}-{upper}\"\n\ndf_train['Age_Range'] = df_train['person_age'].apply(assign_age_range)\ndf_train.head()\n\n\n\n\n\n\n\n\n\nperson_age\nperson_income\nperson_home_ownership\nperson_emp_length\nloan_intent\nloan_grade\nloan_amnt\nloan_int_rate\nloan_status\nloan_percent_income\ncb_person_default_on_file\ncb_person_cred_hist_length\nAge_Range\n\n\n\n\n0\n25\n43200\nRENT\nNaN\nVENTURE\nB\n1200\n9.91\n0\n0.03\nN\n4\n20-30\n\n\n1\n27\n98000\nRENT\n3.0\nEDUCATION\nC\n11750\n13.47\n0\n0.12\nY\n6\n20-30\n\n\n2\n22\n36996\nRENT\n5.0\nEDUCATION\nA\n10000\n7.51\n0\n0.27\nN\n4\n20-30\n\n\n3\n24\n26000\nRENT\n2.0\nMEDICAL\nC\n1325\n12.87\n1\n0.05\nN\n4\n20-30\n\n\n4\n29\n53004\nMORTGAGE\n2.0\nHOMEIMPROVEMENT\nA\n15000\n9.63\n0\n0.28\nN\n10\n20-30\n\n\n\n\n\n\n\n\n\nage_intent = df_train.groupby([\"Age_Range\", 'loan_intent']).size().reset_index(name='count')\n\n\nplt.figure(figsize=(10, 6))\nsns.barplot(data=age_intent, x='Age_Range', y='count', hue='loan_intent')\nplt.title('Loan Intent by Age Range')\nplt.xlabel('Age Range')\nplt.ylabel('Count')\nplt.legend(bbox_to_anchor=(0.9, 0.9), loc='upper right')\nplt.show()\n\n\n\n\n\n\n\n\nThe number of individuals within 20 to 30 years old who are seeking loans is significantly higher than all other age groups. There are smaller proportion of this age group population that are seeking home improvement loans comparing to other loan intentions. As the age increases, there are less incident of people looking for loan from the bank.\nNext, I want to see how interest rates are set given people with different income level. Also, how does an individual’s credit history affect the interest rate.\n\ndf_train['person_emp_length'].fillna(0, inplace=True)\ndf_train = df_train[df_train['person_emp_length'] &lt;= 50]\ndf_train = df_train[df_train['person_income'] &lt;= 3000000]\n\nplt.figure(figsize=(10, 6))\nsns.scatterplot(data=df_train, x='person_income', y='loan_int_rate', hue= 'cb_person_default_on_file')\nplt.title(\"Interest Rate vs. Income by Person's Default History\")\nplt.xlabel('Person Income')\nplt.ylabel('Interest Rate')\nplt.legend(title='Whether Defaulted Before')\nplt.show()\n\n\n\n\n\n\n\n\nIt is apparent that people who have defaulted before receive higher interest rates. In fact, almost all of the individuals who have defaulted before will have an interest rate that is higher than 12.5 percent. In terms of interest rates’ relation with income, people with higher income usually receive lower interest rate. Last but not least, I want to see how interest rate is affected by other variables, namely age, home ownership, and loan intention.\n\nplt.figure(figsize=(12, 8))\n\nplt.subplot(411)\nsns.boxplot(data=df_train, x='loan_int_rate', y='Age_Range', color='turquoise', width=0.5)\nplt.title('Distribution of Interest Rates by Age Range')\nplt.ylabel('Age Range')\n\nplt.subplot(412)\nsns.boxplot(data=df_train, y='person_home_ownership', x='loan_int_rate', color='turquoise', width=0.5)\nplt.ylabel('Home Ownership')\n\n\nplt.subplot(413)\nsns.boxplot(data=df_train, y='loan_intent', x='loan_int_rate', color='turquoise', width=0.5)\nplt.ylabel('Loan Intent')\n\nplt.subplot(414)\nsns.boxplot(data=df_train, y='cb_person_default_on_file', x='loan_int_rate', color='turquoise', width=0.5)\nplt.ylabel('Default History')\n\nText(0, 0.5, 'Default History')\n\n\n\n\n\n\n\n\n\nCompared to the interest rate’s difference in the defult history variable, the differences of interest rates within other variables (age, home ownership, and loan intent) are not that significant.\n\n\nBuild a Model\nBefore we dive into building a model, we want to prepare the data for training. We first seperate our target variable from the training data and create dummy variables.\n\nfrom itertools import combinations\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_val_score\n\n\ndef prepare_data(df_train):\n  df_train = df_train.dropna()\n  y = df_train[\"loan_status\"]\n  df = df_train.drop([\"loan_status\"], axis = 1)\n  df = pd.get_dummies(df)\n  return df, y\n\nX_train, y_train = prepare_data(df_train)\nX_train.head(10)\n\n\n\n\n\n\n\n\n\nperson_age\nperson_income\nperson_emp_length\nloan_amnt\nloan_int_rate\nloan_percent_income\ncb_person_cred_hist_length\nperson_home_ownership_MORTGAGE\nperson_home_ownership_OTHER\nperson_home_ownership_OWN\n...\nloan_grade_F\nloan_grade_G\ncb_person_default_on_file_N\ncb_person_default_on_file_Y\nAge_Range_20-30\nAge_Range_30-40\nAge_Range_40-50\nAge_Range_50-60\nAge_Range_60-70\nAge_Range_70-80\n\n\n\n\n0\n25\n43200\n0.0\n1200\n9.91\n0.03\n4\nFalse\nFalse\nFalse\n...\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n1\n27\n98000\n3.0\n11750\n13.47\n0.12\n6\nFalse\nFalse\nFalse\n...\nFalse\nFalse\nFalse\nTrue\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n2\n22\n36996\n5.0\n10000\n7.51\n0.27\n4\nFalse\nFalse\nFalse\n...\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n3\n24\n26000\n2.0\n1325\n12.87\n0.05\n4\nFalse\nFalse\nFalse\n...\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n4\n29\n53004\n2.0\n15000\n9.63\n0.28\n10\nTrue\nFalse\nFalse\n...\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n6\n21\n21700\n2.0\n5500\n14.91\n0.25\n2\nFalse\nFalse\nFalse\n...\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n7\n39\n43000\n3.0\n6250\n7.68\n0.15\n14\nTrue\nFalse\nFalse\n...\nFalse\nFalse\nTrue\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\n\n\n8\n36\n59004\n11.0\n10000\n7.51\n0.17\n15\nTrue\nFalse\nFalse\n...\nFalse\nFalse\nTrue\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\n\n\n9\n22\n34500\n2.0\n5000\n16.89\n0.14\n3\nFalse\nFalse\nFalse\n...\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n10\n22\n33640\n4.0\n12000\n10.65\n0.36\n3\nTrue\nFalse\nFalse\n...\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n\n\n10 rows × 32 columns\n\n\n\n\nI would like to fit a Logistic Regression model as the score-based machine learning model to the data. First, I want to select my features. According to my exploration, I found that people who have defaulted their previous loan(s) are more likely to have higher interest rate, which could lead to higher risk of defaulting on a new loan. Therefore, I include both the default history and the loan interest rate as the features to predict loan defaulting. Next, I include loan_percent_income because it reflects the ability for an individual to pay the interest using its source of income. Lastly, I include the individual’s home ownerhsip status as a factor because unstable shelter status could deprecate one’s ability to pay back the loan.\n\nLR = LogisticRegression()\n\nquant = [\"loan_percent_income\", \"loan_int_rate\"]\nselected_columns = [\"loan_percent_income\", \"loan_int_rate\", \"person_home_ownership_MORTGAGE\", \"person_home_ownership_OTHER\", \"person_home_ownership_OWN\", \"person_home_ownership_RENT\", \"cb_person_default_on_file_N\", \"cb_person_default_on_file_Y\"]\n\nLR.fit(X_train[selected_columns], y_train)\ncv_scores_LR = cross_val_score(LR, X_train[selected_columns], y_train, cv = 5).mean()\n\ncv_scores_LR\n\n0.8436196450239821\n\n\nHere we have the weight vector \\(\\mathbf{w}\\)\n\nLR.coef_[0]\n\narray([ 8.55153096,  0.27487555, -0.04240708,  0.29801174, -1.11860094,\n        0.9429623 , -0.01229506,  0.09226108])\n\n\n\n\nPart D: Find a Threshold\n\ndef score_function(coef, cols, df):\n    scores = np.dot(df[cols].values, coef[0])\n    return scores\n\nX_train['new_score'] = score_function(LR.coef_, selected_columns, X_train)\n\n\nbest_profit = 0\nbest_t = 0\nX_train[\"real_default\"] = y_train\nT = np.linspace(X_train['new_score'].min()-0.1, X_train['new_score'].max()+0.1, 120)\nprofit_frame = pd.DataFrame(columns=['threshold', 'profit'])\n\n\nfor t in np.linspace(X_train['new_score'].min()-0.1, X_train['new_score'].max()+0.1, 120):\n    #y_pred = X_train['new_score'] &gt;= t\n\n    TrueNegative = X_train[(X_train['new_score'] &lt; t) & (X_train['real_default'] == 0)]\n    TrueNegative_list = TrueNegative[\"loan_amnt\"] * (1 + 0.25 * TrueNegative[\"loan_int_rate\"]/100)**10 - TrueNegative[\"loan_amnt\"]\n    TrueNegative_gain = TrueNegative_list.sum()\n\n    FalseNegative = X_train[(X_train['new_score'] &lt; t) & (X_train['real_default'] == 1)]\n    FalseNegative_list = FalseNegative[\"loan_amnt\"] * (1 + 0.25 * FalseNegative[\"loan_int_rate\"]/100)**3 - 1.7 * FalseNegative[\"loan_amnt\"]\n    FalseNegative_cost = FalseNegative_list.sum()\n\n    #TNR = np.zeros(len(np.linspace(0, 12, 1201)))\n    #FNR = np.zeros(len(np.linspace(0, 12, 1201)))\n    profit = TrueNegative_gain + FalseNegative_cost\n    \n    if profit &gt; best_profit:\n            best_t = t\n            best_profit = profit\n    \n    new_row = pd.Series({'threshold': t, 'profit': profit})\n    profit_frame.loc[len(profit_frame)] = new_row\n\nprint(\"Best profit\", best_profit)\nprint(\"is obtained when the threshold is\", best_t)\nprofit_frame\n\nBest profit 32676339.955216546\nis obtained when the threshold is 6.554722944025221\n\n\n\n\n\n\n\n\n\n\nthreshold\nprofit\n\n\n\n\n0\n0.429960\n0.000000e+00\n\n\n1\n0.522760\n0.000000e+00\n\n\n2\n0.615559\n6.050850e+02\n\n\n3\n0.708358\n1.681992e+03\n\n\n4\n0.801158\n5.463633e+03\n\n\n...\n...\n...\n\n\n115\n11.101895\n1.879761e+07\n\n\n116\n11.194695\n1.878627e+07\n\n\n117\n11.287494\n1.878627e+07\n\n\n118\n11.380294\n1.876960e+07\n\n\n119\n11.473093\n1.876960e+07\n\n\n\n\n120 rows × 2 columns\n\n\n\n\n\n\nplt.plot(profit_frame['threshold'], profit_frame['profit'])\n\nplt.xlabel('Threshold')\nplt.ylabel('Profit')\nplt.title('Profit vs. Threshold')\n\nText(0.5, 1.0, 'Profit vs. Threshold')\n\n\n\n\n\n\n\n\n\nFrom this diagram, we can see that the profit peaks when we set the threshold to be around 6. In fact, my code has determined that the when we set the threshold to be 6.55, the best profit can be reached. At \\(t = 6.55\\), the profit per borrower is\n\nbest_profit/len(X_train)\n\n1387.0591712036908\n\n\n\n\nEvaluate Your Model from the Bank’s Perspective\n\nurl = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/credit-risk/test.csv\"\ndf_test = pd.read_csv(url)\ndf_test.dropna(inplace=True)\ndf_test\n\n\n\n\n\n\n\n\n\nperson_age\nperson_income\nperson_home_ownership\nperson_emp_length\nloan_intent\nloan_grade\nloan_amnt\nloan_int_rate\nloan_status\nloan_percent_income\ncb_person_default_on_file\ncb_person_cred_hist_length\n\n\n\n\n0\n21\n42000\nRENT\n5.0\nVENTURE\nD\n1000\n15.58\n1\n0.02\nN\n4\n\n\n1\n32\n51000\nMORTGAGE\n2.0\nDEBTCONSOLIDATION\nB\n15000\n11.36\n0\n0.29\nN\n9\n\n\n2\n35\n54084\nRENT\n2.0\nDEBTCONSOLIDATION\nC\n3000\n12.61\n0\n0.06\nN\n6\n\n\n3\n28\n66300\nMORTGAGE\n11.0\nMEDICAL\nD\n12000\n14.11\n1\n0.15\nN\n6\n\n\n4\n22\n70550\nRENT\n0.0\nMEDICAL\nE\n7000\n15.88\n1\n0.08\nN\n3\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n6511\n29\n78000\nMORTGAGE\n2.0\nPERSONAL\nA\n18000\n6.62\n0\n0.23\nN\n5\n\n\n6513\n27\n44640\nRENT\n0.0\nMEDICAL\nB\n12800\n11.83\n0\n0.29\nN\n9\n\n\n6514\n24\n48000\nOWN\n5.0\nVENTURE\nA\n10400\n7.37\n0\n0.22\nN\n3\n\n\n6515\n26\n65000\nMORTGAGE\n6.0\nEDUCATION\nA\n6000\n9.07\n0\n0.09\nN\n3\n\n\n6516\n29\n61000\nRENT\n12.0\nVENTURE\nD\n10000\n16.07\n0\n0.16\nN\n9\n\n\n\n\n5731 rows × 12 columns\n\n\n\n\n\ndf_test_forModel = pd.get_dummies(df_test)\ndf_test_forModel = df_test_forModel[selected_columns]\n\nt=6.55\n\ndf_test['new_score'] = score_function(LR.coef_, selected_columns, df_test_forModel)\ndf_test['predicted_loan_status'] = df_test['new_score'] &gt;= t\n\n\nTrueNegative = df_test[(df_test['predicted_loan_status'] == False) & (df_test['loan_status'] == 0)]\nTrueNegative_list = TrueNegative[\"loan_amnt\"] * (1 + 0.25 * TrueNegative[\"loan_int_rate\"]/100)**10 - TrueNegative[\"loan_amnt\"]\nTrueNegative_gain = TrueNegative_list.sum()\n\nFalseNegative = df_test[(df_test['predicted_loan_status'] == False) & (df_test['loan_status'] == 1)]\nFalseNegative_list = FalseNegative[\"loan_amnt\"] * (1 + 0.25 * FalseNegative[\"loan_int_rate\"]/100)**3 - 1.7 * FalseNegative[\"loan_amnt\"]\nFalseNegative_cost = FalseNegative_list.sum()\n\n(FalseNegative_cost+TrueNegative_gain)/len(df_test)\n\n1325.9668532238354\n\n\n\n\nEvaluate Your Model From the Borrower’s Perspective\nIs it more difficulty for people in certain age groups to access credit under my proposed system? Let’s see the result.\n\ndf_test['Age_Range'] = df_test['person_age'].apply(assign_age_range)\ndf_test.groupby([\"Age_Range\",\"predicted_loan_status\"]).size().reset_index(name='count')\n\n\n\n\n\n\n\n\n\nAge_Range\npredicted_loan_status\ncount\n\n\n\n\n0\n20-30\nFalse\n3468\n\n\n1\n20-30\nTrue\n673\n\n\n2\n30-40\nFalse\n1107\n\n\n3\n30-40\nTrue\n167\n\n\n4\n40-50\nFalse\n233\n\n\n5\n40-50\nTrue\n27\n\n\n6\n50-60\nFalse\n38\n\n\n7\n50-60\nTrue\n4\n\n\n8\n60-70\nFalse\n6\n\n\n9\n60-70\nTrue\n6\n\n\n10\n70-80\nFalse\n2\n\n\n\n\n\n\n\n\n\ndf_test.groupby(\"Age_Range\")[\"predicted_loan_status\"].agg(lambda x: (x == True).mean() * 100).reset_index() # adopted from ChatGPT\n\n\n\n\n\n\n\n\n\nAge_Range\npredicted_loan_status\n\n\n\n\n0\n20-30\n16.252113\n\n\n1\n30-40\n13.108320\n\n\n2\n40-50\n10.384615\n\n\n3\n50-60\n9.523810\n\n\n4\n60-70\n50.000000\n\n\n5\n70-80\n0.000000\n\n\n\n\n\n\n\n\nAlthough 50 percent of people are predicted to be defaulted for the age 50 to 60 group, the number of sample in this group is very small (only 4 person). Other than the 50 to 60 range, it is more difficult for people with age between 20 to 30 to access their credit. As the age range increases, the probability that my model would grant credit to each individual increases. Now, we want to see how different loan intents may affect how my model would be more likely or less likely to give them grants.\n\ndf_test.groupby([\"loan_intent\",\"predicted_loan_status\"]).size().reset_index(name='count')\n\n\n\n\n\n\n\n\n\nloan_intent\npredicted_loan_status\ncount\n\n\n\n\n0\nDEBTCONSOLIDATION\nFalse\n756\n\n\n1\nDEBTCONSOLIDATION\nTrue\n148\n\n\n2\nEDUCATION\nFalse\n1008\n\n\n3\nEDUCATION\nTrue\n168\n\n\n4\nHOMEIMPROVEMENT\nFalse\n535\n\n\n5\nHOMEIMPROVEMENT\nTrue\n81\n\n\n6\nMEDICAL\nFalse\n897\n\n\n7\nMEDICAL\nTrue\n176\n\n\n8\nPERSONAL\nFalse\n834\n\n\n9\nPERSONAL\nTrue\n164\n\n\n10\nVENTURE\nFalse\n824\n\n\n11\nVENTURE\nTrue\n140\n\n\n\n\n\n\n\n\n\ndf_test.groupby(\"loan_intent\")[\"predicted_loan_status\"].agg(lambda x: (x == True).mean() * 100).reset_index()\n\n\n\n\n\n\n\n\n\nloan_intent\npredicted_loan_status\n\n\n\n\n0\nDEBTCONSOLIDATION\n16.371681\n\n\n1\nEDUCATION\n14.285714\n\n\n2\nHOMEIMPROVEMENT\n13.149351\n\n\n3\nMEDICAL\n16.402610\n\n\n4\nPERSONAL\n16.432866\n\n\n5\nVENTURE\n14.522822\n\n\n\n\n\n\n\n\nAccording to the prediction of my model, the probability for a loan to be defaulted across different loaning intent is similar. Home imporvement loans see a slightly lower probability of defaulting. Next, we want to see what is is the count of default for loans with different intents in reality.\n\ndf_test.groupby([\"loan_intent\",\"loan_status\"]).size().reset_index(name='count')\n\n\n\n\n\n\n\n\n\nloan_intent\nloan_status\ncount\n\n\n\n\n0\nDEBTCONSOLIDATION\n0\n644\n\n\n1\nDEBTCONSOLIDATION\n1\n260\n\n\n2\nEDUCATION\n0\n979\n\n\n3\nEDUCATION\n1\n197\n\n\n4\nHOMEIMPROVEMENT\n0\n462\n\n\n5\nHOMEIMPROVEMENT\n1\n154\n\n\n6\nMEDICAL\n0\n768\n\n\n7\nMEDICAL\n1\n305\n\n\n8\nPERSONAL\n0\n778\n\n\n9\nPERSONAL\n1\n220\n\n\n10\nVENTURE\n0\n823\n\n\n11\nVENTURE\n1\n141\n\n\n\n\n\n\n\n\n\ndf_test.groupby(\"loan_intent\")[\"loan_status\"].agg(lambda x: (x == 1).mean() * 100).reset_index()\n\n\n\n\n\n\n\n\n\nloan_intent\nloan_status\n\n\n\n\n0\nDEBTCONSOLIDATION\n28.761062\n\n\n1\nEDUCATION\n16.751701\n\n\n2\nHOMEIMPROVEMENT\n25.000000\n\n\n3\nMEDICAL\n28.424977\n\n\n4\nPERSONAL\n22.044088\n\n\n5\nVENTURE\n14.626556\n\n\n\n\n\n\n\n\nLoans borrowed for debt consolidation and medical purposes in reality have higher probability of defaulting. On the other hand, loans borrowed fro education and venture purposes have lower probability of defaulting. Next, I will use a graph to show whether the income level and the interest rate impact the ease with which an individual can access credit under my decision system.\n\nplt.figure(figsize=(10, 6))\nsns.scatterplot(data=df_test, x='person_income', y='loan_int_rate', hue= 'predicted_loan_status')\nplt.title(\"Interest Rate vs. Income by Prediction on Loan Status\")\nplt.xlabel('Person Income')\nplt.ylabel('Interest Rate')\nplt.legend(title='Default Prediction')\nplt.show()\n\n\n\n\n\n\n\n\n\nHere, we can see that most of the individuals that my model predict will default has lower income. Specifically, individuals who have low income and high interest rate have large probability of defaulting according to my model’s prediction. People with higher income do have more ease when my model is deciding whether to grant them a loan: my model is not predicting any individual will default a loan when its income is higher than 200k.\n\n\nConclusion\nRefering back to my previous blog post, I was using different types of machine learning models to classify penguin species using their physiological features, and one of the models I used is logistic regression model. In this post, instead of letting the model to classify for us automatically, we utilized the weight coefficient vectors, calculate the scores, and assigned a threshold using a relatively simplified profit calculating model to maximize bank’s profit. I learned that the decision-making of a machine learning model is not always motivated by higher accuracy; instead, we can intervene the model and make the model to generate prediction result that would meet our specific needs, such as profit in this case.\nThis also leads to a discussion about the fairness of decision making. Among all these loans, there are some loans that have medical intentions. These loans may be crucial in saving ones life. The decision that the bank make is directly linked to whether this person would have a change to live or sentence to death. In this case, if our model is driven by soely profit, then from the table above there are 16.4% of people who asked for medical loan won’t get the money. In our group discussion during the class, we stated that a fairness decision is made after both rational and emotional evaluation. In this loan-granting case, I believe that there should be more emotional component involved in the loan granting, especially when it it about saving one’s life."
  },
  {
    "objectID": "posts/example-blog-post/index.html",
    "href": "posts/example-blog-post/index.html",
    "title": "Hello Blog",
    "section": "",
    "text": "from source import Perceptron\nThis is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/example-blog-post/index.html#math",
    "href": "posts/example-blog-post/index.html#math",
    "title": "Hello Blog",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "posts/new-test-post/index.html",
    "href": "posts/new-test-post/index.html",
    "title": "Second Post",
    "section": "",
    "text": "This is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/new-test-post/index.html#math",
    "href": "posts/new-test-post/index.html#math",
    "title": "Second Post",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "Perceptron Algorithm\n\n\n\n\n\nMy third post of ML 0451\n\n\n\n\n\nApr 10, 2024\n\n\nYide (Alex) Xu\n\n\n\n\n\n\n\n\n\n\n\n\nImplementing Logistic Regression\n\n\n\n\n\nIn this blog post, I implement several optimization algorithms based on the gradients of functions. \n\n\n\n\n\nApr 4, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nClassifying Penguin Species\n\n\n\n\n\nThe first post for CSCI 0451 Machine Learning\n\n\n\n\n\nFeb 19, 2024\n\n\nYide (Alex) Xu\n\n\n\n\n\n\n\n\n\n\n\n\n‘Optimal’ Decision-Making\n\n\n\n\n\nThe first post for CSCI 0451 Machine Learning\n\n\n\n\n\nFeb 19, 2024\n\n\nYide (Alex) Xu\n\n\n\n\n\n\n\n\n\n\n\n\nSecond Post\n\n\n\n\n\nA new blog post that I just made!\n\n\n\n\n\nMar 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\n\n\n\n\n\n\nTimnit Gebru\n\n\n\n\n\nA new blog post that I just made!\n\n\n\n\n\nMar 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\n\n\n\n\n\n\nHello Blog\n\n\n\n\n\nAn example blog post illustrating the key techniques you’ll need to demonstrate your learning in CSCI 0451.\n\n\n\n\n\nJan 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\nNo matching items"
  }
]