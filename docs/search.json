[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I am Yide (Alex) Xu, a Geography and Mathematics double major from Middlebury College. On this webpage I post most of the classwork for CSCI 0451 Machine Learning and some machine learning works."
  },
  {
    "objectID": "posts/new-new-test-post/index.html",
    "href": "posts/new-new-test-post/index.html",
    "title": "Timnit Gebru",
    "section": "",
    "text": "from source import Perceptron\np = Perceptron()\n\nI did it!!\nnot implemented\nThis is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/new-new-test-post/index.html#math",
    "href": "posts/new-new-test-post/index.html#math",
    "title": "Timnit Gebru",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "posts/example-blog-post/index.html",
    "href": "posts/example-blog-post/index.html",
    "title": "Hello Blog",
    "section": "",
    "text": "from source import Perceptron\nThis is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/example-blog-post/index.html#math",
    "href": "posts/example-blog-post/index.html#math",
    "title": "Hello Blog",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "posts/penguins/index.html",
    "href": "posts/penguins/index.html",
    "title": "Classifying Penguin Species",
    "section": "",
    "text": "Classifying Penguin Species\n\nIntroduction\nIn this post, we will be using the Palmer Penguins data set collected by Dr. Kristen Gorman and the Palmer Station, Antarctica LTER, a member of the Long Term Ecological Research Network. It was first published by Gorman, Williams, and Fraser (2014) and was nicely packaged and released for use in the data science community by Horst, Hill, and Gorman (2020). The data contains physiological measurements for a number of individuals from each of three species of penguin. The goal of this post is to investigate different machine learning models’ performance to classify penguins’ species based on penguins’ quantitative and qualitative features.\nFirst, we access the training data\n\nimport pandas as pd\nimport numpy as np\nimport warnings\n\nwarnings.filterwarnings(\"ignore\")\n\ntrain_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/train.csv\"\ntrain = pd.read_csv(train_url)\ntrain.shape\n\n(275, 17)\n\n\n\n\nData Preparation\n\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nle.fit(train[\"Species\"])\n\ndef prepare_data(df):\n  df = df.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\", \"Comments\", \"Region\"], axis = 1)\n  df = df[df[\"Sex\"] != \".\"]\n  df = df.dropna()\n  y = le.transform(df[\"Species\"])\n  df = df.drop([\"Species\"], axis = 1)\n  df = pd.get_dummies(df)\n  return df, y\n\nX_train, y_train = prepare_data(train)\n\n\n# train = train.dropna()\ntrain[\"Species\"] = train[\"Species\"].str.split().str.get(0)\ntrain\n\n\n\n\n\n\n\n\n\nstudyName\nSample Number\nSpecies\nRegion\nIsland\nStage\nIndividual ID\nClutch Completion\nDate Egg\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nSex\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nComments\n\n\n\n\n0\nPAL0809\n31\nChinstrap\nAnvers\nDream\nAdult, 1 Egg Stage\nN63A1\nYes\n11/24/08\n40.9\n16.6\n187.0\n3200.0\nFEMALE\n9.08458\n-24.54903\nNaN\n\n\n1\nPAL0809\n41\nChinstrap\nAnvers\nDream\nAdult, 1 Egg Stage\nN74A1\nYes\n11/24/08\n49.0\n19.5\n210.0\n3950.0\nMALE\n9.53262\n-24.66867\nNaN\n\n\n2\nPAL0708\n4\nGentoo\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN32A2\nYes\n11/27/07\n50.0\n15.2\n218.0\n5700.0\nMALE\n8.25540\n-25.40075\nNaN\n\n\n3\nPAL0708\n15\nGentoo\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN38A1\nYes\n12/3/07\n45.8\n14.6\n210.0\n4200.0\nFEMALE\n7.79958\n-25.62618\nNaN\n\n\n4\nPAL0809\n34\nChinstrap\nAnvers\nDream\nAdult, 1 Egg Stage\nN65A2\nYes\n11/24/08\n51.0\n18.8\n203.0\n4100.0\nMALE\n9.23196\n-24.17282\nNaN\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n270\nPAL0910\n100\nGentoo\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN21A2\nYes\n11/18/09\n51.1\n16.5\n225.0\n5250.0\nMALE\n8.20660\n-26.36863\nNaN\n\n\n271\nPAL0809\n69\nAdelie\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN32A1\nNo\n11/11/08\n35.9\n16.6\n190.0\n3050.0\nFEMALE\n8.47781\n-26.07821\nNest never observed with full clutch.\n\n\n272\nPAL0708\n33\nAdelie\nAnvers\nDream\nAdult, 1 Egg Stage\nN22A1\nYes\n11/9/07\n39.5\n17.8\n188.0\n3300.0\nFEMALE\n9.66523\n-25.06020\nNaN\n\n\n273\nPAL0708\n5\nAdelie\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN3A1\nYes\n11/16/07\n36.7\n19.3\n193.0\n3450.0\nFEMALE\n8.76651\n-25.32426\nNaN\n\n\n274\nPAL0708\n21\nChinstrap\nAnvers\nDream\nAdult, 1 Egg Stage\nN73A1\nYes\n12/3/07\n42.4\n17.3\n181.0\n3600.0\nFEMALE\n9.35138\n-24.68790\nNaN\n\n\n\n\n275 rows × 17 columns\n\n\n\n\n\n\nData Exploration and Visualization\n\nQuantitative Feature Exploration\nWe want to understand how different species and sexs of penguins have different physical characteristics. After some primary data exploration of the dataset, we find that there is one incidence of Gentoo penguin whose sex is unknown. I decided to drop that row.\n\ntrain = train[train['Sex'] != \".\"]\n\nDuring the lecture, we have seen the scatterplot between Culmen Depth (mm) and Culmen Length (mm). I want to investigate the other two quantitative features included in the dataset which are Flipper Length (mm) and Body Mass (g). I will find the mean values of the flipper length and body mass for sexes of each penguin species.\n\nmean_flipper = train.groupby([\"Species\", \"Sex\"])[\"Flipper Length (mm)\"].mean().reset_index()\nprint(mean_flipper)\n\n     Species     Sex  Flipper Length (mm)\n0     Adelie  FEMALE           187.924528\n1     Adelie    MALE           192.327869\n2  Chinstrap  FEMALE           192.064516\n3  Chinstrap    MALE           200.692308\n4     Gentoo  FEMALE           212.836735\n5     Gentoo    MALE           221.204545\n\n\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(7, 4))\nsns.barplot(data=mean_flipper, x = \"Species\", y = \"Flipper Length (mm)\", hue=\"Sex\")\nplt.title(\"Mean Filpper Length by Species and Sex\")\n\nText(0.5, 1.0, 'Mean Filpper Length by Species and Sex')\n\n\n\n\n\n\n\n\n\n\nmean_mass = train.groupby([\"Species\", \"Sex\"])[\"Body Mass (g)\"].mean().reset_index()\nprint(mean_mass)\n\n     Species     Sex  Body Mass (g)\n0     Adelie  FEMALE    3350.471698\n1     Adelie    MALE    4052.868852\n2  Chinstrap  FEMALE    3523.387097\n3  Chinstrap    MALE    4005.769231\n4     Gentoo  FEMALE    4684.693878\n5     Gentoo    MALE    5476.704545\n\n\n\nplt.figure(figsize=(7, 4))\nsns.barplot(data=mean_mass, x = \"Species\", y = \"Body Mass (g)\", hue=\"Sex\")\nplt.title(\"Mean Body Mass by Species and Sex\")\n\nText(0.5, 1.0, 'Mean Body Mass by Species and Sex')\n\n\n\n\n\n\n\n\n\nFrom these two barplot, we can see that compare to Flipper Length (mm), the difference of mean Body Mass (g) between species seems to be more significant, if we don’t concern the scales and units of the two measures. Both sexes of Gentoo have obviously larger average weight than the other two penguin species.\nNext, I construct two scatterplots: Flipper Length vs. Culmen Length and Body Mass vs. Culmen Length. It would be ideal to see the dots of each penguin species clustering in a region, and that region does not contain other species.\n\nfig, ax = plt.subplots(1, 2, figsize = (10, 3.5))\n\np1 = sns.scatterplot(train, x = \"Culmen Length (mm)\", y = \"Flipper Length (mm)\", hue = \"Species\", ax = ax[0])\np2 = sns.scatterplot(train, x = \"Culmen Length (mm)\", y = \"Body Mass (g)\", hue = \"Species\", ax = ax[1])\n\n\n\n\n\n\n\n\nFrom the scatterplots above, it seems that both Flipper Length (mm) and Body Mass (g) are good at differentiating Gentoo from the other two species.\n\n\nQualitative Feature Exploration\nHere I want to explore how Island and Clutch Completion could help us in differentiating between penguin species.\n\nisland_species = train.groupby([\"Island\", \"Species\"]).size().reset_index(name='count')\nisland_species\n\n\n\n\n\n\n\n\n\nIsland\nSpecies\ncount\n\n\n\n\n0\nBiscoe\nAdelie\n33\n\n\n1\nBiscoe\nGentoo\n97\n\n\n2\nDream\nAdelie\n45\n\n\n3\nDream\nChinstrap\n57\n\n\n4\nTorgersen\nAdelie\n42\n\n\n\n\n\n\n\n\nThis table shows that Gentoo is only populated in Biscoe Island and Chinstrap is populated in Dream Island. Adelie is populated in the three islands. Therefore, Island is a good qualitative feature to distinguish between Gentoo and Chinstrap Island.\n\nclutch_species = train.groupby([\"Clutch Completion\", \"Species\"]).size().reset_index(name = 'count')\nclutch_species\n\n\n\n\n\n\n\n\n\nClutch Completion\nSpecies\ncount\n\n\n\n\n0\nNo\nAdelie\n12\n\n\n1\nNo\nChinstrap\n10\n\n\n2\nNo\nGentoo\n8\n\n\n3\nYes\nAdelie\n108\n\n\n4\nYes\nChinstrap\n47\n\n\n5\nYes\nGentoo\n89\n\n\n\n\n\n\n\n\nFrom the table above, we cannot see a very clear distinction between penguin species and their clutch completion status. Therefore, we are not using this feature as part of our feature combination.\n\n\n\nChoosing Features by Logistic Regression\n\nfrom itertools import combinations\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_val_score\n\n\nLR = LogisticRegression()\n\n# these are not actually all the columns: you'll \n# need to add any of the other ones you want to search for\nall_qual_cols = [\"Island\"]\nall_quant_cols = ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Flipper Length (mm)', 'Body Mass (g)']\n\nbest_score = 0\nbest_features = []\n\nfor qual in all_qual_cols: \n  qual_cols = [col for col in X_train.columns if qual in col ]\n  for pair in combinations(all_quant_cols, 2):\n    cols = qual_cols + list(pair) \n    LR.fit(X_train[cols], y_train)\n    cv_scores_LR = cross_val_score(LR, X_train[cols], y_train, cv = 5).mean()\n\n    # select the combination that has the best score. \n    if cv_scores_LR &gt; best_score:\n            best_score = cv_scores_LR\n            best_columns = cols\n     \nprint(\"Best columns:\", best_columns)\nprint(\"Best score:\", best_score)\n\nBest columns: ['Island_Biscoe', 'Island_Dream', 'Island_Torgersen', 'Culmen Length (mm)', 'Culmen Depth (mm)']\nBest score: 0.9922322775263952\n\n\nThe best features for the logistic regression model are Island, Culmen Length (mm), and Culmen Depth (mm). It means that only using this combination of features could easily help us differentiate between penguin species.\n\n\nModel Choices\nWe used Logistic Regression to choose the feature combination. We will use that combination for our model selection. First, here is the accuracy of Logistic Regression, which we have shown above.\n\nLR.fit(X_train[best_columns], y_train)\ncv_scores_LR = cross_val_score(LR, X_train[best_columns], y_train, cv = 5).mean()\ncv_scores_LR\n\n0.9922322775263952\n\n\nWe have several other models to choose from. These models are Decision Trees, Random Forest, and SVC. We first import these models from sklearn. Let’s build these models and see their performance on the training data themselves. Similar to the section above, we would employ cross-validation and calculate the mean accuracy to evaluate the performance of the model.\n\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\n\nDecision Tree\n\nDT = DecisionTreeClassifier(max_depth=5, random_state=30)\n\nDT.fit(X_train[best_columns], y_train)\ncv_Scores_DT = cross_val_score(DT, X_train[best_columns], y_train, cv = 5).mean()\ncv_Scores_DT\n\n0.9687028657616892\n\n\nRandom Forest\n\nRF = RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1, random_state=42)\n\nRF.fit(X_train[best_columns], y_train)\ncv_Scores_RF = cross_val_score(RF, X_train[best_columns], y_train, cv = 5).mean()\ncv_Scores_RF\n\n0.9883107088989442\n\n\nSupport Vector Machine\n\ngamma_values = np.logspace(-3, 2, num=6)\nprint(gamma_values)\n\nfor gma in gamma_values:\n    svc = SVC(gamma=gma, C=1, random_state=42)\n    svc.fit(X_train[best_columns], y_train)\n    cv_Scores_SVC = cross_val_score(svc, X_train[best_columns], y_train, cv = 5).mean()\n    print(cv_Scores_SVC)\n\n[1.e-03 1.e-02 1.e-01 1.e+00 1.e+01 1.e+02]\n0.9217948717948717\n0.9648567119155353\n0.9687782805429863\n0.9610105580693815\n0.7578431372549019\n0.4530920060331825\n\n\nI will choose gamma = 0.1 as the parameter for the SVC model.\n\n\nTesting\n\ntest_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/test.csv\"\ntest = pd.read_csv(test_url)\n\nX_test, y_test = prepare_data(test)\n\n\nLR_test_score = LR.score(X_test[best_columns], y_test)\nDT_test_score = DT.score(X_test[best_columns], y_test)\nRF_test_score = RF.score(X_test[best_columns], y_test)\n\nsvc = SVC(gamma=0.1, C=1, random_state=42)\nsvc.fit(X_train[best_columns], y_train)\nSVC_test_score = svc.score(X_test[best_columns], y_test)\n\nprint(\"Logistic Regression Score:\", LR_test_score)\nprint(\"Decision Tree Score:\", DT_test_score)\nprint(\"Random Forest Score:\", RF_test_score)\nprint(\"SVC Score:\", SVC_test_score)\n\nLogistic Regression Score: 1.0\nDecision Tree Score: 0.9852941176470589\nRandom Forest Score: 1.0\nSVC Score: 0.9852941176470589\n\n\nOur testig results show that logiistic regression model and random forest model both give us 100% accuracy for the testing data!\n\n\nPlotting Decision Regions\n\n# Since my best columns' first three elements are qualitative. I modified the function. \n\nfrom matplotlib.patches import Patch\n\ndef plot_regions(model, X, y):\n    \n    x0 = X[X.columns[3]]\n    x1 = X[X.columns[4]]\n    qual_features = X.columns[0:3]\n    \n    fig, axarr = plt.subplots(1, len(qual_features), figsize = (7, 3))\n\n    # create a grid\n    grid_x = np.linspace(x0.min(),x0.max(),501)\n    grid_y = np.linspace(x1.min(),x1.max(),501)\n    xx, yy = np.meshgrid(grid_x, grid_y)\n    \n    XX = xx.ravel()\n    YY = yy.ravel()\n\n    for i in range(len(qual_features)):\n      XY = pd.DataFrame({\n          X.columns[3] : XX,\n          X.columns[4] : YY\n      })\n\n      for j in qual_features:\n        XY[j] = 0\n\n      XY[qual_features[i]] = 1\n\n      XY = XY[X.columns]\n\n      p = model.predict(XY)\n      p = p.reshape(xx.shape)\n      \n      \n      # use contour plot to visualize the predictions\n      axarr[i].contourf(xx, yy, p, cmap = \"jet\", alpha = 0.2, vmin = 0, vmax = 2)\n      \n      ix = X[qual_features[i]] == 1\n      # plot the data\n      axarr[i].scatter(x0[ix], x1[ix], c = y[ix], cmap = \"jet\", vmin = 0, vmax = 2)\n      \n      axarr[i].set(xlabel = X.columns[3], \n            ylabel  = X.columns[4], \n            title = qual_features[i])\n      \n      patches = []\n      for color, spec in zip([\"red\", \"green\", \"blue\"], [\"Adelie\", \"Chinstrap\", \"Gentoo\"]):\n        patches.append(Patch(color = color, label = spec))\n\n      plt.legend(title = \"Species\", handles = patches, loc = \"best\")\n      \n      plt.tight_layout()\n\nHere is the decision regions of Logistic Regression Model\n\nplot_regions(LR, X_train[best_columns], y_train)\n\n\n\n\n\n\n\n\nHere is the decision regions of Decision Tree Model\n\nplot_regions(DT, X_train[best_columns], y_train)\n\n\n\n\n\n\n\n\nHere is the decision regions of Random Forest Model\n\nplot_regions(RF, X_train[best_columns], y_train)\n\n\n\n\n\n\n\n\nHere is the decision regions of SVC Model\n\nplot_regions(svc, X_train[best_columns], y_train)\n\n\n\n\n\n\n\n\nI would like to do a little exploration and see how do the decision regions look like for higher gamma value for the SVC model. Here, I let gamma to be 10.\n\nsvc_10 = SVC(gamma=10, C=1, random_state=42)\nsvc_10.fit(X_train[best_columns], y_train)\nplot_regions(svc_10, X_train[best_columns], y_train)\n\n\n\n\n\n\n\n\nIt is noticeable that the decision regions for Chinstrap and Adelie penguins got more restrictive and small. Only regions that are very close to the training data are included as the decision region for these two species. The boundary of the region got more cursive than it was when \\(\\mathbf{gamma} = 0.1\\)\n\n\nConfusion Matrix\nConfusion Matrix for Logistic Regression\n\nfrom sklearn.metrics import confusion_matrix\n\ny_test_pred_LR = LR.predict(X_test[best_columns])\nLR_C = confusion_matrix(y_test, y_test_pred_LR)\nLR_C\n\narray([[31,  0,  0],\n       [ 0, 11,  0],\n       [ 0,  0, 26]])\n\n\nConfusion Matrix for Decision Trees\n\ny_test_pred_DT = DT.predict(X_test[best_columns])\nDT_C = confusion_matrix(y_test, y_test_pred_DT)\nDT_C\n\narray([[30,  0,  1],\n       [ 0, 11,  0],\n       [ 0,  0, 26]])\n\n\n\nfor i in range(3):\n    for j in range(3):\n        print(f\"There were {DT_C[i,j]} {le.classes_[i]} penguin(s) who were classified as {le.classes_[j]}.\")\n\nThere were 30 Adelie Penguin (Pygoscelis adeliae) penguin(s) who were classified as Adelie Penguin (Pygoscelis adeliae).\nThere were 0 Adelie Penguin (Pygoscelis adeliae) penguin(s) who were classified as Chinstrap penguin (Pygoscelis antarctica).\nThere were 1 Adelie Penguin (Pygoscelis adeliae) penguin(s) who were classified as Gentoo penguin (Pygoscelis papua).\nThere were 0 Chinstrap penguin (Pygoscelis antarctica) penguin(s) who were classified as Adelie Penguin (Pygoscelis adeliae).\nThere were 11 Chinstrap penguin (Pygoscelis antarctica) penguin(s) who were classified as Chinstrap penguin (Pygoscelis antarctica).\nThere were 0 Chinstrap penguin (Pygoscelis antarctica) penguin(s) who were classified as Gentoo penguin (Pygoscelis papua).\nThere were 0 Gentoo penguin (Pygoscelis papua) penguin(s) who were classified as Adelie Penguin (Pygoscelis adeliae).\nThere were 0 Gentoo penguin (Pygoscelis papua) penguin(s) who were classified as Chinstrap penguin (Pygoscelis antarctica).\nThere were 26 Gentoo penguin (Pygoscelis papua) penguin(s) who were classified as Gentoo penguin (Pygoscelis papua).\n\n\nOne Adelie Penguin is misidentified as a Gentoo Penguin by the Decision Trees Model.\nConfusion Matrix for Random Forest\n\ny_test_pred_RF = RF.predict(X_test[best_columns])\nRF_C = confusion_matrix(y_test, y_test_pred_RF)\nRF_C\n\narray([[31,  0,  0],\n       [ 0, 11,  0],\n       [ 0,  0, 26]])\n\n\nConfusion Matrix for SVC\n\ny_test_pred_svc = svc.predict(X_test[best_columns])\nsvc_C = confusion_matrix(y_test, y_test_pred_svc)\nsvc_C\n\narray([[31,  0,  0],\n       [ 0, 11,  0],\n       [ 0,  1, 25]])\n\n\n\nfor i in range(3):\n    for j in range(3):\n        print(f\"There were {svc_C[i,j]} {le.classes_[i]} penguin(s) who were classified as {le.classes_[j]}.\")\n\nThere were 31 Adelie Penguin (Pygoscelis adeliae) penguin(s) who were classified as Adelie Penguin (Pygoscelis adeliae).\nThere were 0 Adelie Penguin (Pygoscelis adeliae) penguin(s) who were classified as Chinstrap penguin (Pygoscelis antarctica).\nThere were 0 Adelie Penguin (Pygoscelis adeliae) penguin(s) who were classified as Gentoo penguin (Pygoscelis papua).\nThere were 0 Chinstrap penguin (Pygoscelis antarctica) penguin(s) who were classified as Adelie Penguin (Pygoscelis adeliae).\nThere were 11 Chinstrap penguin (Pygoscelis antarctica) penguin(s) who were classified as Chinstrap penguin (Pygoscelis antarctica).\nThere were 0 Chinstrap penguin (Pygoscelis antarctica) penguin(s) who were classified as Gentoo penguin (Pygoscelis papua).\nThere were 0 Gentoo penguin (Pygoscelis papua) penguin(s) who were classified as Adelie Penguin (Pygoscelis adeliae).\nThere were 1 Gentoo penguin (Pygoscelis papua) penguin(s) who were classified as Chinstrap penguin (Pygoscelis antarctica).\nThere were 25 Gentoo penguin (Pygoscelis papua) penguin(s) who were classified as Gentoo penguin (Pygoscelis papua).\n\n\nOne Gentoo penguin is misidentified as a Chinstrap penguin by the SVC model.\n\n\nDiscussion\nIn this post, I made some further investigation of the Penguin Dataset and tried out different machine learning models for classification. I selected the three features for classification using some preliminary data exploration and testing iterations of feature combinations on the Logistic Regression model to find the combination that gives me the best accuracy in the model. Then I tested Decision Tree, Random Forest, and Support Vector Machine models using the cross-validation on the training data and plotted their decision regions. At the final testing step, I used all these models and found that Logistic Regression and Random Forest models both give me a 100% accuracy.\nWhen choosing the parameters at the modeling building step, I ran into risk of overfitting my model, even though a large portion of the risk is avoided by cross-validation. For example, when selecting the gamma value, I chose the gamma that gives me the best result in the training data.\nIt is very interesting to see the decision regions of different models. The shapes of the regions reflect how different models work and classify the data. The boundaries of the tree models’ decision regions are parallel to the axis, while the logistic regression and support vector machine has linear and non-linear lines to seperate different decision regions. Also, when \\(\\mathbf{gamma}\\) gets larger, the model becomes more prudent in drawing decision regions for two penguin species, risking of overfitting the model."
  },
  {
    "objectID": "posts/new-test-post/index.html",
    "href": "posts/new-test-post/index.html",
    "title": "Second Post",
    "section": "",
    "text": "This is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/new-test-post/index.html#math",
    "href": "posts/new-test-post/index.html#math",
    "title": "Second Post",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "Classifying Penguin Species\n\n\n\n\n\nThe first post for CSCI 0451 Machine Learning\n\n\n\n\n\nFeb 19, 2024\n\n\nYide (Alex) Xu\n\n\n\n\n\n\n\n\n\n\n\n\nSecond Post\n\n\n\n\n\nA new blog post that I just made!\n\n\n\n\n\nMar 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\n\n\n\n\n\n\nTimnit Gebru\n\n\n\n\n\nA new blog post that I just made!\n\n\n\n\n\nMar 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\n\n\n\n\n\n\nHello Blog\n\n\n\n\n\nAn example blog post illustrating the key techniques you’ll need to demonstrate your learning in CSCI 0451.\n\n\n\n\n\nJan 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\nNo matching items"
  }
]